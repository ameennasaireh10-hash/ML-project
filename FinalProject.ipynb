{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "!pip install mglearn\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install catboost\n"
      ],
      "metadata": {
        "id": "_frfso72YvU6",
        "outputId": "b69e0807-e62a-4f3e-f0c1-0c83db422a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_frfso72YvU6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 43, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dependencies)\n",
            "                                       ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 175, in version\n",
            "    return parse_version(self._dist.version)\n",
            "                         ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 467, in version\n",
            "    return self.metadata['Version']\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 452, in metadata\n",
            "    return _adapters.Message(email.message_from_string(text))\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/__init__.py\", line 37, in message_from_string\n",
            "    return Parser(*args, **kws).parsestr(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/parser.py\", line 64, in parsestr\n",
            "    return self.parse(StringIO(text), headersonly=headersonly)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/parser.py\", line 53, in parse\n",
            "    feedparser.feed(data)\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 174, in feed\n",
            "    self._call_parse()\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 178, in _call_parse\n",
            "    self._parse()\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 238, in _parsegen\n",
            "    self._parse_headers(headers)\n",
            "  File \"/usr/lib/python3.12/email/feedparser.py\", line 486, in _parse_headers\n",
            "    self._cur.set_raw(*self.policy.header_source_parse(lastvalue))\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/email/_policybase.py\", line 309, in header_source_parse\n",
            "    name, value = sourcelines[0].split(':', 1)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1527, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1280, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 711, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 661, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 733, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 418, in _extract_from_extended_frame_gen\n",
            "    for f, (lineno, end_lineno, colno, end_colno) in frame_gen:\n",
            "                                                     ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 355, in _walk_tb_with_full_positions\n",
            "    positions = _get_code_position(tb.tb_frame.f_code, tb.tb_lasti)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 369, in _get_code_position\n",
            "    return next(itertools.islice(positions_gen, instruction_index // 2, None))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Collecting mglearn\n",
            "  Downloading mglearn-0.2.0-py2.py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mglearn) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mglearn) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from mglearn) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from mglearn) (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from mglearn) (11.3.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from mglearn) (0.12.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from mglearn) (2.37.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from mglearn) (1.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mglearn) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mglearn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mglearn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mglearn) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mglearn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mglearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->mglearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->mglearn) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->mglearn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->mglearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mglearn) (1.17.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
            "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
            "    self._complete_partial_requirements(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
            "    for link, (filepath, _) in batch_download:\n",
            "                               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/network/download.py\", line 169, in __call__\n",
            "    resp = _http_get_download(self._session, link)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/network/download.py\", line 118, in _http_get_download\n",
            "    resp = session.get(target_url, headers=HEADERS, stream=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/sessions.py\", line 602, in get\n",
            "    return self.request(\"GET\", url, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/network/session.py\", line 522, in request\n",
            "    return super().request(method, url, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/sessions.py\", line 575, in request\n",
            "    prep = self.prepare_request(req)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/sessions.py\", line 484, in prepare_request\n",
            "    p.prepare(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/models.py\", line 369, in prepare\n",
            "    self.prepare_cookies(cookies)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/models.py\", line 626, in prepare_cookies\n",
            "    cookie_header = get_cookie_header(self._cookies, self)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/cookies.py\", line 146, in get_cookie_header\n",
            "    r = MockRequest(request)\n",
            "        ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/cookies.py\", line 38, in __init__\n",
            "    self.type = urlparse(self._r.url).scheme\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/urllib/parse.py\", line 401, in urlparse\n",
            "    result = ParseResult(scheme, netloc, url, params, query, fragment)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1527, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1280, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 711, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 661, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 733, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 418, in _extract_from_extended_frame_gen\n",
            "    for f, (lineno, end_lineno, colno, end_colno) in frame_gen:\n",
            "                                                     ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 355, in _walk_tb_with_full_positions\n",
            "    positions = _get_code_position(tb.tb_frame.f_code, tb.tb_lasti)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 369, in _get_code_position\n",
            "    return next(itertools.islice(positions_gen, instruction_index // 2, None))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9ac008",
      "metadata": {
        "id": "6b9ac008"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mglearn as mg\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline , make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder , StandardScaler , LabelEncoder , OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier , KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split , KFold , cross_val_score , GridSearchCV , TimeSeriesSplit , GroupKFold\n",
        "from sklearn.linear_model import Lasso , LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sb\n",
        "import category_encoders as ce\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import scipy.stats as stats\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import ConfusionMatrixDisplay , accuracy_score , mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor , DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29378f98",
      "metadata": {
        "id": "29378f98"
      },
      "source": [
        "0: reading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6535d9",
      "metadata": {
        "id": "2a6535d9"
      },
      "outputs": [],
      "source": [
        "aisles = pd.read_csv(r\"archive\\aisles.csv\")\n",
        "department = pd.read_csv(r\"archive\\departments.csv\")\n",
        "pro_prior = pd.read_csv(r\"archive\\order_products__prior.csv\")\n",
        "pro_train = pd.read_csv(r\"archive/order_products__train.csv\")\n",
        "orders = pd.read_csv(r\"archive\\orders.csv\")\n",
        "products = pd.read_csv(r\"archive\\products.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb53e77",
      "metadata": {
        "id": "dbb53e77"
      },
      "source": [
        "1: Joins and memory optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b03abb2",
      "metadata": {
        "id": "8b03abb2"
      },
      "outputs": [],
      "source": [
        "def reduce_memory(df):\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if \"int\" in str(col_type):                 #السترينق حطيتها لانه برجع np.dtype ف لازم نحولها لنوعها المنطقي عشان نقدر نعمل مقارنه\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "\n",
        "        elif \"float\" in str(col_type):\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "\n",
        "        #بالبدايه حطيت بس else بعدين اكتشفت انه في انواع بيانات ثانيه مثل bool\n",
        "        elif col_type == \"object\":\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8d6cd4",
      "metadata": {
        "id": "7c8d6cd4"
      },
      "outputs": [],
      "source": [
        "dfs = [aisles , department , pro_prior , pro_train , orders , products]\n",
        "\n",
        "for i in range(len(dfs)):\n",
        "    dfs[i] = reduce_memory(dfs[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542d99fa",
      "metadata": {
        "id": "542d99fa"
      },
      "outputs": [],
      "source": [
        "#   في m1\n",
        "#   pro_prior بلشنا ب هذول لانه يحتوي تفاصيل المنتجات داخل الطلبات\n",
        "#   orders هو اللي بحتوي على معلومات الطلب , المستخدم , الوقت\n",
        "m1 = pd.merge(pro_prior , orders , on = \"order_id\" , how = \"left\")\n",
        "m2 = pd.merge(m1 , products , on = \"product_id\" , how = \"left\")\n",
        "m3 = pd.merge(m2 , department , on = \"department_id\" , how = \"left\")\n",
        "\n",
        "Full_DataSet = pd.merge(m3 , aisles , on = \"aisle_id\" , how = \"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c2a575",
      "metadata": {
        "id": "f3c2a575"
      },
      "outputs": [],
      "source": [
        "print(Full_DataSet.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08db0c66",
      "metadata": {
        "id": "08db0c66"
      },
      "outputs": [],
      "source": [
        "#كنت بدي اصير اعمل قراءه للملف مره ثانيه عشان ما اضل اعمل دمج كل ما اشغل الكود , بس اكتشفت انه تحميله بقعد وقت اكثر\n",
        "#Full_DataSet.to_csv('archive/full_instacart_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07c4665",
      "metadata": {
        "id": "e07c4665"
      },
      "source": [
        "2: EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9e07c3",
      "metadata": {
        "id": "ae9e07c3"
      },
      "outputs": [],
      "source": [
        "#cheking for null values\n",
        "missing_values = Full_DataSet.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "print( missing_values )\n",
        "print(\"==\"*40 )\n",
        "missing_percent = (missing_values / len(Full_DataSet)) * 100\n",
        "print(f\"Percentage of missing values in columns\\n {missing_percent}\", )\n",
        "\n",
        "missing_percent.plot(kind='bar', figsize=(8, 5), width=0.3, color='red', rot=0)\n",
        "plt.ylabel(\"Missing Percentage\")\n",
        "plt.title(\"Missing Values Percentage per Feature\")\n",
        "plt.show()\n",
        "# هاي مش قيم مفقوده بالغلط هاي بتدل انه الزبون اول مره بيطلب فمفيش عندو قيمه ل days_since_prior_order"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fbfa82",
      "metadata": {
        "id": "b1fbfa82"
      },
      "source": [
        "Distribution plots for numeric features and target(s) (histogram, density).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a2dd7a",
      "metadata": {
        "id": "f0a2dd7a"
      },
      "outputs": [],
      "source": [
        "# لازم نختار الاعمده الرقميه الي الها معنى او بتفيدنا لو عملنا الها هستوغرام او دينستي بالاحرى مين مهم افهم الديستربويشن تبعو\n",
        "Full_DataSet['order_hour_of_day'].plot(kind='hist', bins=24, figsize=(10, 6), title='Distribution of Orders by Hour of Day')\n",
        "\n",
        "#Histogram (المدرج التكراري) رسمة أعمدة بتبين الكمية في كل فترة\n",
        "\n",
        "plt.xlabel('Hour of Day (0 - 23)') # سمينا المحور عشان الدكتور يفهم\n",
        "\n",
        "plt.ylabel('Frequency (Number of Orders)')\n",
        "plt.show()\n",
        "\n",
        "#بنلاحظ  فوق الوقت الي بكون فيه وقت الذروه للزباين متى بكون باليوم"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e9dc89c",
      "metadata": {
        "id": "4e9dc89c"
      },
      "outputs": [],
      "source": [
        "Full_DataSet['days_since_prior_order'].plot(kind='hist', bins=30, figsize=(7, 6), color='orange', title='Distribution of Days Since Prior Order')\n",
        "\n",
        "plt.xlabel('Days Since Last Order')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b1c24a",
      "metadata": {
        "id": "f6b1c24a"
      },
      "outputs": [],
      "source": [
        "#Weekly shoppers (peak at 7 days) and Monthly shoppers (peak at 30 days). The spike at 30 days also includes customers who haven't ordered for more than a month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91573d7f",
      "metadata": {
        "id": "91573d7f"
      },
      "outputs": [],
      "source": [
        "'''# 1. نأخذ عينة عشوائية (10% مثلاً) عشان الرام ما تنفجر\n",
        "# هذا العمود يمثل \"ساعات اليوم\" من 0 لـ 23\n",
        "sample_hours = Full_DataSet['order_hour_of_day'].sample(frac=0.1, random_state=42)\n",
        "\n",
        "# 2. الرسم\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sample_hours.plot(kind='kde', color='green', title='Density Plot of Order Hour of Day')\n",
        "\n",
        "plt.xlabel('Hour of Day (0-23)')\n",
        "plt.xlim(0, 23) # عشان نحصر الرسمة في حدود اليوم\n",
        "plt.show()\n",
        "\n",
        "#الدينستي بعطينا هون تفاصيل اكثر او معبر اكثر بس بوخذ وقت اكثر للامانه '''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9815cbf8",
      "metadata": {
        "id": "9815cbf8"
      },
      "source": [
        "Categorical cardinality analysis (barplots / top-k frequencies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c21bba9",
      "metadata": {
        "id": "3c21bba9"
      },
      "outputs": [],
      "source": [
        "# first we need to do some cardinality checking for the catorgorical features\n",
        "categorical_cols = Full_DataSet.select_dtypes(include=['category']).columns\n",
        "categorical_cols\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4983a57e",
      "metadata": {
        "id": "4983a57e"
      },
      "outputs": [],
      "source": [
        "Full_DataSet[\"product_name\"].value_counts() # هون بنلاحظ انو في منتجات كتير متكرره يعني الكارديناليتي عاليه فهاض العامود مابفيدني اعملو فيجواليزشن"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a96f72",
      "metadata": {
        "id": "09a96f72"
      },
      "outputs": [],
      "source": [
        "# نعمل فحص للباقي\n",
        "checking = ['department', 'aisle', 'product_name']\n",
        "\n",
        "cardinality_counts = Full_DataSet[checking].nunique()\n",
        "print(\"عدد الأنواع في كل عمود\")\n",
        "print(cardinality_counts)\n",
        "# هون بنلاحظ الديبراتمنت فيه تنوع واطي فا بنقدر نعملو فيجواليز ونستفيد منه\n",
        "# الممرات 134 يعتبر عاللي فامبنقدر نرسمو كلو رح نوخذ الاكثر تكرارا تمام نفس الحاله بنطبقها على اسماء المنتجات"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9bbad05",
      "metadata": {
        "id": "e9bbad05"
      },
      "outputs": [],
      "source": [
        "# what is eval_set refer to ?\n",
        "Full_DataSet['eval_set'].value_counts()\n",
        "# هون شفنا انو هاض العامود بحتوي على داتا بتمثل الطلبات القديمه فهاض الاشي مابفيدني اني اعملو فيجواليز"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ab5451",
      "metadata": {
        "id": "17ab5451"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#  بنرسمه كأعمدة Barplot\n",
        "Full_DataSet['department'].value_counts().plot(kind='bar', figsize=(12,6) , color='blue')\n",
        "\n",
        "plt.title('Total Orders per Department', fontsize=15)\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.xlabel('Department Name')\n",
        "plt.xticks(rotation=45, ha='right') # ميلنا الأسماء عشان نقرأها بوضوح\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2670448a",
      "metadata": {
        "id": "2670448a"
      },
      "outputs": [],
      "source": [
        "top_aisles = Full_DataSet['aisle'].value_counts().head(25)\n",
        "#  هون ممكن تسالني انت طيب كيف رتبتهم من الاكثر للاقل ؟ لان الميثود تاعت الفاليوز بالديفولت تاعها بترتبهم من الاكبر لاصغبر\n",
        "\n",
        "top_aisles.plot(kind='bar',figsize=(12, 6), color='blue')\n",
        "\n",
        "\n",
        "plt.title('Top 25 Best Selling aisles ', fontsize=14)\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.xlabel('Aisle Name')\n",
        "plt.xticks(rotation=45, ha='right') # ميلنا الكلام بزاوية 45 عشان ينقرأ بوضوح\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ccf6739",
      "metadata": {
        "id": "0ccf6739"
      },
      "outputs": [],
      "source": [
        "products_for_visuals = Full_DataSet['product_name'].value_counts().head(20) # هون اخذنا اكثر 20 بس عشان يصير مقروء بشكل احسن\n",
        "\n",
        "products_for_visuals.plot(kind='bar', figsize=(12, 6), color='blue')\n",
        "plt.title('Top 25 Best Selling Products ', fontsize=14)\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.xlabel('Product Name')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c677155a",
      "metadata": {
        "id": "c677155a"
      },
      "source": [
        "• Correlation matrix, heatmap and pairwise scatter plots for selected numeric features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ff9079",
      "metadata": {
        "id": "67ff9079"
      },
      "outputs": [],
      "source": [
        "#  هاي الخطوة بدنا نعرف شو العوامل اللي بتخلي الزبون يعمل اعادة طلب او reorder\n",
        "# في هاي الحاله لازم نختار الاعمده الرقميه الي الها سلوك او بمعنى اصح الها معنى"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e009d53e",
      "metadata": {
        "id": "e009d53e"
      },
      "outputs": [],
      "source": [
        "selected_features = [\n",
        "    'order_number',\n",
        "    'order_dow',\n",
        "    'order_hour_of_day',\n",
        "    'days_since_prior_order',\n",
        "    'add_to_cart_order',\n",
        "    'reordered' # هاض العامود الي بدنا نعرف شو العوامل الي بتأثر عليه مثل ماقلنا فوق\n",
        "]\n",
        "\n",
        "# 2. حساب المصفوفة (Correlation Matrix)\n",
        "# الدالة .corr() هي العقل المدبر اللي بيحسب العلاقات\n",
        "corr_matrix = Full_DataSet[selected_features].corr()\n",
        "\n",
        "\n",
        "# annot=True: عشان يكتب الرقم جوا المربع\n",
        "# cmap='coolwarm': ألوان (أحمر للحار/الموجب، أزرق للبارد/السالب)\n",
        "# fmt='.2f': منزلتين عشريتين بس\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, square=False)\n",
        "\n",
        "plt.title('Correlation Heatmap of Numeric Features', fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447b5667",
      "metadata": {
        "id": "447b5667"
      },
      "outputs": [],
      "source": [
        "# 1. نأخذ عينة صغيرة جداً (1000 سطر) عشان الرسم يكون خفيف وواضح\n",
        "# Scatter Plot بيموت لو الداتا كبيرة\n",
        "scatter_sample = Full_DataSet.sample(n=1000, random_state=42)\n",
        "\n",
        "# 2. تحديد الأعمدة اللي بدنا نشوف علاقتها ببعض\n",
        "# ركزنا على أهم 3 أعمدة عشان ما نضيع وقت\n",
        "cols_to_plot = ['add_to_cart_order', 'days_since_prior_order', 'reordered']\n",
        "\n",
        "# 3. رسم الـ Pairplot\n",
        "# hue='reordered': عشان يلون النقاط (برتقالي للمكرر، أزرق للجديد)\n",
        "sns.pairplot(scatter_sample[cols_to_plot], hue='reordered', palette='husl', height=3)\n",
        "\n",
        "plt.suptitle('Pairwise Scatter Plots ', y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70efd15a",
      "metadata": {
        "id": "70efd15a"
      },
      "outputs": [],
      "source": [
        "#طيب اللون الاخضر هون بمثل المنتجات المعاد شرائها بنلاحظ بالرسمه انو دايما بالبدايه ببدا الزبون يجيب اغراضو الي  متعود عليها\n",
        "#بعدين بعد ما يجيبهم بجيب او بجرب اغراض جديده  مثل مابثمل اللون الزهري الي بمثل الاشياء الجديده\n",
        "#الرسمة الشمال تحت النقط بتوريك إن المنتجات المكررة الخضراء دايما محجوز الها المقاعد الأولى بالسلة  بغض النظر عن كم يوم مر.\n",
        "\n",
        "# الرسمة اليمين جبال بتأكد إن الناس بتتسوق بنظام أسبوعي أو شهري والمنتجات الجديدة والقديمة بتمشي على نفس هذا النظام."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcb4df0",
      "metadata": {
        "id": "cbcb4df0"
      },
      "source": [
        "• Time-of-day, day-of-week, and monthly seasonality plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb1c68a",
      "metadata": {
        "id": "2cb1c68a"
      },
      "outputs": [],
      "source": [
        "hourrr=Full_DataSet['order_hour_of_day'].value_counts()\n",
        "hourrr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff76183",
      "metadata": {
        "id": "aff76183"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(16, 7))\n",
        "\n",
        "\n",
        "# هون لازم قبل مانرسم نجهز الداتا لانها عباره عن مليون سطر فا لازم نرتبها اول حسب التكرار بعدين بنرتبها حسب الاندكس تصاعدني\n",
        "hour_counts = Full_DataSet['order_hour_of_day'].value_counts().sort_index()\n",
        "#لو تركناها هيك لاحظت رح تطلع معنا الرسمه الساعات من صفر ل 23 فا حيكون شوي مش مقروء الوضع فا افترحت لو بدنا نوري هالرسمه لاي حد لو نزبط الاندكس\n",
        "#ونخليه بنظام am و pm\n",
        "#بكون احسن ليش فقط عشان نخليه مقروء واريح للعين اكثر\n",
        "labels_of_hours = [\n",
        "    \"12 AM\", \"1 AM\", \"2 AM\", \"3 AM\", \"4 AM\", \"5 AM\",\n",
        "    \"6 AM\", \"7 AM\", \"8 AM\", \"9 AM\", \"10 AM\", \"11 AM\",\n",
        "    \"12 PM\", \"1 PM\", \"2 PM\", \"3 PM\", \"4 PM\", \"5 PM\",\n",
        "    \"6 PM\", \"7 PM\", \"8 PM\", \"9 PM\", \"10 PM\", \"11 PM\"\n",
        "]\n",
        "# x=labels_of_hours (الساعات)\n",
        "# y=hour_counts.values (عدد الطلبات)\n",
        "#هون اخترنا بار بلوت عشان سريع وبرضو برسملنا ال 24 عمود بسرعه عاليه\n",
        "sns.barplot(x=labels_of_hours, y=hour_counts.values, color='orange',)\n",
        "\n",
        "plt.title('Time of day Orders', fontsize=15)\n",
        "plt.xlabel('Hours: 12 AM - 11 PM  ', fontsize=12,color='blue')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684a6487",
      "metadata": {
        "id": "684a6487"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# رح يطلع معنا 7 أرقام من 0 ـ 6\n",
        "day_counts = Full_DataSet['order_dow'].value_counts().sort_index()\n",
        "\n",
        "#بنرتبهم كمان مره زي ماعملنا فوق\n",
        " # الويك إند عند الاجانب ببدا من السبت فهو رح يكون رقم صفر\n",
        "days_labels = [\"Saturday\",  \"Sunday\", \"Monday\",  \"Tuesday\", \"Wednesday\", \"Thursday\",\"Friday \"]\n",
        "\n",
        "\n",
        "\n",
        "sns.barplot(x=days_labels, y=day_counts.values, color=\"orange\")\n",
        "\n",
        "plt.title('Orders Day of Week ', fontsize=15)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Day', fontsize=12)\n",
        "\n",
        "plt.show()\n",
        "#بنلاحظ انو بالويكند اعلى طلبات وهاض المنطقي لانو الناس بتكون معطله"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc960f2a",
      "metadata": {
        "id": "dc960f2a"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "#  هون رح نستخدم هاض الكولوم لانو انسب اشي للمنثلي لانو مافي عنا بالداتا سيت كولم عن الاشهر\n",
        "days_of_month_counts = Full_DataSet['days_since_prior_order'].value_counts().sort_index()\n",
        "#رح نستخدم اللاين بلوت عشان نوضح التغيرات بشكل افضل\n",
        "sns.lineplot(x=days_of_month_counts.index, y=days_of_month_counts.values, marker='o', color='red', linewidth=2)\n",
        "\n",
        "# 3. تحسين المحاور\n",
        "plt.title('Monthly Seasonality', fontsize=14)\n",
        "plt.xlabel('Days Since Last Order', fontsize=12)\n",
        "plt.ylabel('Number of Orders', fontsize=12)\n",
        "plt.show()\n",
        "#بنلاحظ في قمه عند ال 7 و 30 يوم هاض يعني انو الزبون كل اسبوع غالبا بيجي يشتري وفي زباين بتيجي كل شهر او اكثر من شهر هاض كلو بنحط عند ال 30"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08a9fcc4",
      "metadata": {
        "id": "08a9fcc4"
      },
      "source": [
        "3: cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b6c7b6",
      "metadata": {
        "id": "d5b6c7b6"
      },
      "outputs": [],
      "source": [
        "del Full_DataSet[\"aisle\"]\n",
        "del Full_DataSet[\"department\"]\n",
        "del Full_DataSet[\"eval_set\"]\n",
        "\n",
        "print(Full_DataSet.isnull().sum())\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "#كان هدفي اشوف ال نان من هون بس ما زبطت ف شفتها من الملف نفسه\n",
        "Full_DataSet.head(10)\n",
        "#اللي بين معي انه ال نان بكون موجود لكل اول اوردير بطلبه المستخدم\\الزبون"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1419e778",
      "metadata": {
        "id": "1419e778"
      },
      "outputs": [],
      "source": [
        "def check_outliers(Full_DataSet):\n",
        "\n",
        "    out = [\"order_id\" , \"product_id\" , \"user_id\" , \"aisle_id\" , \"department_id\" , \"eval_set\"]\n",
        "    Outliers_DF = []\n",
        "    numeric_cols_all = Full_DataSet.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "\n",
        "    for i in numeric_cols_all:\n",
        "        if i not in out:\n",
        "            Outliers_DF.append(i)\n",
        "\n",
        "    col_length = len(Outliers_DF)\n",
        "    row = (col_length // 3) + 1\n",
        "    plt.figure(figsize=(20 , 5 * row))\n",
        "\n",
        "    #لاني بحتاج index + value خلال التكرار لرسم subplots.\n",
        "    #عشان هيك استخدمت enumerate\n",
        "    for i , col in enumerate(Outliers_DF):\n",
        "        plt.subplot(row , 3 , i + 1)\n",
        "\n",
        "        plot_data = Full_DataSet[col].dropna().sample(n = min(100000 , len(Full_DataSet)))\n",
        "\n",
        "        sb.boxplot(x = plot_data , color=\"lightblue\")\n",
        "        plt.title(col , fontsize = 12)\n",
        "        plt.xlabel(\" \")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45b92be",
      "metadata": {
        "id": "f45b92be"
      },
      "outputs": [],
      "source": [
        "#print(check_outliers(Full_DataSet))\n",
        "\n",
        "#من خلال الرسم بين معي انه الحد الفاصل بين اكبر قيمه والاوتلايرز هي 30\n",
        "Full_DataSet[\"add_to_cart_order\"] =  np.where(Full_DataSet[\"add_to_cart_order\"] > 30 , 30 , Full_DataSet[\"add_to_cart_order\"])\n",
        "#ونفس المبدأ بنطبق هون\n",
        "Full_DataSet[\"order_number\"] =  np.where(Full_DataSet[\"order_number\"] > 50 , 50 , Full_DataSet[\"order_number\"])\n",
        "\n",
        "print(Full_DataSet[\"add_to_cart_order\"].describe())\n",
        "print()\n",
        "print(Full_DataSet[\"order_number\"].describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb379afb",
      "metadata": {
        "id": "eb379afb"
      },
      "outputs": [],
      "source": [
        "Full_DataSet.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e13178",
      "metadata": {
        "id": "26e13178"
      },
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy = \"constant\" , fill_value = 0)\n",
        "#بعد ما تفرجت عالداتا من الاكسل , اكتشفت انه النان موجوده بس عند اول طلب للمستخدم , يعني ما عنده طلب مسبق\n",
        "#الموديل لما يشوف الصفر رح يقلك هاض المستخدم جديد , وعشان هيك ما عنده طلبات مسبقه\n",
        "Full_DataSet[\"days_since_prior_order\"] = imputer.fit_transform(Full_DataSet[\"days_since_prior_order\"].values.reshape(-1,1))\n",
        "\n",
        "DF = pd.DataFrame(Full_DataSet , columns = Full_DataSet.columns)\n",
        "\n",
        "#DF.to_csv('archive/full_instacart_data.csv', index=False)\n",
        "print(DF.isnull().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5372fa85",
      "metadata": {
        "id": "5372fa85"
      },
      "source": [
        "4: feature engineering (mandatory list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b414ff",
      "metadata": {
        "id": "46b414ff"
      },
      "outputs": [],
      "source": [
        "#total orders per user يعني كل مستخدم كم مره فات المحل واشترى يعني كم طلب عملو بشكل كامل مش كم منتج اشتراه بحياتو\n",
        "# طيب هون القروب باي بيجمعلي كل الداتا تبع كل يوزر ايدي لحال\n",
        "#بعدين بقله لكل يوزر بدي كولوم الاوردر نمبر تمام هسا الاورودر نمبر لكل زبون بمثل كل زبون كم طلب عملو لحد الان فا لو اخذتلو ال ماكس\n",
        "#لو اخذت الماكس رح تعطيني رقم اخر فاتوره والي بمثل عدد الطلبات الكلي لكل زبون طيب ممكن تسالني كان بمكاني اختار كاونت مش ماكس صح كلامك لكن\n",
        "#الداتا الي عندي او كولوم الاوردر نمبر يعني بحتوي على ارقام الطلبات مش عدد الطلبات فلو اخذت كاونت رح يطلعلي عدد المنتجات مش عدد الطلباتؤ\n",
        "#استخدمنا الريست اندكس عشان يرجعلي الداتا فريم مش سيرييز لانو قروب باي بيرجع سيرييز افتراضيا وبتخرب الداتا بصير اليوزر هو كولوم الاندكس\n",
        "user_total_orders = DF.groupby('user_id')['order_number'].max().reset_index()\n",
        "# 3. بنسمي العمود اسم واضح عشان ما نتخربط بعدين\n",
        "user_total_orders.columns = ['user_id', 'user_total_orders']\n",
        "\n",
        "print(user_total_orders.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e8efa8b",
      "metadata": {
        "id": "6e8efa8b"
      },
      "outputs": [],
      "source": [
        "#  average basket size\n",
        "#هذه الميزة بتحدد القدرة الشرائية Purchasing Power\n",
        "#وبتحدد نمط التسوق عند الزبون يعني بعرف الزبون الي عادته يشتري مثلا 50 غرض هاض زبون بمتوسط سلة كبيرة فا بهمني انو المودل ممكن\n",
        "#بهمني انو المودل ممكن يشوف هالشي ويستفيد منه بحيث لو كان شاري 3 اغراض بس يضل يقترح عليه لانو هالزبون من عادتو يشتري كثير\n",
        "\n",
        "#============================================================================================================\n",
        "\n",
        "#طيب السواال كيف بنحسبها ؟ بنحسبها عن طريق انو بنجيب لكل زبون كم المنتجات الي اشتراها بشكل كلي\n",
        "#بعدين بنجيب كم طلب عملو بشكل كلي\n",
        "#بعدين بنقسم المجموع على العدد\n",
        "#============================================================================================================\n",
        "#هون اولا عشان نجيب مجموع المنتجات الي اشتراها كل زبون بنجيب كولوم البروودكت ايدي وبنعمللو كاونت هيك بنعد كل المنتجات الي اشتراها\n",
        "#بعدين بنجيب كولوم الاوردر نمبر وبناخد الماكس زي ما شرحنا فوق عشان نعرف كم طلب عملو بشكل كلي\n",
        "\n",
        "#واستخدمنا ميثود الاقريقيت الي بتتيحلي اعمل اكشنز متعدده على كولمز مختلفه بنفس الوقت بدل ما اعمل قروب باي مرتين\n",
        "basket_data = DF.groupby('user_id').agg({ 'product_id':'count', 'order_number': 'max'}).reset_index()\n",
        "\n",
        "# مجرد تسميت الكولمز بشكل واضح بس\n",
        "basket_data.columns = ['user_id', 'total_items_bought', 'total_orders_made']\n",
        "\n",
        "#الحسبه\n",
        "basket_data['avg_basket_size'] = basket_data['total_items_bought'] / basket_data['total_orders_made']\n",
        "\n",
        "print(basket_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339a4785",
      "metadata": {
        "id": "339a4785"
      },
      "outputs": [],
      "source": [
        "#  User-Level Features full\n",
        "#هون لازم نعمل سورت اول عشان اخر مطلوب اللاست لاخر طلب فا ممكن يكون اخر طلب الي بجيبو مش هو اخر طلب عملو\n",
        "#بس الجهاز عندي بتحملش\n",
        "# Full_DataSet.sort_values(['user_id', 'order_number'], inplace=True)\n",
        "user_features = DF.groupby('user_id').agg({\n",
        "      # 1. Total #Orders\n",
        "    'order_number': 'max',\n",
        "    # 2. هاض بساعدنا نعرف كم منتج اشتراها كل زبون بشكل كلي\n",
        "     'product_id': 'count',\n",
        "    # 3. Reorder Ratio\n",
        "    'reordered': 'mean',\n",
        "\n",
        "   #هون حسبنا اخر مطلوبين بخطوه وحده بدل ما نعمل قروب باي مرتين\n",
        "    'days_since_prior_order': ['mean', 'last']\n",
        "}).reset_index()\n",
        "\n",
        "\n",
        "user_features.columns = ['user_id',  'user_total_orders',  'user_total_items', 'user_reorder_ratio','user_avg_days_between', 'user_days_since_last_order'  ]\n",
        "\n",
        "# Basket Size\n",
        "user_features['user_avg_basket_size'] = user_features['user_total_items'] / user_features['user_total_orders']\n",
        "\n",
        "#هاض العامود مش ضروري بعد ما حسبنا الافريج مابنحتاجه\n",
        "del user_features['user_total_items']\n",
        "\n",
        "display(user_features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba2c5a4",
      "metadata": {
        "id": "dba2c5a4"
      },
      "source": [
        "Product-level features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac0a591",
      "metadata": {
        "id": "4ac0a591"
      },
      "outputs": [],
      "source": [
        "# --- Product-Level Features ---\n",
        "\n",
        "# هالمره التجميع رح يكون حسب المنتج مش حسب اليوزر\n",
        "product_features = DF.groupby('product_id').agg({\n",
        "\n",
        "    #  Popularity\n",
        "    #هون شو ممكن يفيدنا هون قصدو من البوبولاريتي انه نعرف كم مرة انباع هاد المنتج ممكن من خلاله نعرف  اكثر المنتجات بينباع او وين اقل منتج\n",
        "    'user_id': 'count',\n",
        "    # ليش user_id؟ وليش count؟\n",
        "    # عشان نعد كم زبون اشترى هاد المنتج لانو كل سطر بيمثل  عنا عمليه شراء.\n",
        "    #====================================================================================\n",
        "    #  Reorder Rate\n",
        "    #هون بنحسب نسبة اعادة الطلبات للمنتج\n",
        "    'reordered': 'mean',\n",
        "    #هاي واضحه من اسمها بدهاش اشي\n",
        "    #====================================================================================\n",
        "    # Average  Position\n",
        "    #طيب ممكن تسالني ليش اخترنا هاض الكولوم بالذات رح لانه بمثل ترتيب المنتج داخل السلة فلو اخذنا المين رح يعطينا ترتيب المنتج داخل السلة زي ماهو طالب\n",
        "    'add_to_cart_order': 'mean'\n",
        "\n",
        "}).reset_index()\n",
        "\n",
        "product_features.columns = [ 'product_id', 'product_total_purchases', 'product_reorder_rate', 'product_avg_cart_position' ]\n",
        "\n",
        "display(product_features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55efd949",
      "metadata": {
        "id": "55efd949"
      },
      "source": [
        "User×Product interaction features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77231b20",
      "metadata": {
        "id": "77231b20"
      },
      "outputs": [],
      "source": [
        "#UserxProduct Interaction Features\n",
        "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "\n",
        "#دمجتها من هسا عشان اجرب اشغل user_days_since_last_order لانها مش جزء من الداتا الاساسيه ف ما اشتغل الكود\n",
        "DF = DF.merge(user_features[['user_id' , 'user_days_since_last_order']] , on = 'user_id', how = 'left')\n",
        "\n",
        "\n",
        "uxp_features = DF.groupby(['user_id', 'product_id']).agg({\n",
        "# Total Purchases of Product by User\n",
        "#هون عشان نجيبها كان ممكن نستخدم اي عمود بس بدنا واحد  بس المهم نعد كم مرة هاد الزبون اشترا هاد المنتج\n",
        "#طب ليش اخترت هاض الكولوم بالذات اختصارا للوقت بس عشان المطلوب الثاني رح ارجع اطلبه تمام\n",
        "#====================================================================================\n",
        "\n",
        "# Reorder Probability of Product by User\n",
        "#هون استخدمنا برضو بنفس العامود بس اخذنا المين عشان يعطينا النسبة الي طلبها الزبون من هاد المنتج\n",
        "    'reordered': ['count', 'mean'],\n",
        "\n",
        "    'user_days_since_last_order': 'max'    # هاي مش عارف اعملها علقت كيف ممكن نجيب ال   days since last purchase by     user_days_since_last_order  days_since_prior_order\n",
        "}).reset_index()\n",
        "\n",
        "# 2. تسمية الأعمدة بشكل واضح (عشان الدكتور يفهم كل عمود شو هو)\n",
        "uxp_features.columns = [\n",
        "    'user_id',\n",
        "    'product_id',\n",
        "    'uxp_total_bought',\n",
        "    'uxp_reorder_ratio',\n",
        "    'uxp_days_last_order_'\n",
        "]\n",
        "\n",
        "display(uxp_features.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc48c8af",
      "metadata": {
        "id": "bc48c8af"
      },
      "source": [
        "Temporal features: hour/day/month/year, season, holiday flags (if available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "796fe777",
      "metadata": {
        "id": "796fe777"
      },
      "outputs": [],
      "source": [
        "# طيب هون المطلوب منا بالتيمبورال فيشترز انو نزبط الوقت مثلا نخليه صبح ومسا ومثلا الايام نقسمها ايام عاديه وايام عطله والشهر\n",
        "#بس مبادايا الشهر والسنه والموسم مابنقدر لانو مامعنا معلومات عنها  في رح نكتفي بالساعات واليوم\n",
        "#نبدا بالساعات هسا التوقيت  عنا من 0 ل 23 فممكن نقسمهم لثلاث فترات\n",
        "def time_of_day(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Midnight'\n",
        "DF['time_of_day'] = DF['order_hour_of_day'].apply(time_of_day)\n",
        "print(DF['time_of_day'].head(10))\n",
        "print(\"==\"*40)\n",
        "#طيب قلنا للايام بنقسمها لايام عاديه وايام عطله\n",
        "#وزي مابنعرف بالداتا الي عندي السبت والاحد همه العطله فرقمهم بكون 0 و 1\n",
        "#هون اختصارا على حالنا لقدام خليت الكولوم الجديد عباره عن ارقام 0 و 1 بدل ما اخليها نصوص عشان اسهل التعامل معها بعدين\n",
        "def day_type(day):\n",
        "    if (day == 0) or (day == 1):\n",
        "        return 1 # طبعا واحد بتعني انها شسمو ويكند\n",
        "    else:\n",
        "        return 0\n",
        "DF['is_weekend'] = DF['order_dow'].apply(day_type)\n",
        "print(DF['is_weekend'].head(30))\n",
        "#حطيت 30 لانو خفت كلها صفار ههههه يسعد ربك\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d0e9eb",
      "metadata": {
        "id": "31d0e9eb"
      },
      "outputs": [],
      "source": [
        "# --- 5. Aggregations over Windows (Last 3 Orders) - بدون Lambda ---\n",
        "\n",
        "# 1. تجهيز الداتا: بنحسب حجم السلة لكل طلب (جدول صغير وخفيف)\n",
        "orders_summary = DF.groupby(['user_id', 'order_number']).size().reset_index(name='basket_size')\n",
        "\n",
        "# 2. الترتيب (مهم جداً): عشان لما نقول \"آخر 3\" يكونوا عنجد آخر 3 زمنياً\n",
        "orders_summary = orders_summary.sort_values(['user_id', 'order_number'])\n",
        "\n",
        "# 3. تعريف الفنكشن العادي (بدل اللمدا)\n",
        "# هذا الفنكشن بياخذ عمود أرقام، وبحسب المتوسط المتحرك لآخر 3 قيم\n",
        "def calculate_last_3_avg(series):\n",
        "    # window=3: يعني خذ 3 قيم\n",
        "    # min_periods=1: يعني حتى لو الزبون عنده طلب واحد بس، احسبله المعدل (ما ترجع Null)\n",
        "    return series.rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "# 4. تطبيق الفنكشن على كل زبون\n",
        "# transform: بتمسك الفنكشن اللي كتبناه فوق، وبتطبقه على كل \"مجموعة\" (زبون)\n",
        "orders_summary['rolling_avg_3_orders'] = orders_summary.groupby('user_id')['basket_size'].transform(calculate_last_3_avg)\n",
        "\n",
        "# 5. النتيجة النهائية\n",
        "# احنا بهمنا \"آخر وضع\" وصله الزبون، فبناخذ آخر سطر لكل زبون\n",
        "user_window_features = orders_summary.groupby('user_id').last().reset_index()\n",
        "\n",
        "# ترتيب وتنظيف الجدول النهائي\n",
        "user_window_features = user_window_features[['user_id', 'rolling_avg_3_orders']]\n",
        "user_window_features.columns = ['user_id', 'u_avg_basket_last_3']\n",
        "\n",
        "display(user_window_features.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba4a0f9",
      "metadata": {
        "id": "8ba4a0f9"
      },
      "source": [
        "✅ تم حساب (Rolling Window) باستخدام فنكشن عادي!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff55361",
      "metadata": {
        "id": "5ff55361"
      },
      "source": [
        "At least one engineered non-linear feature : log transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fff0338",
      "metadata": {
        "id": "8fff0338"
      },
      "outputs": [],
      "source": [
        "# افترض إنك حسبت user_features في الخطوة الأولى\n",
        "# بدنا نحول عمود \"عدد الطلبات\" باستخدام اللوغاريتم\n",
        "#هسا\n",
        "print(\"=\")\n",
        "#هون طقعت ديسبلاي لانها اوضح بس من برنت\n",
        "user_features['u_total_orders_log'] = np.log(user_features['user_total_orders'])\n",
        "\n",
        "# حطيناهم جمب بعض عشان تشوف الفرق\n",
        "print(\"user_total_orders trasform\")\n",
        "display(user_features[['user_total_orders', 'u_total_orders_log']].head(90))\n",
        "#\n",
        "#كمان فيتشر ثاني نعملو مش غلط\n",
        "product_features['p_total_purchases_log'] = np.log(product_features['product_total_purchases'])\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(\"product_ total_purchases transform\")\n",
        "\n",
        "display(product_features[['product_total_purchases', 'p_total_purchases_log']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5fa79c",
      "metadata": {
        "id": "1f5fa79c"
      },
      "outputs": [],
      "source": [
        "#بدي اسوي نسخه للاحتياط , هسا انا صرت بمرحله حرجه شوي و اي خطأ ممكن يدمر الداتا كامله ف الاحتياط واجب\n",
        "final_df = DF.copy()\n",
        "\n",
        "final_df = final_df.merge(user_features , on = 'user_id' , how = 'left')\n",
        "#لاني دمجتهم من قبل , عملت هالحركه عشان اتأكد ما يصير عندي اي تكرار\n",
        "final_df = final_df.drop(columns=[col for col in user_features.columns if col in final_df.columns and col != 'user_id'])\n",
        "\n",
        "final_df = final_df.merge(user_features , on = 'user_id' , how = 'left')\n",
        "final_df = final_df.merge(product_features , on = 'product_id' , how = 'left')\n",
        "final_df = final_df.merge(uxp_features , on = ['user_id' , 'product_id'] , how = 'left')\n",
        "final_df = final_df.merge(user_window_features , on = 'user_id' , how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f347b8",
      "metadata": {
        "id": "56f347b8"
      },
      "outputs": [],
      "source": [
        "print(final_df.shape)\n",
        "print(DF.shape)\n",
        "print()\n",
        "print(final_df.isnull().sum())\n",
        "final_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87833c0",
      "metadata": {
        "id": "b87833c0"
      },
      "outputs": [],
      "source": [
        "#بدي اختار الاعمده اللي لازم ازبط الميموري الهم\n",
        "final_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdac7c25",
      "metadata": {
        "id": "fdac7c25"
      },
      "outputs": [],
      "source": [
        "def reduce_memory_FE(df , col_name):\n",
        "\n",
        "    for col in col_name:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if \"int\" in str(col_type):\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "\n",
        "        elif \"float\" in str(col_type):\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "\n",
        "        elif col_type == \"object\":\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae4c2a9",
      "metadata": {
        "id": "fae4c2a9"
      },
      "outputs": [],
      "source": [
        "col_name = [\"uxp_reorder_ratio\" , \"u_avg_basket_last_3\" , \"uxp_total_bought\" , \"p_total_purchases_log\" , \"product_avg_cart_position\" , \"product_reorder_rate\" , \"product_total_purchases\" , \"u_total_orders_log\" , \"user_avg_basket_size\" , \"user_reorder_ratio\" , \"is_weekend\" , \"time_of_day\"]\n",
        "\n",
        "final_df = reduce_memory_FE(final_df, col_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3da99f4",
      "metadata": {
        "id": "e3da99f4"
      },
      "outputs": [],
      "source": [
        "final_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacc11e6",
      "metadata": {
        "id": "dacc11e6"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.drop(columns=['user_days_since_last_order_x' ,\n",
        "                                  'user_days_since_last_order_y'] ,\n",
        "    errors='ignore'\n",
        ")\n",
        "\n",
        "#ما عرفت اقلل الذاكره اكثر من هيك 😢\n",
        "final_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ca72fc",
      "metadata": {
        "id": "61ca72fc"
      },
      "outputs": [],
      "source": [
        "#final_df.to_csv('archive/new_instacart_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4a7bcc",
      "metadata": {
        "id": "1c4a7bcc"
      },
      "source": [
        "5: Dimensionality & collinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388e21eb",
      "metadata": {
        "id": "388e21eb"
      },
      "outputs": [],
      "source": [
        "id_cols = ['order_id' , 'user_id' , 'product_id' , 'aisle_id' , 'department_id']\n",
        "low_cols = [\"department_id\" , \"order_dow\" , \"time_of_day\"]\n",
        "high_cols = [\"user_id\" , \"product_id\" , \"aisle_id\"]\n",
        "\n",
        "final_df[high_cols] = final_df[high_cols].astype(str)\n",
        "#هالحركه سويتها بعد ما متت وانا بحلل الكود بعد ما طلعلي التنبيه هاض\n",
        "#Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
        "#لما راجعت الانكوديرز تذكرت انه التارقيت ما بشتغل غير مع نصوص والاعمده اللي انا معطيه اياهم رقميات\n",
        "\n",
        "target_col = \"reordered\"\n",
        "Frequency_col = \"product_name\"\n",
        "\n",
        "#عدد الاعمده كبير جدا فقلت بعمل لوب + استثناءات عشان اريح راسي\n",
        "num_cols = (final_df.drop(columns=[target_col]).select_dtypes(include=[\"int32\" , \"float32\"]).columns.tolist())\n",
        "num_cols = [c for c in num_cols if c not in id_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "653b1743",
      "metadata": {
        "id": "653b1743"
      },
      "outputs": [],
      "source": [
        "#بدي استخدم VIF , هاض عباره عن فنكشن رياضي ببين قديش في ارتباط وتكرار بين الاعمده نفسهم\n",
        "#الهدف منه اني اشوف شو في اعمده فيهم تشابه كبير وبقدمو نفس المعلومه تقريبا عشان احذف واحد منهم\n",
        "\n",
        "SAMPLE_SIZE = 50000\n",
        "V = final_df[num_cols].sample(n=SAMPLE_SIZE , random_state = 42)\n",
        "\n",
        "vif = pd.DataFrame()\n",
        "vif[\"feature\"] = V[num_cols].columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(V.values , i) for i in range(V.shape[1])]\n",
        "\n",
        "#تحت 5 ممتاز\n",
        "#بين ال 6 وال 10 مقبول\n",
        "#اكثر من هيك بدك تشوف شو و وين في ترابط غير مهم وتبلش تحذف\n",
        "#ال inf حذف مباشره\n",
        "\n",
        "vif.sort_values(\"VIF\" , ascending = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1eb004",
      "metadata": {
        "id": "6e1eb004"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8 , 5))\n",
        "plt.scatter(vif[\"VIF\"] , vif[\"feature\"])\n",
        "plt.axvline(10 , color = 'red' , linestyle = '--')\n",
        "plt.xlabel(\"VIF\")\n",
        "plt.title(\"VIF Scatter Plot\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f310ebf",
      "metadata": {
        "id": "0f310ebf"
      },
      "outputs": [],
      "source": [
        "#حذفت القيم اللانهائيه , والقيم اللي فيها النسبه عاليه , لانهم بدلو على تكرار وتشابه المعلومات , يعني لو خليتهم كلهم زي كأني مكرر نفس العامود ما فرقت\n",
        "drop_cols = [\n",
        "    \"user_days_since_last_order\" ,\n",
        "    \"uxp_days_last_order_\" ,\n",
        "    #u_total_orders_log كنت بدي اخليه بما انه تعبنا عليه بمرحلة الهندسه بس النسبه فيه كانت كثير عاليه :(\n",
        "    \"user_total_orders\" ,\n",
        "\n",
        "    \"product_avg_cart_position\" ,\n",
        "    \"product_reorder_rate\" ,\n",
        "    \"user_reorder_ratio\"\n",
        "    ]\n",
        "\n",
        "final_df = final_df.drop(columns = drop_cols , errors=\"ignore\")\n",
        "\n",
        "#صار عندي ايرور بالبريبروسيسر لانه الداتا صار فيها عدم تطابق بعد الدروب ف بدي ارجع انسخ الاعمده كمان مره\n",
        "num_cols = (final_df.drop(columns=[target_col]).select_dtypes(include=[\"int32\" , \"float32\"]).columns.tolist())\n",
        "num_cols = [c for c in num_cols if c not in id_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "542dcb37",
      "metadata": {
        "id": "542dcb37"
      },
      "source": [
        "6+7: preprocessing and scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50781d9a",
      "metadata": {
        "id": "50781d9a"
      },
      "outputs": [],
      "source": [
        "def best_params_for_TE(DF , high_cols):\n",
        "\n",
        "    #المشكله اللي صارت انه لما اقسم الداتا , لسا ما شغل البريبروسيسر عليها ف لسا مش كل الفيتشرز تحولو لقيم رقميه\n",
        "    del DF[\"product_name\"]\n",
        "\n",
        "    unique_users = DF['user_id'].unique()\n",
        "    selected_users = np.random.choice(unique_users , size = 3000 , replace=False)\n",
        "    #رفعت عدد العينات ل 5000 والكود طول لتنه اشتغل ف عشان هيك قللتهم ك حل وسط وهون طلع معي افضل نتيجع بعد عدة تكرارات\n",
        "\n",
        "    df_check = DF[DF['user_id'].isin(selected_users)].copy()\n",
        "\n",
        "    df_check[high_cols] = df_check[high_cols].astype(str)\n",
        "\n",
        "    xc = df_check.drop(\"reordered\", axis=1)\n",
        "    yc = df_check['reordered']\n",
        "\n",
        "    xc_train , xc_test , yc_train , yc_test = train_test_split(xc , yc , test_size = 0.2 , random_state = 42)\n",
        "\n",
        "    pipeline = make_pipeline(\n",
        "        ce.TargetEncoder(cols=high_cols) ,\n",
        "        RandomForestClassifier(\n",
        "            n_estimators = 100,  # عدد الأشجار\n",
        "            max_depth = 10 ,      # عمق الشجرة (عشان ما يوخذ وقت طويل)\n",
        "            random_state = 42 ,\n",
        "            n_jobs = -1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    param_grid = {\n",
        "        'targetencoder__smoothing': [1, 10, 50] ,\n",
        "        'targetencoder__min_samples_leaf': [1, 20]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator = pipeline ,\n",
        "        param_grid = param_grid ,\n",
        "        cv = 3 ,\n",
        "        scoring = 'roc_auc' ,\n",
        "        verbose = 1\n",
        "    )\n",
        "\n",
        "    print(\"\\n\")\n",
        "    grid_search.fit(xc_train, yc_train)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print(f\"Best ROC_AUC: {grid_search.best_score_:.4f}\")\n",
        "    print(\"Best Parameters:\")\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "    #تجربه فاشله لتحديد افضل المعاملات\n",
        "    #السبب انه عدد العينات كبير نسبيا ومدامني بجرب كل رقم بلوب لحال ف رح يوخذ مني وقت كبيييييير جدااااا\n",
        "    '''\n",
        "    smoothing_op = [1, 2, 10, 20, 50, 100]\n",
        "    leaf_op = [1, 5, 10, 20, 50]\n",
        "    Kfold_op = [3, 5, 10]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for k in Kfold_op:\n",
        "        current_kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "        for sm in smoothing_op:\n",
        "            for leaf in leaf_op:\n",
        "\n",
        "                encoder = ce.TargetEncoder(\n",
        "                    cols=high_cols,\n",
        "                    min_samples_leaf=leaf,\n",
        "                    smoothing=sm\n",
        "                )\n",
        "\n",
        "                model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "                pipeline = make_pipeline(encoder, model)\n",
        "\n",
        "                try:\n",
        "                    scores = cross_val_score(pipeline, xc, yc, cv=current_kf, scoring=\"roc_auc\")\n",
        "                    mean_auc = scores.mean()\n",
        "                    std_auc = scores.std()\n",
        "\n",
        "                    results.append({\n",
        "                        'n_splits': k,\n",
        "                        'smoothing': sm,\n",
        "                        'min_samples_leaf': leaf,\n",
        "                        'auc_mean': mean_auc,\n",
        "                        'auc_std': std_auc\n",
        "                    })\n",
        "\n",
        "                    print(f\"K={k}, Smooth={sm}, Leaf={leaf} -> AUC: {mean_auc:.4f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error at K={k}, Smooth={sm}, Leaf={leaf}: {e}\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    sorted_results = results_df.sort_values(by='auc_mean', ascending=False)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"TOP 3 PARAMETER COMBINATIONS:\")\n",
        "    print(sorted_results.head(3))\n",
        "    print()\n",
        "\n",
        "    if not sorted_results.empty:\n",
        "        best_params = sorted_results.iloc[0]\n",
        "        print(f\"\\nthe best:\\nSmoothing: {best_params['smoothing']}\\nMin Samples Leaf: {best_params['min_samples_leaf']}\\nK-Fold Splits: {int(best_params['n_splits'])}\")\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6791e61a",
      "metadata": {
        "id": "6791e61a"
      },
      "outputs": [],
      "source": [
        "def which_scaler(num_cols):\n",
        "\n",
        "    SAMPLE_SIZE = 500000\n",
        "    df_sampled = DF.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
        "\n",
        "    fig, axes = plt.subplots(len(num_cols), 2, figsize=(14, 4 * len(num_cols)))\n",
        "    plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "    for i, col in enumerate(num_cols):\n",
        "\n",
        "        sns.histplot(\n",
        "            df_sampled[col],\n",
        "            kde=True,\n",
        "            ax=axes[i, 0],\n",
        "            color='skyblue',\n",
        "            edgecolor='black',\n",
        "            line_kws={'linewidth': 3}\n",
        "        )\n",
        "        axes[i, 0].set_title(f'Distribution of {col}', fontsize=12)\n",
        "        axes[i, 0].set_xlabel(col, fontsize=10)\n",
        "        axes[i, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        stats.probplot(\n",
        "            df_sampled[col].dropna(),\n",
        "            dist=\"norm\",\n",
        "            plot=axes[i, 1]\n",
        "        )\n",
        "        axes[i, 1].set_title(f'Q-Q Plot of {col}', fontsize=12)\n",
        "        axes[i, 1].set_xlabel('Theoretical Quantiles (Normal)', fontsize=10)\n",
        "        axes[i, 1].set_ylabel('Sample Quantiles', fontsize=10)\n",
        "\n",
        "    plt.savefig('numerical_features_distribution_analysis.png', bbox_inches='tight')\n",
        "    print(\"تم حفظ تحليل التوزيعات في 'numerical_features_distribution_analysis.png'\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b074bb0",
      "metadata": {
        "id": "9b074bb0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "num_cols = [\"order_hour_of_day\", \"days_since_prior_order\", \"add_to_cart_order\", \"order_number\"]\n",
        "low_cols = [\"department_id\", \"order_dow\"]\n",
        "high_cols = [\"user_id\", \"product_id\", \"aisle_id\"]\n",
        "target_col = \"reordered\"\n",
        "Frequency_col = \"product_name\"\n",
        "\n",
        "#الرسم ببينلك انه التوزيع مبعثر وغير طبيعي وبحتوي على اوتلايرز كثيييييررر ,  عشان هيك كان افضل خيار استخدام\n",
        "#SD لانه افضل بالتعامل مع الاوتلايرز وما بتأثر فيهم بشكل واضح وسلبي\n",
        "\n",
        "#which_scaler(num_cols)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5d4e34",
      "metadata": {
        "id": "6c5d4e34"
      },
      "outputs": [],
      "source": [
        "print(\"Numeric Columns:\" , len(num_cols))\n",
        "print(num_cols)\n",
        "\n",
        "#بعد اكثر من تجربه افضل ناتج طلعلي كن زي اللي حطيتهم بالبريبروسيس\n",
        "#best_params_for_TE(final_df , high_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf024c3d",
      "metadata": {
        "id": "cf024c3d"
      },
      "source": [
        "8: Imbalanced data handling (classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba744831",
      "metadata": {
        "id": "ba744831"
      },
      "outputs": [],
      "source": [
        "final_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "921d2b17",
      "metadata": {
        "id": "921d2b17"
      },
      "source": [
        "اللي قاعد بصير هسا اني نقلت الانكوديرز لبعد مرحلة الهندسه , ليش ؟\n",
        "لانه بكل بساطه احترت كيف فعليا المفروض نسوي انكودينق للاعمده الجديده اللي عملناهم بعد الهندسه وكان هاض الحل الوحيد المنطقي ++ ما بزبط اسوي\n",
        "VIF وانا عامل انكودينق"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4f4a9a",
      "metadata": {
        "id": "2a4f4a9a"
      },
      "outputs": [],
      "source": [
        "\"\"\"#الموضوع هاض عباره عن وجود عدم توازين في تصنيف البيانات , مثلا الكلاس الاول نسبته اعلى من الثاني , ف هيك الموديل بصير يعتمد عالاول ويهمل الثاني\n",
        "#ف هيك بصير الموديل فاشل باكتشاف الانماط الجديد\n",
        "\n",
        "#تجهيز بيانات التصنيف#\n",
        "\n",
        "#جبت عينات من الداتا لانه اللابتوب شلف عندي لما اشتغلت عالداتا كامله , حاولت اكبر نسبة العينات قد ما بقدر\n",
        "SAMPLE_SIZE = 1000000\n",
        "final_sample = final_df.sample(n = SAMPLE_SIZE , random_state = 42)\n",
        "\n",
        "x_c = final_sample.drop(\"reordered\", axis=1)\n",
        "y_c = final_sample[\"reordered\"]\n",
        "\n",
        "xc_train , xc_test , yc_train , yc_test = train_test_split(x_c , y_c , test_size = 0.3 , stratify = y_c , random_state = 42)\n",
        "\n",
        "#طلعلي هيك SettingWithCopyWarning\n",
        "xc_train = xc_train.copy()\n",
        "xc_test  = xc_test.copy()\n",
        "\n",
        "#هالحركه سويتها بعد ما متت وانا بحلل الكود بعد ما طلعلي التنبيه هاض\n",
        "#Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
        "#لما راجعت الانكوديرز تذكرت انه التارقيت ما بشتغل غير مع نصوص والاعمده اللي انا معطيه اياهم رقميات\n",
        "\n",
        "train_cls = preprocessor.fit_transform(xc_train , yc_train)\n",
        "test_cls = preprocessor.transform(xc_test)\n",
        "\n",
        "''''''''''''''''''''''''''''''''''''''''''''''''\n",
        "#طيب هون بالتايم اوير سبلتنق المفروض نقسم الداتا حسب التوقيت طيب ليش ؟\n",
        "#الداتا الجديده بقصد فيها الي صارت اخر شي او الطلبات الجديده او يعني الي اخر اشي عمله الزبون\n",
        "#اولا عشان التقسيم العشوائي بالداتا تبعتنا بعمللنا مشكله شو المشكله هي انو المودل لما يجي يعمل فيت للداتا رح يعمل فيت ويتعلم على داتا جدي\n",
        "#بعد مايعمل فيت على داتا ممكن تكون جديده ممكن يكون التيست عنا داتا قديمه فهيك المودل رح يكون متعلم من داتا الجديده وهيك بغش حالو\n",
        "#وهيك بغش حالو  وبجيب اكيورسي عاليه كذابه تمام فا لازم نحل هالمشكله\n",
        "#طيب لازم نعمل سامبل سايز لانو الامور هيك حتصير كثير كبيره بدونو ومع السموت هاض الداتا بتتضاعف ومابتحمل الرام\n",
        "#ولازن مانتسخدم السامبلز سايز عادي عشان مانخرب توزيع الداتا افضل يعني\n",
        "#لو استحدمت سامبلنق عادي رح نفقد كثير داتا يعني مثلا ممكن ليوز معين نفقد الاوردر تاعو رقم 3 او اورد رقم 5\n",
        "#بينما باليوزر سامبلنق انا باخذ كل الداتا لاشخاص اقل بس\n",
        "#  User Sampling\n",
        "unique_users = final_df['user_id'].unique()\n",
        "\n",
        "# هون السايز خليتو انتجر لانو مثيود الرانودوم تشويس لازمها عدد صحيح مش عشري\n",
        "new_size = int(len(unique_users) * 0.05)\n",
        "\n",
        "#  اخترت اليوزرز بشكل عشوائي بدون تكرار\n",
        "my_users = np.random.choice(unique_users, size=new_size , replace=False)\n",
        "\n",
        "#  فلترت الداتا خليت بس الأسطر اللي بتخص اليوزرز اللي اخترتهم\n",
        "final_sample = final_df[final_df['user_id'].isin(my_users)].copy()\n",
        "# هسا لازم نتاكد انو الداتا مرتبه\n",
        "\n",
        "final_sample = final_sample.sort_values(by=[\"user_id\",\"order_number\"])\n",
        "\n",
        "# هسا بعد مارتبنا مطلوب منا نعمل نقسم الداتا طيب شو في طريقه نقسمها في اكثر من طريقه لكن كلهم معقدين فا هاي ابسط اشي لحالتنا\n",
        "#الي هي انو نعمل كولوم جديده نحط فيه الداتا الجديده تمام وبنقسم على ااساسها\n",
        "#هون الهدف نعمل كولوم جديد فيه قيم صح وخطا الان القيم الصح او الترو بنحطها للتيست وقيم الفولس بنحطها للترين تمام ها\n",
        "#الان كيف نعمل هاي القصه من خلال انو بنمرر كولم الاوردر نمبر تمام وبنقارن كل اوردر لكل يوزر مع الماكس او الطلب الاخير لهاض اليوز\n",
        "#فلو كان اليوزر الطلب الي بنقارنه هو نفسه الطلب الاخير اله حيرجع ترو تمام\n",
        "#هون جربت استخدم بدون ترانسفورم لاني ماكنت اعرفها طلع عندي ايرور والايرور لانو لو خليت الميثود بدونها رح ترجعلي داتا مضغوطه لانو بجيب الزبده لكل\n",
        "#لانو بجيب الزبده لكل كولوم فالحل ترانسفورم عشان نطبق الماكس عكل نقطه داتا عندي تم ؟ تم\n",
        "\n",
        "final_sample[\"last__orders\"]= final_sample[\"order_number\"] == final_sample.groupby(\"user_id\")[\"order_number\"].transform(\"max\")\n",
        "#هون قسمنا زي ما مطلوب منا\n",
        "train_df = final_sample[final_sample['last__orders'] == False]\n",
        "test_df = final_sample[final_sample['last__orders'] == True]\n",
        "#===\n",
        "\n",
        "# هون بدنا نقسم الداتا تاعت التريت بدنا نعطي الاكس الكولمز المهمه بس ونشيل كولوم التارقيت منها\n",
        "not_for_X_columns = ['reordered', 'eval_set', 'last__orders',  'order_id',]\n",
        "X_train = train_df.drop(columns=not_for_X_columns, errors='ignore')\n",
        "y_train = train_df['reordered']\n",
        "\n",
        "#نفس الشي للتيست\n",
        "X_test = test_df.drop(columns=not_for_X_columns, errors='ignore')\n",
        "y_test = test_df['reordered']\n",
        "\n",
        "# Fit على التدريب فقط\n",
        "train_cls = preprocessor.fit_transform(X_train , y_train)\n",
        "test_cls = preprocessor.transform(X_test)\n",
        "\"\"\"\n",
        "\n",
        "target_users_count = 10000\n",
        "\n",
        "unique_users = final_df['user_id'].unique()\n",
        "\n",
        "if target_users_count > len(unique_users):\n",
        "    target_users_count = len(unique_users)\n",
        "\n",
        "np.random.seed(42)\n",
        "my_users = np.random.choice(unique_users , size=target_users_count , replace=False)\n",
        "\n",
        "final_sample = final_df[final_df['user_id'].isin(my_users)].copy()\n",
        "\n",
        "final_sample = final_sample.sort_values(by=[\"user_id\", \"order_number\"])\n",
        "final_sample[\"last__orders\"] = final_sample[\"order_number\"] == final_sample.groupby(\"user_id\")[\"order_number\"].transform(\"max\")\n",
        "\n",
        "train_df = final_sample[final_sample['last__orders'] == False]\n",
        "test_df = final_sample[final_sample['last__orders'] == True]\n",
        "\n",
        "not_for_X_columns = ['reordered' , 'eval_set' , 'last__orders' ,  'order_id' , 'add_to_cart_order' , 'uxp_total_bought' , 'uxp_reorder_ratio' , 'order_number']\n",
        "\n",
        "xc_train = train_df.drop(columns = not_for_X_columns , errors = 'ignore')\n",
        "yc_train = train_df['reordered']\n",
        "\n",
        "xc_test = test_df.drop(columns = not_for_X_columns , errors = 'ignore')\n",
        "yc_test = test_df['reordered']\n",
        "\n",
        "real_num_cols = [c for c in num_cols if c not in not_for_X_columns]\n",
        "real_low_cols = [c for c in low_cols if c not in not_for_X_columns]\n",
        "real_high_cols = [c for c in high_cols if c not in not_for_X_columns]\n",
        "\n",
        "#مدامني قسمت الداتا حسب الزمن فهيك عالاغلب مش رح احتاجه\n",
        "KF = KFold(n_splits = 5 , shuffle = True , random_state = 42)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"encoding\" , OneHotEncoder(handle_unknown = \"ignore\" , sparse_output = True , drop = \"first\") , real_low_cols) ,\n",
        "\n",
        "        # Target Encoding يُستخدم مع الأعمدة الفئوية ذات عدد القيم الكبير.\n",
        "        # يتم تحويل كل فئة إلى متوسط قيمة المتغير الهدف المرتبط بها.\n",
        "        # لتجنب تسريب الهدف (Target Leakage)، يتم تطبيق الترميز داخل\n",
        "        # الـ Cross-Validation بحيث يُحسب الترميز من بيانات التدريب فقط.\n",
        "        # معاملات min_samples_leaf و smoothing تقلل تأثير الفئات النادرة\n",
        "        # عبر تقريبها من المتوسط العام، مما يحد من الـ overfitting.\n",
        "        (\"target_encoding\" , ce.TargetEncoder(min_samples_leaf = 20 , smoothing = 50) , real_high_cols) ,\n",
        "        (\"Frequency\" , ce.CountEncoder(normalize = True) , Frequency_col) ,\n",
        "        (\"scaling\" , StandardScaler() , real_num_cols)\n",
        "                 ]\n",
        ")\n",
        "# معامل الفريكوانسي ترو ليش؟ , لانه اذا حطيتو فولز اللي رح يصير انه رح يوخذ عدد التكرارات زي ما هو في هيك بصير عندي تباين كبير ورح يصير بحاجه لسكيلينق\n",
        "#اما هيك اللي رح يعملو انه رح يحولهم لنسبة بين ال 0 وال 1\n",
        "\n",
        "\n",
        "\n",
        "train_cls = preprocessor.fit_transform(xc_train , yc_train)\n",
        "test_cls = preprocessor.transform(xc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c06d70",
      "metadata": {
        "id": "e3c06d70"
      },
      "outputs": [],
      "source": [
        "available_users = xc_train[\"user_id\"].unique()\n",
        "n_users = len(available_users)\n",
        "n_users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11140884",
      "metadata": {
        "id": "11140884"
      },
      "outputs": [],
      "source": [
        "#اصريت اتأكد انه الترانسفورم تطبق\n",
        "print(train_cls.shape)\n",
        "print(xc_train.shape)\n",
        "print()\n",
        "preprocessor.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa1a40c",
      "metadata": {
        "id": "eaa1a40c"
      },
      "outputs": [],
      "source": [
        "print(yc_train.value_counts(normalize=True))\n",
        "\n",
        "plt.figure(figsize=(6 , 4))\n",
        "sns.countplot(x=yc_train)\n",
        "plt.title(\"Class Distribution in Training Set\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "#هيك بين عندي انه في تفاوت بنسب الكلاسين اللي عندي , الفرق ما بأثر كثير بس لازم اعمل موازنه لانه الدكتور طلب"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe25caa3",
      "metadata": {
        "id": "fe25caa3"
      },
      "outputs": [],
      "source": [
        "'''smote = SMOTE(random_state=42)\n",
        "smote_xtrain, smote_ytrain = smote.fit_resample(train_cls, yc_train)      #بتعمل KNN\n",
        "#اللي بصير هون انه السموت رح تزيد قيم الكلاس القليل بقيم ثناعيه او وهميه عشان التوازن\n",
        "print(smote_ytrain.value_counts())\n",
        "\n",
        "print(\"----------------------------------------------------------------\")\n",
        "'''\n",
        "rus = RandomUnderSampler(random_state = 42)\n",
        "under_xtrain , under_ytrain = rus.fit_resample(train_cls , yc_train)\n",
        "#هاي بتحذف من الكلاس الكبير بطريقه يصير قريب للكلاس الاقل , فيها مشكله انه ممكن تخسر بيانات مهمه من وراها واصلا هيك هيك الدقه فيها مش احسن اشي ف ما رح نستخدمها\n",
        "print(under_ytrain.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7dd078",
      "metadata": {
        "id": "cd7dd078"
      },
      "outputs": [],
      "source": [
        "'''model_original = RandomForestClassifier(n_estimators = 50 , max_depth = 20 , class_weight = \"balanced\")\n",
        "#class_weight = 'balanced' هاي بتعطي اهميه اكبر للكلاس الاقل , يعني بتعاقب الموديل لما يهمله ف مجازيا بزيد وزنه , تعتبر بديل لكلشي عملناه تحت\n",
        "#بدونها رح يكون في تحييز للكلاس اللي حجمه اكبر , وليش ؟ لانه الموديل بفهم انه هاض الكلاس لانه اكبر معناها هاض مهم والثاني لا\n",
        "model_original.fit(train_cls , yc_train)\n",
        "\n",
        "pred_O = model_original.predict(test_cls)\n",
        "print(classification_report(yc_test , pred_O))\n",
        "print(model_original.score(test_cls , yc_test))\n",
        "\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "model_smote = RandomForestClassifier(n_estimators = 50 , max_depth = 20)\n",
        "model_smote.fit(smote_xtrain , smote_ytrain)\n",
        "\n",
        "pred_S = model_smote.predict(test_cls)\n",
        "print(classification_report(yc_test , pred_S))\n",
        "print(model_smote.score(test_cls , yc_test))\n",
        "\n",
        "#السكور زاد بنسبه خفيفه , استدعاء الفئه اللي كانت اقل تحسن , الدقه قلت بنسبة خفيفه والسبب انه بطل يعتمد على كلاس واحد صار يعتمد على ثنين بالتصنيف ف نسبة الدقه اقل هون\n",
        "\n",
        "\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "model_under = RandomForestClassifier(n_estimators = 50 , max_depth = 20 , random_state = 42)\n",
        "model_under.fit(under_xtrain , under_ytrain)\n",
        "\n",
        "pred_U = model_under.predict(test_cls)\n",
        "print(classification_report(yc_test , pred_U))\n",
        "print(model_under.score(test_cls , yc_test))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d321154",
      "metadata": {
        "id": "1d321154"
      },
      "outputs": [],
      "source": [
        "'''fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "models_list = [\n",
        "    (model_original, \"Original (Class Weights)\"),\n",
        "    (model_smote, \"SMOTE\"),\n",
        "    (model_under, \"UnderSampling\")\n",
        "]\n",
        "\n",
        "for i, (model, title) in enumerate(models_list):\n",
        "    disp = ConfusionMatrixDisplay.from_estimator(\n",
        "        model,\n",
        "        test_cls,\n",
        "        yc_test,\n",
        "        display_labels=['Not Reordered', 'Reordered'],\n",
        "        cmap=plt.cm.Blues,\n",
        "        normalize='true',\n",
        "        ax=axes[i]\n",
        "    )\n",
        "    axes[i].set_title(title)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''\n",
        "#اول مره جربت اشغل هذول عالبيانات قبل التوزيع حسب الزمن , كان تقسيم عشوائي , وطلع معي انع الالفضل سموت , بس بعد التقسيم حسب الزمن رح نعتمد الداتا الاصليه\n",
        "## رجعت جربت بيانات اكبر وطلع معي السموت اعلى بنسبه بسيييييييطه وعشان هيك مش رح اهتم بالنسبه هاي ورح اعتمد الداتا الاصليه لانها ما بتوخذ وقت عكس السموت"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "406aeeb5",
      "metadata": {
        "id": "406aeeb5"
      },
      "source": [
        "رح نعتمد هذول بالتدريب train_cls , yc_train\n",
        "\n",
        "وهذول بالاختبار test_cls , yc_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02076bca",
      "metadata": {
        "id": "02076bca"
      },
      "source": [
        "Task A"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36a7698b",
      "metadata": {
        "id": "36a7698b"
      },
      "source": [
        "1- KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f911c4",
      "metadata": {
        "id": "16f911c4"
      },
      "outputs": [],
      "source": [
        "#هون رح اعتمد الاندر , لانه الموديل هون بصنف حسب الاقرب وبحالتي هاي ما بزبط يكون الداتا فيها عدم توازن , وما بقدر استخدم\n",
        "# class_weight = 'balanced'\n",
        "\n",
        "'''np.random.seed(42)\n",
        "\n",
        "small_users = np.random.choice(xc_train['user_id'].unique(), size=3000, replace=False)\n",
        "\n",
        "xtrain_tune = xc_train[xc_train['user_id'].isin(small_users)].copy()\n",
        "ytrain_tune = yc_train.loc[xtrain_tune.index].copy()\n",
        "\n",
        "xtest_tune = xc_test[xc_test['user_id'].isin(small_users)].copy()\n",
        "ytest_tune = yc_test.loc[xtest_tune.index].copy()'''\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tuning_x = xc_train.copy()\n",
        "tuning_y = yc_train.copy()\n",
        "\n",
        "t = TimeSeriesSplit(n_splits = 3)\n",
        "\n",
        "'''trainK = []\n",
        "testK = []\n",
        "k_values = range(1 , 22 , 2)\n",
        "\n",
        "for n in k_values:\n",
        "    tempK = KNeighborsClassifier(n_neighbors = n , n_jobs = -1)\n",
        "    Knn_model = tempK.fit(under_xtrain , under_ytrain)\n",
        "    trainK.append(tempK.score(x_trainS , y_trainS))\n",
        "    testK.append(tempK.score(x_testS , y_testS))\n",
        "\n",
        "print(testK)\n",
        "\n",
        "plt.plot(k_values , trainK , label = \"Traink\")\n",
        "plt.plot(k_values , testK , label = \"Testk\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"n_neighbors\")\n",
        "plt.legend()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e97d996",
      "metadata": {
        "id": "0e97d996"
      },
      "outputs": [],
      "source": [
        "'''FAST_TUNING_USERS = 1000\n",
        "\n",
        "np.random.seed(42)\n",
        "# بنختار عينة جديدة صغيرة من العينة الأصلية الكبيرة\n",
        "KNNsmall_users = np.random.choice(xtrain_tune['user_id'].unique(), size=800, replace=False)\n",
        "\n",
        "# تصفية الداتا\n",
        "knn_x_small = xtrain_tune[xtrain_tune['user_id'].isin(small_users)].copy()\n",
        "knn_y_small = ytrain_tune[xtrain_tune['user_id'].isin(small_users)]\n",
        "\n",
        "KNN_pipline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    KNeighborsClassifier()\n",
        ")\n",
        "\n",
        "\n",
        "t = TimeSeriesSplit(n_splits = 3)\n",
        "\n",
        "param_grid = {\n",
        "    \"kneighborsclassifier__n_neighbors\": [7 , 10 , 20 , 25 , 30] ,\n",
        "    \"kneighborsclassifier__weights\" : ['distance' , 'uniform']\n",
        "}\n",
        "\n",
        "KNN_grid_search = GridSearchCV(\n",
        "    estimator = KNN_pipline ,\n",
        "    param_grid = param_grid ,\n",
        "    cv = t ,\n",
        "    scoring = 'f1' ,\n",
        "    n_jobs = -1 ,\n",
        "    verbose=3\n",
        ")\n",
        "\n",
        "print(\"Done.....\")\n",
        "\n",
        "KNN_grid_search.fit(knn_x_small , knn_y_small)\n",
        "\n",
        "print(\"Best CV score:\" , KNN_grid_search.best_score_)\n",
        "print(\"Best params:\" , KNN_grid_search.best_params_)'''\n",
        "\n",
        "\n",
        "KNNsmall_users = np.random.choice(tuning_x['user_id'].unique() , size = 2000 , replace = False)\n",
        "\n",
        "knn_x_small = tuning_x[tuning_x['user_id'].isin(KNNsmall_users)].copy()\n",
        "knn_y_small = tuning_y[tuning_x['user_id'].isin(KNNsmall_users)]\n",
        "\n",
        "'''\n",
        "\n",
        "KNN_pipline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    KNeighborsClassifier(n_jobs = -1)\n",
        ")\n",
        "\n",
        "KNNparam_grid = {\n",
        "    \"kneighborsclassifier__n_neighbors\": [7 , 10 , 20 , 25 , 30] ,\n",
        "    \"kneighborsclassifier__weights\" : ['distance' , 'uniform']\n",
        "}\n",
        "\n",
        "KNN_grid_search = GridSearchCV(\n",
        "    estimator = KNN_pipline ,\n",
        "    param_grid = KNNparam_grid ,\n",
        "    cv = t ,\n",
        "    scoring = \"f1\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "KNN_grid_search.fit(knn_x_small , knn_y_small)\n",
        "\n",
        "print(\"\\nBest Score:\", KNN_grid_search.best_score_)\n",
        "print(\"Best Parameters:\", KNN_grid_search.best_params_)'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4953978",
      "metadata": {
        "id": "c4953978"
      },
      "outputs": [],
      "source": [
        "'''best_w = KNN_grid_search.best_params_['kneighborsclassifier__weights']\n",
        "best_k = KNN_grid_search.best_params_['kneighborsclassifier__n_neighbors']\n",
        "\n",
        "finalKNN_pip = make_pipeline(\n",
        "        preprocessor ,\n",
        "        KNeighborsClassifier(n_neighbors = best_k , weights = best_w , n_jobs = -1)\n",
        ")\n",
        "\n",
        "finalKNN_pip.fit(xc_train, yc_train)\n",
        "\n",
        "\n",
        "y_pred_KNN = finalKNN_pip.predict(xc_test)\n",
        "\n",
        "print(classification_report(yc_test , y_pred_KNN))\n",
        "print(accuracy_score(yc_test , y_pred_KNN))\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816c7928",
      "metadata": {
        "id": "816c7928"
      },
      "outputs": [],
      "source": [
        "#بعد تطبيق تقسيم زمني واقعي ومعالجة عدم التوازن، انخفضت الدقة من قيم مرتفعة غير واقعية إلى ~0.7،\n",
        "#إلا أن قدرة النموذج على اكتشاف المنتجات المعاد طلبها (Recall) تحسّنت بشكل واضح،\n",
        "#مما يجعل النموذج أكثر ملاءمة للاستخدام الحقيقي."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f866ab53",
      "metadata": {
        "id": "f866ab53"
      },
      "source": [
        "2-Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068f5ca7",
      "metadata": {
        "id": "068f5ca7"
      },
      "outputs": [],
      "source": [
        "'''tuning_users = np.random.choice(tuning_x[\"user_id\"].unique(), size=2500, replace=False)\n",
        "\n",
        "# تجهيز عينة التونينق\n",
        "LoR_x_small = tuning_x[tuning_x['user_id'].isin(tuning_users)].copy().sort_values(by=['user_id'])\n",
        "LoR_y_small = tuning_y.loc[LoR_x_small.index]\n",
        "\n",
        "LoR_pipline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    LogisticRegression(solver = 'liblinear' , class_weight = 'balanced' , random_state = 42 , n_jobs = -1)\n",
        ")\n",
        "\n",
        "LoRparam_grid = {\n",
        "    \"logisticregression__C\": [0.001 , 0.01 , 0.1 , 1 , 10] ,\n",
        "    \"logisticregression__penalty\": ['l1' , 'l2']\n",
        "}\n",
        "\n",
        "LoR_grid_search = GridSearchCV(\n",
        "    estimator = LoR_pipline ,\n",
        "    param_grid = LoRparam_grid ,\n",
        "    cv = t ,\n",
        "    scoring = \"f1\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "LoR_grid_search.fit(LoR_x_small , LoR_y_small)\n",
        "\n",
        "print(\"\\nBest Score:\", LoR_grid_search.best_score_)\n",
        "print(\"Best Parameters:\", LoR_grid_search.best_params_)'''\n",
        "\n",
        "\"\"\"Best Score: 0.7038598274542919\n",
        "Best Parameters: {'logisticregression__C': 0.001, 'logisticregression__penalty': 'l2'}\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacdee95",
      "metadata": {
        "id": "cacdee95"
      },
      "outputs": [],
      "source": [
        "'''best_C = LoR_grid_search.best_params_[\"logisticregression__C\"]\n",
        "best_P = LoR_grid_search.best_params_[\"logisticregression__penalty\"]\n",
        "\n",
        "final_LoR_pipeline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    LogisticRegression(solver = 'liblinear' , C = best_C , penalty = best_P , class_weight = 'balanced' , random_state = 42 , n_jobs = -1)\n",
        ")\n",
        "\n",
        "final_LoR_pipeline.fit(xc_train, yc_train)\n",
        "\n",
        "y_pred_LoR = final_LoR_pipeline.predict(xc_test)\n",
        "\n",
        "print(classification_report(yc_test, y_pred_LoR))\n",
        "print(f\"Logistic Accuracy: {accuracy_score(yc_test , y_pred_LoR):.4f}\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd5a355",
      "metadata": {
        "id": "bdd5a355"
      },
      "source": [
        "3- Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8348ab2",
      "metadata": {
        "id": "f8348ab2"
      },
      "outputs": [],
      "source": [
        "'''tuning_users = np.random.choice(tuning_x[\"user_id\"].unique(), size=800, replace=False)\n",
        "RF_x_small = tuning_x[tuning_x['user_id'].isin(tuning_users)].copy().sort_values(by=['user_id'])\n",
        "RF_y_small = tuning_y.loc[RF_x_small.index]\n",
        "\n",
        "RF_pipline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    RandomForestClassifier(class_weight = \"balanced\" , random_state = 42 , n_jobs = -1)\n",
        ")\n",
        "\n",
        "RFparam_grid = {\n",
        "    \"randomforestclassifier__n_estimators\": [100 , 150 , 200] ,\n",
        "    \"randomforestclassifier__max_depth\": [None , 10 , 20] ,\n",
        "    \"randomforestclassifier__min_samples_split\": [2 , 5 , 10] ,\n",
        "    \"randomforestclassifier__min_samples_leaf\": [1 , 2 , 4] ,\n",
        "    \"randomforestclassifier__class_weight\": ['balanced' , 'balanced_subsample']\n",
        "}\n",
        "\n",
        "RF_grid_search = GridSearchCV(\n",
        "    estimator = RF_pipline ,\n",
        "    param_grid = RFparam_grid ,\n",
        "    cv = t ,\n",
        "    scoring = \"f1\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "RF_grid_search.fit(RF_x_small , RF_y_small)\n",
        "\n",
        "print(\"\\nBest Score:\", RF_grid_search.best_score_)\n",
        "print(\"Best Parameters:\", RF_grid_search.best_params_)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd8469f0",
      "metadata": {
        "id": "bd8469f0"
      },
      "outputs": [],
      "source": [
        "'''best_N = RF_grid_search.best_params_[\"randomforestclassifier__n_estimators\"]\n",
        "best_MD = RF_grid_search.best_params_[\"randomforestclassifier__max_depth\"]\n",
        "best_MSP = RF_grid_search.best_params_[ \"randomforestclassifier__min_samples_split\"]\n",
        "best_MSL = RF_grid_search.best_params_[\"randomforestclassifier__min_samples_leaf\"]\n",
        "best_W = RF_grid_search.best_params_[\"randomforestclassifier__class_weight\"]\n",
        "\n",
        "final_RF_pipeline = make_pipeline(\n",
        "    preprocessor,\n",
        "    RandomForestClassifier(n_estimators = best_N , max_depth = best_MD ,\n",
        "                    min_samples_leaf = best_MSL , min_samples_split = best_MSP ,\n",
        "                 class_weight = best_W , n_jobs = -1 , random_state = 42\n",
        "                        )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "final_RF_pipeline.fit(xc_train , yc_train)\n",
        "\n",
        "y_pred_RF = final_RF_pipeline.predict(xc_test)\n",
        "\n",
        "print(classification_report(yc_test, y_pred_RF))\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(yc_test, y_pred_RF):.4f}\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bddecf5b",
      "metadata": {
        "id": "bddecf5b"
      },
      "source": [
        "RF: 0.7339 | LoR: 0.7066 | KNN: 0.7227"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78b8aaab",
      "metadata": {
        "id": "78b8aaab"
      },
      "source": [
        "Task B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b7e2e3",
      "metadata": {
        "id": "05b7e2e3"
      },
      "outputs": [],
      "source": [
        "cleaned_df = final_df[final_df['order_number'] > 1].copy()\n",
        "orders_level = (\n",
        "    cleaned_df\n",
        "    .sort_values([\"user_id\", \"order_number\"])      # مهم قبل drop_duplicates [web:116]\n",
        "    .drop_duplicates(\"order_id\", keep=\"last\")      # أي صف يمثل نفس الطلب [web:116]\n",
        "    .copy()\n",
        ")\n",
        "\n",
        "\n",
        "target_users_count = 15000\n",
        "unique_users = orders_level[\"user_id\"].unique()\n",
        "\n",
        "if target_users_count > len(unique_users):\n",
        "    target_users_count = len(unique_users)\n",
        "\n",
        "np.random.seed(42)\n",
        "my_users = np.random.choice(unique_users, size=target_users_count, replace=False)\n",
        "\n",
        "final_sample = orders_level[orders_level[\"user_id\"].isin(my_users)].copy()\n",
        "final_sample = final_sample.sort_values(by=[\"user_id\", \"order_number\"])\n",
        "\n",
        "\n",
        "final_sample[\"last__orders\"] = (\n",
        "    final_sample[\"order_number\"]\n",
        "    == final_sample.groupby(\"user_id\")[\"order_number\"].transform(\"max\")\n",
        ")\n",
        "\n",
        "train_df = final_sample[final_sample[\"last__orders\"] == False].copy()\n",
        "test_df  = final_sample[final_sample[\"last__orders\"] == True].copy()\n",
        "\n",
        "y_col = \"days_since_prior_order\"\n",
        "#احتياط المفروض\n",
        "train_df = train_df.dropna(subset=[y_col])\n",
        "test_df = test_df.dropna(subset=[y_col])\n",
        "\n",
        "not_for_X_columns = [\n",
        "    \"eval_set\", \"last__orders\", \"order_id\",\n",
        "    \"reordered\", \"add_to_cart_order\",\n",
        "    \"order_number\", y_col\n",
        "]\n",
        "\n",
        "xr_train = train_df.drop(columns=not_for_X_columns + [\"user_id\"], errors=\"ignore\")\n",
        "yr_train = train_df[y_col].astype(\"float32\")\n",
        "\n",
        "xr_test  = test_df.drop(columns=not_for_X_columns + [\"user_id\"], errors=\"ignore\")\n",
        "yr_test  = test_df[y_col].astype(\"float32\")\n",
        "\n",
        "\n",
        "\n",
        "real_num_col_reg = xr_train.select_dtypes(include=[\"int32\", \"float32\", \"float64\"]).columns.tolist()\n",
        "\n",
        "real_num_col_reg = [c for c in real_num_col_reg if c not in id_cols]\n",
        "\n",
        "real_low_col_reg  = [c for c in low_cols  if c in xr_train.columns]\n",
        "real_high_col_reg = [c for c in high_cols if c in xr_train.columns]\n",
        "real_freq_col_reg = [c for c in Frequency_col if c in xr_train.columns]\n",
        "\n",
        "\n",
        "\n",
        "preprocessor_reg = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"encoding\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop=\"first\"), real_low_col_reg),\n",
        "        (\"target_encoding\", ce.TargetEncoder(min_samples_leaf=20, smoothing=50), real_high_col_reg),\n",
        "        (\"frequency\", ce.CountEncoder(normalize=True), real_freq_col_reg),\n",
        "        (\"scaling\", StandardScaler(), real_num_col_reg),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "train_reg = preprocessor_reg.fit_transform(xr_train, yr_train)\n",
        "test_reg  = preprocessor_reg.transform(xr_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"xr_train shape : {xr_train.shape}\")\n",
        "print(f\"train_reg shape : {train_reg.shape}\")"
      ],
      "metadata": {
        "id": "9WDukkTR-645"
      },
      "id": "9WDukkTR-645",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5933f9bb",
      "metadata": {
        "id": "5933f9bb"
      },
      "source": [
        "1- KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745b008a",
      "metadata": {
        "id": "745b008a"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tuning_xr = xr_train.copy()\n",
        "tuning_yr = yr_train.copy()\n",
        "GF = GroupKFold(n_splits=5)\n",
        "\n",
        "tuning_groups = train_df[\"user_id\"].copy()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d34944",
      "metadata": {
        "id": "45d34944"
      },
      "outputs": [],
      "source": [
        "def model_score(model , xr_train , yr_train , xr_test , yr_test):\n",
        "    model.fit(xr_train , yr_train)\n",
        "    y_pred = model.predict(xr_test)\n",
        "\n",
        "    mae = mean_absolute_error(yr_test , y_pred)\n",
        "    print(f\"MAE:  {mae}\")\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(yr_test , y_pred))\n",
        "    print(f\"RMSE: {rmse}\")\n",
        "\n",
        "    r2 = r2_score(yr_test , y_pred)\n",
        "    print(f\"R2:   {r2}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde17af1",
      "metadata": {
        "id": "bde17af1",
        "outputId": "ef9fc9d7-0ba3-4b5a-889a-7a540e3ea507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best Score: 0.0580445647239685\n",
            "Best Parameters: {'kneighborsregressor__n_neighbors': 30, 'kneighborsregressor__weights': 'uniform'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Best Score: -8.939542770385742        neg_root_mean_squared_error\\nBest Parameters: {'kneighborsregressor__n_neighbors': 30, 'kneighborsregressor__weights': 'uniform'}\""
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "KNNRsmall_users = np.random.choice(tuning_groups.unique(), size=5000, replace=False)\n",
        "\n",
        "mask = tuning_groups.isin(KNNRsmall_users)\n",
        "\n",
        "KNNR_x_small = tuning_xr.loc[mask].copy()\n",
        "KNNR_y_small = tuning_yr.loc[mask].copy()\n",
        "KNNR_groups_small = tuning_groups.loc[mask].copy()\n",
        "\n",
        "KNNR_pipline = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    KNeighborsRegressor()\n",
        ")\n",
        " #  [7 , 10 , 20 , 25 , 30] هون اول اشي عملنا اخترنا\n",
        "#تمام بما انو المودل اختار 30 هاض بيعني انو لسا في مجال نضيف عدد اكبر من النيبرز فا رح نعدلها ل [30, 50, 70, 100]\n",
        "# لو بدك تاكد تقلل ال 100 لو بتعمل اندر فيت وشوف\n",
        "KNNRparan_grid = {\n",
        "    \"kneighborsregressor__n_neighbors\": [30, 50, 70, 100] ,\n",
        "    \"kneighborsregressor__weights\" : ['distance' , 'uniform']\n",
        "}\n",
        "\n",
        "KNNR_grid_search = GridSearchCV(\n",
        "    estimator = KNNR_pipline ,\n",
        "    param_grid = KNNRparan_grid ,\n",
        "    cv = GF ,\n",
        "    scoring = \"r2\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "\n",
        "KNNR_grid_search.fit(KNNR_x_small , KNNR_y_small , groups = KNNR_groups_small)\n",
        "\n",
        "print(\"\\nBest Score:\" , KNNR_grid_search.best_score_)\n",
        "print(\"Best Parameters:\" , KNNR_grid_search.best_params_)\n",
        "#النتائج هاي قبل تزبيط السبلت\n",
        "#\"\"\"Best Score: -8.939542770385742        neg_root_mean_squared_error\n",
        "#Best Parameters: {'kneighborsregressor__n_neighbors': 30, 'kneighborsregressor__weights': 'uniform'}\"\"\"\n",
        "#=============================================\n",
        "#النتائج هاي بعد تزبيط السبلت وعدد الجيران\n",
        "#Best Score: 0.19590063095092775\n",
        "#Best Parameters: {'kneighborsregressor__n_neighbors': 100, 'kneighborsregressor__weights': 'uniform'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da3640ba",
      "metadata": {
        "id": "da3640ba",
        "outputId": "4c8631b0-9a5e-4f23-8f06-169fa85081da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE:  5.356390476226807\n",
            "RMSE: 7.753037626488352\n",
            "R2:   0.3814122676849365\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "best_Rk = KNNR_grid_search.best_params_[\"kneighborsregressor__n_neighbors\"]\n",
        "\n",
        "best_Rw = KNNR_grid_search.best_params_[\"kneighborsregressor__weights\"]\n",
        "\n",
        "final_KNNR_pip = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    KNeighborsRegressor(n_neighbors = best_Rk , weights = best_Rw , n_jobs = -1)\n",
        ")\n",
        "\n",
        "model_score(final_KNNR_pip , xr_train , yr_train , xr_test , yr_test)\n",
        "#قبل التعديل على السبلت وعدد الجيران هيك كانت النتائج\n",
        "#MAE:  5.356390476226807\n",
        "#RMSE: 7.753037626488352\n",
        "#R2:   0.3814122676849365\n",
        "\n",
        "#==============================\n",
        "#بعد التعديل\n",
        "#MAE:  4.928064823150635\n",
        "# RMSE: 6.896374702453613\n",
        "# R2:   0.5105602741241455\n",
        "#شوف فرق ال ار 2 بعد التعديل مش بطال على كي ان ان على داتا زي هيك"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81039735",
      "metadata": {
        "id": "81039735"
      },
      "source": [
        "2- Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7c4690",
      "metadata": {
        "id": "5b7c4690"
      },
      "outputs": [],
      "source": [
        "DTRsmall_users = np.random.choice(tuning_groups.unique(), size=5000, replace=False)\n",
        "mask = tuning_groups.isin(DTRsmall_users)\n",
        "\n",
        "DTR_x_small = tuning_xr.loc[mask].copy()\n",
        "DTR_y_small = tuning_yr.loc[mask].copy()\n",
        "DTR_groups_small = tuning_groups.loc[mask].copy()\n",
        "\n",
        "\n",
        "DTR_pipline = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    DecisionTreeRegressor(random_state = 42)\n",
        ")\n",
        "\n",
        "DTRparam_grid = {\n",
        "    \"decisiontreeregressor__max_depth\": [None , 10 , 20 , 30] ,\n",
        "    \"decisiontreeregressor__min_samples_split\": [2 , 10 , 20] ,\n",
        "    \"decisiontreeregressor__min_samples_leaf\": [1 , 10 , 50] ,\n",
        "}\n",
        "\n",
        "DTR_grid_search = GridSearchCV(\n",
        "    estimator = DTR_pipline ,\n",
        "    param_grid = DTRparam_grid ,\n",
        "    cv = GF ,\n",
        "    scoring = \"r2\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "DTR_grid_search.fit(DTR_x_small, DTR_y_small, groups = DTR_groups_small)\n",
        "\n",
        "print(\"\\nBest Score:\" , DTR_grid_search.best_score_)\n",
        "print(\"Best Parameters:\" , DTR_grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be356c8e",
      "metadata": {
        "id": "be356c8e",
        "outputId": "6844bce8-c357-484b-9514-67ca28982b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE:  5.257224113534276\n",
            "RMSE: 7.805453685792402\n",
            "R2:   0.3730198334737874\n"
          ]
        }
      ],
      "source": [
        "best_RMD = DTR_grid_search.best_params_[\"decisiontreeregressor__max_depth\"]\n",
        "best_RMSS = DTR_grid_search.best_params_[ \"decisiontreeregressor__min_samples_split\"]\n",
        "best_RMSL = DTR_grid_search.best_params_[\"decisiontreeregressor__min_samples_leaf\"]\n",
        "\n",
        "final_DTR_pip = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    DecisionTreeRegressor(max_depth = best_RMD , min_samples_leaf = best_RMSL ,\n",
        "                           min_samples_split = best_RMSS , random_state = 42)\n",
        ")\n",
        "model_score(final_DTR_pip , xr_train , yr_train , xr_test , yr_test)\n",
        "#قبل تعديل السبلت\n",
        "# MAE:  5.257224113534276\n",
        "# RMSE: 7.805453685792402\n",
        "# R2:   0.3730198334737874\n",
        " #بعد التعديل\n",
        "# MAE:  4.69943136062894\n",
        "# RMSE: 6.862364731848102\n",
        "# R2:   0.5153758030454685"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}