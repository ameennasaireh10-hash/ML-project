{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9ac008",
      "metadata": {
        "id": "6b9ac008"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mglearn as mg\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline , make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder , StandardScaler , LabelEncoder , OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier , KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split , KFold , cross_val_score , GridSearchCV , TimeSeriesSplit , GroupKFold , learning_curve , validation_curve\n",
        "from sklearn.linear_model import Lasso , LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sb\n",
        "import category_encoders as ce\n",
        "from sklearn.ensemble import RandomForestClassifier , RandomForestRegressor\n",
        "import scipy.stats as stats\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import ConfusionMatrixDisplay , accuracy_score , mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor , DecisionTreeClassifier\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "import statsmodels.api as sm\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.base import clone\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier , plot_importance\n",
        "import shap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29378f98",
      "metadata": {
        "id": "29378f98"
      },
      "source": [
        "0: reading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6535d9",
      "metadata": {
        "id": "2a6535d9"
      },
      "outputs": [],
      "source": [
        "aisles = pd.read_csv(r\"archive\\aisles.csv\")\n",
        "department = pd.read_csv(r\"archive\\departments.csv\")\n",
        "pro_prior = pd.read_csv(r\"archive\\order_products__prior.csv\")\n",
        "pro_train = pd.read_csv(r\"archive/order_products__train.csv\")\n",
        "orders = pd.read_csv(r\"archive\\orders.csv\")\n",
        "products = pd.read_csv(r\"archive\\products.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb53e77",
      "metadata": {
        "id": "dbb53e77"
      },
      "source": [
        "1: Joins and memory optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b03abb2",
      "metadata": {
        "id": "8b03abb2"
      },
      "outputs": [],
      "source": [
        "def reduce_memory(df):\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if \"int\" in str(col_type):                 #السترينق حطيتها لانه برجع np.dtype ف لازم نحولها لنوعها المنطقي عشان نقدر نعمل مقارنه\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "\n",
        "        elif \"float\" in str(col_type):\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "\n",
        "        #بالبدايه حطيت بس else بعدين اكتشفت انه في انواع بيانات ثانيه مثل bool\n",
        "        elif col_type == \"object\":\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8d6cd4",
      "metadata": {
        "id": "7c8d6cd4"
      },
      "outputs": [],
      "source": [
        "dfs = [aisles , department , pro_prior , pro_train , orders , products]\n",
        "\n",
        "for i in range(len(dfs)):\n",
        "    dfs[i] = reduce_memory(dfs[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542d99fa",
      "metadata": {
        "id": "542d99fa"
      },
      "outputs": [],
      "source": [
        "all_products = pd.concat(\n",
        "    [pro_prior, pro_train],\n",
        "    axis=0,\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "\n",
        "#   في m1\n",
        "#   pro_prior بلشنا ب هذول لانه يحتوي تفاصيل المنتجات داخل الطلبات\n",
        "#   orders هو اللي بحتوي على معلومات الطلب , المستخدم , الوقت\n",
        "m1 = all_products.merge(orders, on=\"order_id\", how=\"left\")\n",
        "m2 = m1.merge(products, on=\"product_id\", how=\"left\")\n",
        "m3 = m2.merge(department, on=\"department_id\", how=\"left\")\n",
        "\n",
        "Full_DataSet = m3.merge(aisles, on=\"aisle_id\", how=\"left\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c2a575",
      "metadata": {
        "id": "f3c2a575"
      },
      "outputs": [],
      "source": [
        "print(Full_DataSet.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08db0c66",
      "metadata": {
        "id": "08db0c66"
      },
      "outputs": [],
      "source": [
        "#كنت بدي اصير اعمل قراءه للملف مره ثانيه عشان ما اضل اعمل دمج كل ما اشغل الكود , بس اكتشفت انه تحميله بقعد وقت اكثر\n",
        "#Full_DataSet.to_csv('archive/full_instacart_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07c4665",
      "metadata": {
        "id": "e07c4665"
      },
      "source": [
        "2: EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9e07c3",
      "metadata": {
        "id": "ae9e07c3"
      },
      "outputs": [],
      "source": [
        "#cheking for null values\n",
        "missing_values = Full_DataSet.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "print( missing_values )\n",
        "print(\"==\"*40 )\n",
        "missing_percent = (missing_values / len(Full_DataSet)) * 100\n",
        "print(f\"Percentage of missing values in columns\\n {missing_percent}\", )\n",
        "\n",
        "missing_percent.plot(kind='bar', figsize=(8, 5), width=0.3, color='red', rot=0)\n",
        "plt.ylabel(\"Missing Percentage\")\n",
        "plt.title(\"Missing Values Percentage per Feature\")\n",
        "plt.show()\n",
        "# هاي مش قيم مفقوده بالغلط هاي بتدل انه الزبون اول مره بيطلب فمفيش عندو قيمه ل days_since_prior_order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d706ee8",
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_percent = (missing_values / len(Full_DataSet)) * 100\n",
        "print(f\"Percentage of missing values in columns\\n {missing_percent}\", )\n",
        "\n",
        "missing_percent.plot(kind='bar', figsize=(8, 5), width=0.3, color='red', rot=0)\n",
        "plt.ylabel(\"Missing Percentage\")\n",
        "plt.title(\"Missing Values Percentage per Feature\")\n",
        "plt.show()\n",
        "# هاي مش قيم مفقوده بالغلط هاي بتدل انه الزبون اول مره بيطلب فمفيش عندو قيمه ل days_since_prior_order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1bb4c98",
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_percent = (missing_values / len(Full_DataSet)) * 100\n",
        "print(f\"Percentage of missing values in columns\\n {missing_percent}\", )\n",
        "\n",
        "missing_percent.plot(kind='bar', figsize=(8, 5), width=0.3, color='red', rot=0)\n",
        "plt.ylabel(\"Missing Percentage\")\n",
        "plt.title(\"Missing Values Percentage per Feature\")\n",
        "plt.show()\n",
        "# هاي مش قيم مفقوده بالغلط هاي بتدل انه الزبون اول مره بيطلب فمفيش عندو قيمه ل days_since_prior_order"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fbfa82",
      "metadata": {
        "id": "b1fbfa82"
      },
      "source": [
        "Distribution plots for numeric features and target(s) (histogram, density).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a2dd7a",
      "metadata": {
        "id": "f0a2dd7a"
      },
      "outputs": [],
      "source": [
        "# لازم نختار الاعمده الرقميه الي الها معنى او بتفيدنا لو عملنا الها هستوغرام او دينستي بالاحرى مين مهم افهم الديستربويشن تبعو\n",
        "Full_DataSet['order_hour_of_day'].plot(kind='hist', bins=24, figsize=(10, 6), title='Distribution of Orders by Hour of Day')\n",
        "\n",
        "#Histogram (المدرج التكراري) رسمة أعمدة بتبين الكمية في كل فترة\n",
        "\n",
        "plt.xlabel('Hour of Day (0 - 23)') # سمينا المحور عشان الدكتور يفهم\n",
        "\n",
        "plt.ylabel('Frequency (Number of Orders)')\n",
        "plt.show()\n",
        "\n",
        "#بنلاحظ  فوق الوقت الي بكون فيه وقت الذروه للزباين متى بكون باليوم"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e9dc89c",
      "metadata": {
        "id": "4e9dc89c"
      },
      "outputs": [],
      "source": [
        "Full_DataSet['days_since_prior_order'].plot(kind='hist', bins=30, figsize=(7, 6), color='orange', title='Distribution of Days Since Prior Order')\n",
        "\n",
        "plt.xlabel('Days Since Last Order')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b1c24a",
      "metadata": {
        "id": "f6b1c24a"
      },
      "outputs": [],
      "source": [
        "#Weekly shoppers (peak at 7 days) and Monthly shoppers (peak at 30 days). The spike at 30 days also includes customers who haven't ordered for more than a month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91573d7f",
      "metadata": {
        "id": "91573d7f"
      },
      "outputs": [],
      "source": [
        "'''# 1. نأخذ عينة عشوائية (10% مثلاً) عشان الرام ما تنفجر\n",
        "# هذا العمود يمثل \"ساعات اليوم\" من 0 لـ 23\n",
        "sample_hours = Full_DataSet['order_hour_of_day'].sample(frac=0.1, random_state=42)\n",
        "\n",
        "# 2. الرسم\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sample_hours.plot(kind='kde', color='green', title='Density Plot of Order Hour of Day')\n",
        "\n",
        "plt.xlabel('Hour of Day (0-23)')\n",
        "plt.xlim(0, 23) # عشان نحصر الرسمة في حدود اليوم\n",
        "plt.show()\n",
        "\n",
        "#الدينستي بعطينا هون تفاصيل اكثر او معبر اكثر بس بوخذ وقت اكثر للامانه '''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9815cbf8",
      "metadata": {
        "id": "9815cbf8"
      },
      "source": [
        "Categorical cardinality analysis (barplots / top-k frequencies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c21bba9",
      "metadata": {
        "id": "3c21bba9"
      },
      "outputs": [],
      "source": [
        "# first we need to do some cardinality checking for the catorgorical features\n",
        "categorical_cols = Full_DataSet.select_dtypes(include=['category']).columns\n",
        "categorical_cols\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4983a57e",
      "metadata": {
        "id": "4983a57e"
      },
      "outputs": [],
      "source": [
        "Full_DataSet[\"product_name\"].value_counts() # هون بنلاحظ انو في منتجات كتير متكرره يعني الكارديناليتي عاليه فهاض العامود مابفيدني اعملو فيجواليزشن"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a96f72",
      "metadata": {
        "id": "09a96f72"
      },
      "outputs": [],
      "source": [
        "# نعمل فحص للباقي\n",
        "checking = ['department', 'aisle', 'product_name']\n",
        "\n",
        "cardinality_counts = Full_DataSet[checking].nunique()\n",
        "print(\"عدد الأنواع في كل عمود\")\n",
        "print(cardinality_counts)\n",
        "# هون بنلاحظ الديبراتمنت فيه تنوع واطي فا بنقدر نعملو فيجواليز ونستفيد منه\n",
        "# الممرات 134 يعتبر عاللي فامبنقدر نرسمو كلو رح نوخذ الاكثر تكرارا تمام نفس الحاله بنطبقها على اسماء المنتجات"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9bbad05",
      "metadata": {
        "id": "e9bbad05"
      },
      "outputs": [],
      "source": [
        "# what is eval_set refer to ?\n",
        "Full_DataSet['eval_set'].value_counts()\n",
        "# هون شفنا انو هاض العامود بحتوي على داتا بتمثل الطلبات القديمه فهاض الاشي مابفيدني اني اعملو فيجواليز"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ab5451",
      "metadata": {
        "id": "17ab5451"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#  بنرسمه كأعمدة Barplot\n",
        "Full_DataSet['department'].value_counts().plot(kind='bar', figsize=(12,6) , color='blue')\n",
        "\n",
        "plt.title('Total Orders per Department', fontsize=15)\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.xlabel('Department Name')\n",
        "plt.xticks(rotation=45, ha='right') # ميلنا الأسماء عشان نقرأها بوضوح\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2670448a",
      "metadata": {
        "id": "2670448a"
      },
      "outputs": [],
      "source": [
        "top_aisles = Full_DataSet['aisle'].value_counts().head(25)\n",
        "#  هون ممكن تسالني انت طيب كيف رتبتهم من الاكثر للاقل ؟ لان الميثود تاعت الفاليوز بالديفولت تاعها بترتبهم من الاكبر لاصغبر\n",
        "\n",
        "top_aisles.plot(kind='bar',figsize=(12, 6), color='blue')\n",
        "\n",
        "\n",
        "plt.title('Top 25 Best Selling aisles ', fontsize=14)\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.xlabel('Aisle Name')\n",
        "plt.xticks(rotation=45, ha='right') # ميلنا الكلام بزاوية 45 عشان ينقرأ بوضوح\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ccf6739",
      "metadata": {
        "id": "0ccf6739"
      },
      "outputs": [],
      "source": [
        "products_for_visuals = Full_DataSet['product_name'].value_counts().head(20) # هون اخذنا اكثر 20 بس عشان يصير مقروء بشكل احسن\n",
        "\n",
        "products_for_visuals.plot(kind='bar', figsize=(12, 6), color='blue')\n",
        "plt.title('Top 25 Best Selling Products ', fontsize=14)\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.xlabel('Product Name')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c677155a",
      "metadata": {
        "id": "c677155a"
      },
      "source": [
        "• Correlation matrix, heatmap and pairwise scatter plots for selected numeric features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ff9079",
      "metadata": {
        "id": "67ff9079"
      },
      "outputs": [],
      "source": [
        "#  هاي الخطوة بدنا نعرف شو العوامل اللي بتخلي الزبون يعمل اعادة طلب او reorder\n",
        "# في هاي الحاله لازم نختار الاعمده الرقميه الي الها سلوك او بمعنى اصح الها معنى"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e009d53e",
      "metadata": {
        "id": "e009d53e"
      },
      "outputs": [],
      "source": [
        "selected_features = [\n",
        "    'order_number',\n",
        "    'order_dow',\n",
        "    'order_hour_of_day',\n",
        "    'days_since_prior_order',\n",
        "    'add_to_cart_order',\n",
        "    'reordered' # هاض العامود الي بدنا نعرف شو العوامل الي بتأثر عليه مثل ماقلنا فوق\n",
        "]\n",
        "\n",
        "# 2. حساب المصفوفة (Correlation Matrix)\n",
        "# الدالة .corr() هي العقل المدبر اللي بيحسب العلاقات\n",
        "corr_matrix = Full_DataSet[selected_features].corr()\n",
        "\n",
        "\n",
        "# annot=True: عشان يكتب الرقم جوا المربع\n",
        "# cmap='coolwarm': ألوان (أحمر للحار/الموجب، أزرق للبارد/السالب)\n",
        "# fmt='.2f': منزلتين عشريتين بس\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, square=False)\n",
        "\n",
        "plt.title('Correlation Heatmap of Numeric Features', fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447b5667",
      "metadata": {
        "id": "447b5667"
      },
      "outputs": [],
      "source": [
        "# 1. نأخذ عينة صغيرة جداً (1000 سطر) عشان الرسم يكون خفيف وواضح\n",
        "# Scatter Plot بيموت لو الداتا كبيرة\n",
        "scatter_sample = Full_DataSet.sample(n=1000, random_state=42)\n",
        "\n",
        "# 2. تحديد الأعمدة اللي بدنا نشوف علاقتها ببعض\n",
        "# ركزنا على أهم 3 أعمدة عشان ما نضيع وقت\n",
        "cols_to_plot = ['add_to_cart_order', 'days_since_prior_order', 'reordered']\n",
        "\n",
        "# 3. رسم الـ Pairplot\n",
        "# hue='reordered': عشان يلون النقاط (برتقالي للمكرر، أزرق للجديد)\n",
        "sns.pairplot(scatter_sample[cols_to_plot], hue='reordered', palette='husl', height=3)\n",
        "\n",
        "plt.suptitle('Pairwise Scatter Plots ', y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70efd15a",
      "metadata": {
        "id": "70efd15a"
      },
      "outputs": [],
      "source": [
        "#طيب اللون الاخضر هون بمثل المنتجات المعاد شرائها بنلاحظ بالرسمه انو دايما بالبدايه ببدا الزبون يجيب اغراضو الي  متعود عليها\n",
        "#بعدين بعد ما يجيبهم بجيب او بجرب اغراض جديده  مثل مابثمل اللون الزهري الي بمثل الاشياء الجديده\n",
        "#الرسمة الشمال تحت النقط بتوريك إن المنتجات المكررة الخضراء دايما محجوز الها المقاعد الأولى بالسلة  بغض النظر عن كم يوم مر.\n",
        "\n",
        "# الرسمة اليمين جبال بتأكد إن الناس بتتسوق بنظام أسبوعي أو شهري والمنتجات الجديدة والقديمة بتمشي على نفس هذا النظام."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcb4df0",
      "metadata": {
        "id": "cbcb4df0"
      },
      "source": [
        "• Time-of-day, day-of-week, and monthly seasonality plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb1c68a",
      "metadata": {
        "id": "2cb1c68a"
      },
      "outputs": [],
      "source": [
        "hourrr=Full_DataSet['order_hour_of_day'].value_counts()\n",
        "hourrr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff76183",
      "metadata": {
        "id": "aff76183"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(16, 7))\n",
        "\n",
        "\n",
        "# هون لازم قبل مانرسم نجهز الداتا لانها عباره عن مليون سطر فا لازم نرتبها اول حسب التكرار بعدين بنرتبها حسب الاندكس تصاعدني\n",
        "hour_counts = Full_DataSet['order_hour_of_day'].value_counts().sort_index()\n",
        "#لو تركناها هيك لاحظت رح تطلع معنا الرسمه الساعات من صفر ل 23 فا حيكون شوي مش مقروء الوضع فا افترحت لو بدنا نوري هالرسمه لاي حد لو نزبط الاندكس\n",
        "#ونخليه بنظام am و pm\n",
        "#بكون احسن ليش فقط عشان نخليه مقروء واريح للعين اكثر\n",
        "labels_of_hours = [\n",
        "    \"12 AM\", \"1 AM\", \"2 AM\", \"3 AM\", \"4 AM\", \"5 AM\",\n",
        "    \"6 AM\", \"7 AM\", \"8 AM\", \"9 AM\", \"10 AM\", \"11 AM\",\n",
        "    \"12 PM\", \"1 PM\", \"2 PM\", \"3 PM\", \"4 PM\", \"5 PM\",\n",
        "    \"6 PM\", \"7 PM\", \"8 PM\", \"9 PM\", \"10 PM\", \"11 PM\"\n",
        "]\n",
        "# x=labels_of_hours (الساعات)\n",
        "# y=hour_counts.values (عدد الطلبات)\n",
        "#هون اخترنا بار بلوت عشان سريع وبرضو برسملنا ال 24 عمود بسرعه عاليه\n",
        "sns.barplot(x=labels_of_hours, y=hour_counts.values, color='orange',)\n",
        "\n",
        "plt.title('Time of day Orders', fontsize=15)\n",
        "plt.xlabel('Hours: 12 AM - 11 PM  ', fontsize=12,color='blue')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684a6487",
      "metadata": {
        "id": "684a6487"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# رح يطلع معنا 7 أرقام من 0 ـ 6\n",
        "day_counts = Full_DataSet['order_dow'].value_counts().sort_index()\n",
        "\n",
        "#بنرتبهم كمان مره زي ماعملنا فوق\n",
        " # الويك إند عند الاجانب ببدا من السبت فهو رح يكون رقم صفر\n",
        "days_labels = [\"Saturday\",  \"Sunday\", \"Monday\",  \"Tuesday\", \"Wednesday\", \"Thursday\",\"Friday \"]\n",
        "\n",
        "\n",
        "\n",
        "sns.barplot(x=days_labels, y=day_counts.values, color=\"orange\")\n",
        "\n",
        "plt.title('Orders Day of Week ', fontsize=15)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Day', fontsize=12)\n",
        "\n",
        "plt.show()\n",
        "#بنلاحظ انو بالويكند اعلى طلبات وهاض المنطقي لانو الناس بتكون معطله"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc960f2a",
      "metadata": {
        "id": "dc960f2a"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "#  هون رح نستخدم هاض الكولوم لانو انسب اشي للمنثلي لانو مافي عنا بالداتا سيت كولم عن الاشهر\n",
        "days_of_month_counts = Full_DataSet['days_since_prior_order'].value_counts().sort_index()\n",
        "#رح نستخدم اللاين بلوت عشان نوضح التغيرات بشكل افضل\n",
        "sns.lineplot(x=days_of_month_counts.index, y=days_of_month_counts.values, marker='o', color='red', linewidth=2)\n",
        "\n",
        "# 3. تحسين المحاور\n",
        "plt.title('Monthly Seasonality', fontsize=14)\n",
        "plt.xlabel('Days Since Last Order', fontsize=12)\n",
        "plt.ylabel('Number of Orders', fontsize=12)\n",
        "plt.show()\n",
        "#بنلاحظ في قمه عند ال 7 و 30 يوم هاض يعني انو الزبون كل اسبوع غالبا بيجي يشتري وفي زباين بتيجي كل شهر او اكثر من شهر هاض كلو بنحط عند ال 30"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08a9fcc4",
      "metadata": {
        "id": "08a9fcc4"
      },
      "source": [
        "3: cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b6c7b6",
      "metadata": {
        "id": "d5b6c7b6"
      },
      "outputs": [],
      "source": [
        "del Full_DataSet[\"aisle\"]\n",
        "del Full_DataSet[\"department\"]\n",
        "\n",
        "print(Full_DataSet.isnull().sum())\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "#كان هدفي اشوف ال نان من هون بس ما زبطت ف شفتها من الملف نفسه\n",
        "Full_DataSet.head(10)\n",
        "#اللي بين معي انه ال نان بكون موجود لكل اول اوردير بطلبه المستخدم\\الزبون"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1419e778",
      "metadata": {
        "id": "1419e778"
      },
      "outputs": [],
      "source": [
        "def check_outliers(Full_DataSet):\n",
        "\n",
        "    out = [\"order_id\" , \"product_id\" , \"user_id\" , \"aisle_id\" , \"department_id\" , \"eval_set\"]\n",
        "    Outliers_DF = []\n",
        "    numeric_cols_all = Full_DataSet.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "\n",
        "    for i in numeric_cols_all:\n",
        "        if i not in out:\n",
        "            Outliers_DF.append(i)\n",
        "\n",
        "    col_length = len(Outliers_DF)\n",
        "    row = (col_length // 3) + 1\n",
        "    plt.figure(figsize=(20 , 5 * row))\n",
        "\n",
        "    #لاني بحتاج index + value خلال التكرار لرسم subplots.\n",
        "    #عشان هيك استخدمت enumerate\n",
        "    for i , col in enumerate(Outliers_DF):\n",
        "        plt.subplot(row , 3 , i + 1)\n",
        "\n",
        "        plot_data = Full_DataSet[col].dropna().sample(n = min(100000 , len(Full_DataSet)))\n",
        "\n",
        "        sb.boxplot(x = plot_data , color=\"lightblue\")\n",
        "        plt.title(col , fontsize = 12)\n",
        "        plt.xlabel(\" \")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45b92be",
      "metadata": {
        "id": "f45b92be"
      },
      "outputs": [],
      "source": [
        "#print(check_outliers(Full_DataSet))\n",
        "\n",
        "#من خلال الرسم بين معي انه الحد الفاصل بين اكبر قيمه والاوتلايرز هي 30\n",
        "Full_DataSet[\"add_to_cart_order\"] =  np.where(Full_DataSet[\"add_to_cart_order\"] > 30 , 30 , Full_DataSet[\"add_to_cart_order\"])\n",
        "#ونفس المبدأ بنطبق هون\n",
        "Full_DataSet[\"order_number\"] =  np.where(Full_DataSet[\"order_number\"] > 50 , 50 , Full_DataSet[\"order_number\"])\n",
        "\n",
        "print(Full_DataSet[\"add_to_cart_order\"].describe())\n",
        "print()\n",
        "print(Full_DataSet[\"order_number\"].describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb379afb",
      "metadata": {
        "id": "eb379afb"
      },
      "outputs": [],
      "source": [
        "Full_DataSet.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e13178",
      "metadata": {
        "id": "26e13178"
      },
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy = \"constant\" , fill_value = 0)\n",
        "#بعد ما تفرجت عالداتا من الاكسل , اكتشفت انه النان موجوده بس عند اول طلب للمستخدم , يعني ما عنده طلب مسبق\n",
        "#الموديل لما يشوف الصفر رح يقلك هاض المستخدم جديد , وعشان هيك ما عنده طلبات مسبقه\n",
        "Full_DataSet[\"days_since_prior_order\"] = imputer.fit_transform(Full_DataSet[\"days_since_prior_order\"].values.reshape(-1,1))\n",
        "\n",
        "DF = pd.DataFrame(Full_DataSet , columns = Full_DataSet.columns)\n",
        "\n",
        "#DF.to_csv('archive/full_instacart_data.csv', index=False)\n",
        "print(DF.isnull().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5372fa85",
      "metadata": {
        "id": "5372fa85"
      },
      "source": [
        "4: feature engineering (mandatory list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b414ff",
      "metadata": {
        "id": "46b414ff"
      },
      "outputs": [],
      "source": [
        "#total orders per user يعني كل مستخدم كم مره فات المحل واشترى يعني كم طلب عملو بشكل كامل مش كم منتج اشتراه بحياتو\n",
        "# طيب هون القروب باي بيجمعلي كل الداتا تبع كل يوزر ايدي لحال\n",
        "#بعدين بقله لكل يوزر بدي كولوم الاوردر نمبر تمام هسا الاورودر نمبر لكل زبون بمثل كل زبون كم طلب عملو لحد الان فا لو اخذتلو ال ماكس\n",
        "#لو اخذت الماكس رح تعطيني رقم اخر فاتوره والي بمثل عدد الطلبات الكلي لكل زبون طيب ممكن تسالني كان بمكاني اختار كاونت مش ماكس صح كلامك لكن\n",
        "#الداتا الي عندي او كولوم الاوردر نمبر يعني بحتوي على ارقام الطلبات مش عدد الطلبات فلو اخذت كاونت رح يطلعلي عدد المنتجات مش عدد الطلباتؤ\n",
        "#استخدمنا الريست اندكس عشان يرجعلي الداتا فريم مش سيرييز لانو قروب باي بيرجع سيرييز افتراضيا وبتخرب الداتا بصير اليوزر هو كولوم الاندكس\n",
        "user_total_orders = DF.groupby('user_id')['order_number'].max().reset_index()\n",
        "# 3. بنسمي العمود اسم واضح عشان ما نتخربط بعدين\n",
        "user_total_orders.columns = ['user_id', 'user_total_orders']\n",
        "\n",
        "print(user_total_orders.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e8efa8b",
      "metadata": {
        "id": "6e8efa8b"
      },
      "outputs": [],
      "source": [
        "#  average basket size\n",
        "#هذه الميزة بتحدد القدرة الشرائية Purchasing Power\n",
        "#وبتحدد نمط التسوق عند الزبون يعني بعرف الزبون الي عادته يشتري مثلا 50 غرض هاض زبون بمتوسط سلة كبيرة فا بهمني انو المودل ممكن\n",
        "#بهمني انو المودل ممكن يشوف هالشي ويستفيد منه بحيث لو كان شاري 3 اغراض بس يضل يقترح عليه لانو هالزبون من عادتو يشتري كثير\n",
        "\n",
        "#============================================================================================================\n",
        "\n",
        "#طيب السواال كيف بنحسبها ؟ بنحسبها عن طريق انو بنجيب لكل زبون كم المنتجات الي اشتراها بشكل كلي\n",
        "#بعدين بنجيب كم طلب عملو بشكل كلي\n",
        "#بعدين بنقسم المجموع على العدد\n",
        "#============================================================================================================\n",
        "#هون اولا عشان نجيب مجموع المنتجات الي اشتراها كل زبون بنجيب كولوم البروودكت ايدي وبنعمللو كاونت هيك بنعد كل المنتجات الي اشتراها\n",
        "#بعدين بنجيب كولوم الاوردر نمبر وبناخد الماكس زي ما شرحنا فوق عشان نعرف كم طلب عملو بشكل كلي\n",
        "\n",
        "#واستخدمنا ميثود الاقريقيت الي بتتيحلي اعمل اكشنز متعدده على كولمز مختلفه بنفس الوقت بدل ما اعمل قروب باي مرتين\n",
        "basket_data = DF.groupby('user_id').agg({ 'product_id':'count', 'order_number': 'max'}).reset_index()\n",
        "\n",
        "# مجرد تسميت الكولمز بشكل واضح بس\n",
        "basket_data.columns = ['user_id', 'total_items_bought', 'total_orders_made']\n",
        "\n",
        "#الحسبه\n",
        "basket_data['avg_basket_size'] = basket_data['total_items_bought'] / basket_data['total_orders_made']\n",
        "\n",
        "print(basket_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339a4785",
      "metadata": {
        "id": "339a4785"
      },
      "outputs": [],
      "source": [
        "#  User-Level Features full\n",
        "#هون لازم نعمل سورت اول عشان اخر مطلوب اللاست لاخر طلب فا ممكن يكون اخر طلب الي بجيبو مش هو اخر طلب عملو\n",
        "#بس الجهاز عندي بتحملش\n",
        "# Full_DataSet.sort_values(['user_id', 'order_number'], inplace=True)\n",
        "user_features = DF.groupby('user_id').agg({\n",
        "      # 1. Total #Orders\n",
        "    'order_number': 'max',\n",
        "    # 2. هاض بساعدنا نعرف كم منتج اشتراها كل زبون بشكل كلي\n",
        "     'product_id': 'count',\n",
        "    # 3. Reorder Ratio\n",
        "    'reordered': 'mean',\n",
        "\n",
        "   #هون حسبنا اخر مطلوبين بخطوه وحده بدل ما نعمل قروب باي مرتين\n",
        "    'days_since_prior_order': ['mean', 'last']\n",
        "}).reset_index()\n",
        "\n",
        "\n",
        "user_features.columns = ['user_id',  'user_total_orders',  'user_total_items', 'user_reorder_ratio','user_avg_days_between', 'user_days_since_last_order'  ]\n",
        "\n",
        "# Basket Size\n",
        "user_features['user_avg_basket_size'] = user_features['user_total_items'] / user_features['user_total_orders']\n",
        "\n",
        "#هاض العامود مش ضروري بعد ما حسبنا الافريج مابنحتاجه\n",
        "del user_features['user_total_items']\n",
        "\n",
        "display(user_features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba2c5a4",
      "metadata": {
        "id": "dba2c5a4"
      },
      "source": [
        "Product-level features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac0a591",
      "metadata": {
        "id": "4ac0a591"
      },
      "outputs": [],
      "source": [
        "# --- Product-Level Features ---\n",
        "\n",
        "# هالمره التجميع رح يكون حسب المنتج مش حسب اليوزر\n",
        "product_features = DF.groupby('product_id').agg({\n",
        "\n",
        "    #  Popularity\n",
        "    #هون شو ممكن يفيدنا هون قصدو من البوبولاريتي انه نعرف كم مرة انباع هاد المنتج ممكن من خلاله نعرف  اكثر المنتجات بينباع او وين اقل منتج\n",
        "    'user_id': 'count',\n",
        "    # ليش user_id؟ وليش count؟\n",
        "    # عشان نعد كم زبون اشترى هاد المنتج لانو كل سطر بيمثل  عنا عمليه شراء.\n",
        "    #====================================================================================\n",
        "    #  Reorder Rate\n",
        "    #هون بنحسب نسبة اعادة الطلبات للمنتج\n",
        "    'reordered': 'mean',\n",
        "    #هاي واضحه من اسمها بدهاش اشي\n",
        "    #====================================================================================\n",
        "    # Average  Position\n",
        "    #طيب ممكن تسالني ليش اخترنا هاض الكولوم بالذات رح لانه بمثل ترتيب المنتج داخل السلة فلو اخذنا المين رح يعطينا ترتيب المنتج داخل السلة زي ماهو طالب\n",
        "    'add_to_cart_order': 'mean'\n",
        "\n",
        "}).reset_index()\n",
        "\n",
        "product_features.columns = [ 'product_id', 'product_total_purchases', 'product_reorder_rate', 'product_avg_cart_position' ]\n",
        "\n",
        "display(product_features.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55efd949",
      "metadata": {
        "id": "55efd949"
      },
      "source": [
        "User×Product interaction features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77231b20",
      "metadata": {
        "id": "77231b20"
      },
      "outputs": [],
      "source": [
        "#UserxProduct Interaction Features\n",
        "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "\n",
        "#دمجتها من هسا عشان اجرب اشغل user_days_since_last_order لانها مش جزء من الداتا الاساسيه ف ما اشتغل الكود\n",
        "DF = DF.merge(user_features[['user_id' , 'user_days_since_last_order']] , on = 'user_id', how = 'left')\n",
        "\n",
        "prior_df = DF[DF['eval_set'] == 'prior'].copy()\n",
        "# uxp_features = DF.groupby(['user_id', 'product_id']).agg({\n",
        "\n",
        "uxp_features = prior_df.groupby(['user_id', 'product_id']).agg({\n",
        "# Total Purchases of Product by User\n",
        "#هون عشان نجيبها كان ممكن نستخدم اي عمود بس بدنا واحد  بس المهم نعد كم مرة هاد الزبون اشترا هاد المنتج\n",
        "#طب ليش اخترت هاض الكولوم بالذات اختصارا للوقت بس عشان المطلوب الثاني رح ارجع اطلبه تمام\n",
        "#====================================================================================\n",
        "\n",
        "# Reorder Probability of Product by User\n",
        "#هون استخدمنا برضو بنفس العامود بس اخذنا المين عشان يعطينا النسبة الي طلبها الزبون من هاد المنتج\n",
        "    'reordered': ['count', 'mean'],\n",
        "\n",
        "    'user_days_since_last_order': 'max'    # هاي مش عارف اعملها علقت كيف ممكن نجيب ال   days since last purchase by     user_days_since_last_order  days_since_prior_order\n",
        "}).reset_index()\n",
        "\n",
        "# 2. تسمية الأعمدة بشكل واضح (عشان الدكتور يفهم كل عمود شو هو)\n",
        "uxp_features.columns = [\n",
        "    'user_id',\n",
        "    'product_id',\n",
        "    'uxp_total_bought',\n",
        "    'uxp_reorder_ratio',\n",
        "    'uxp_days_last_order_'\n",
        "]\n",
        "\n",
        "display(uxp_features.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc48c8af",
      "metadata": {
        "id": "bc48c8af"
      },
      "source": [
        "Temporal features: hour/day/month/year, season, holiday flags (if available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "796fe777",
      "metadata": {
        "id": "796fe777"
      },
      "outputs": [],
      "source": [
        "# طيب هون المطلوب منا بالتيمبورال فيشترز انو نزبط الوقت مثلا نخليه صبح ومسا ومثلا الايام نقسمها ايام عاديه وايام عطله والشهر\n",
        "#بس مبادايا الشهر والسنه والموسم مابنقدر لانو مامعنا معلومات عنها  في رح نكتفي بالساعات واليوم\n",
        "#نبدا بالساعات هسا التوقيت  عنا من 0 ل 23 فممكن نقسمهم لثلاث فترات\n",
        "def time_of_day(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Midnight'\n",
        "DF['time_of_day'] = DF['order_hour_of_day'].apply(time_of_day)\n",
        "print(DF['time_of_day'].head(10))\n",
        "print(\"==\"*40)\n",
        "#طيب قلنا للايام بنقسمها لايام عاديه وايام عطله\n",
        "#وزي مابنعرف بالداتا الي عندي السبت والاحد همه العطله فرقمهم بكون 0 و 1\n",
        "#هون اختصارا على حالنا لقدام خليت الكولوم الجديد عباره عن ارقام 0 و 1 بدل ما اخليها نصوص عشان اسهل التعامل معها بعدين\n",
        "def day_type(day):\n",
        "    if (day == 0) or (day == 1):\n",
        "        return 1 # طبعا واحد بتعني انها شسمو ويكند\n",
        "    else:\n",
        "        return 0\n",
        "DF['is_weekend'] = DF['order_dow'].apply(day_type)\n",
        "print(DF['is_weekend'].head(30))\n",
        "#حطيت 30 لانو خفت كلها صفار ههههه يسعد ربك\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d0e9eb",
      "metadata": {
        "id": "31d0e9eb"
      },
      "outputs": [],
      "source": [
        "# --- 5. Aggregations over Windows (Last 3 Orders) - بدون Lambda ---\n",
        "\n",
        "# 1. تجهيز الداتا: بنحسب حجم السلة لكل طلب (جدول صغير وخفيف)\n",
        "orders_summary = DF.groupby(['user_id', 'order_number']).size().reset_index(name='basket_size')\n",
        "\n",
        "# 2. الترتيب (مهم جداً): عشان لما نقول \"آخر 3\" يكونوا عنجد آخر 3 زمنياً\n",
        "orders_summary = orders_summary.sort_values(['user_id', 'order_number'])\n",
        "\n",
        "# 3. تعريف الفنكشن العادي (بدل اللمدا)\n",
        "# هذا الفنكشن بياخذ عمود أرقام، وبحسب المتوسط المتحرك لآخر 3 قيم\n",
        "def calculate_last_3_avg(series):\n",
        "    # window=3: يعني خذ 3 قيم\n",
        "    # min_periods=1: يعني حتى لو الزبون عنده طلب واحد بس، احسبله المعدل (ما ترجع Null)\n",
        "    return series.rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "# 4. تطبيق الفنكشن على كل زبون\n",
        "# transform: بتمسك الفنكشن اللي كتبناه فوق، وبتطبقه على كل \"مجموعة\" (زبون)\n",
        "orders_summary['rolling_avg_3_orders'] = orders_summary.groupby('user_id')['basket_size'].transform(calculate_last_3_avg)\n",
        "\n",
        "# 5. النتيجة النهائية\n",
        "# احنا بهمنا \"آخر وضع\" وصله الزبون، فبناخذ آخر سطر لكل زبون\n",
        "user_window_features = orders_summary.groupby('user_id').last().reset_index()\n",
        "\n",
        "# ترتيب وتنظيف الجدول النهائي\n",
        "user_window_features = user_window_features[['user_id', 'rolling_avg_3_orders']]\n",
        "user_window_features.columns = ['user_id', 'u_avg_basket_last_3']\n",
        "\n",
        "display(user_window_features.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba4a0f9",
      "metadata": {
        "id": "8ba4a0f9"
      },
      "source": [
        "✅ تم حساب (Rolling Window) باستخدام فنكشن عادي!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff55361",
      "metadata": {
        "id": "5ff55361"
      },
      "source": [
        "At least one engineered non-linear feature : log transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fff0338",
      "metadata": {
        "id": "8fff0338"
      },
      "outputs": [],
      "source": [
        "# افترض إنك حسبت user_features في الخطوة الأولى\n",
        "# بدنا نحول عمود \"عدد الطلبات\" باستخدام اللوغاريتم\n",
        "#هسا\n",
        "print(\"=\")\n",
        "#هون طقعت ديسبلاي لانها اوضح بس من برنت\n",
        "user_features['u_total_orders_log'] = np.log(user_features['user_total_orders'])\n",
        "\n",
        "# حطيناهم جمب بعض عشان تشوف الفرق\n",
        "print(\"user_total_orders trasform\")\n",
        "display(user_features[['user_total_orders', 'u_total_orders_log']].head(90))\n",
        "#\n",
        "#كمان فيتشر ثاني نعملو مش غلط\n",
        "product_features['p_total_purchases_log'] = np.log(product_features['product_total_purchases'])\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(\"product_ total_purchases transform\")\n",
        "\n",
        "display(product_features[['product_total_purchases', 'p_total_purchases_log']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5fa79c",
      "metadata": {
        "id": "1f5fa79c"
      },
      "outputs": [],
      "source": [
        "#بدي اسوي نسخه للاحتياط , هسا انا صرت بمرحله حرجه شوي و اي خطأ ممكن يدمر الداتا كامله ف الاحتياط واجب\n",
        "final_df = DF.copy()\n",
        "\n",
        "final_df = final_df.merge(user_features , on = 'user_id' , how = 'left')\n",
        "#لاني دمجتهم من قبل , عملت هالحركه عشان اتأكد ما يصير عندي اي تكرار\n",
        "final_df = final_df.drop(columns=[col for col in user_features.columns if col in final_df.columns and col != 'user_id'])\n",
        "\n",
        "final_df = final_df.merge(user_features , on = 'user_id' , how = 'left')\n",
        "final_df = final_df.merge(product_features , on = 'product_id' , how = 'left')\n",
        "final_df = final_df.merge(uxp_features , on = ['user_id' , 'product_id'] , how = 'left')\n",
        "final_df = final_df.merge(user_window_features , on = 'user_id' , how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f347b8",
      "metadata": {
        "id": "56f347b8"
      },
      "outputs": [],
      "source": [
        "print(final_df.shape)\n",
        "print(DF.shape)\n",
        "print()\n",
        "print(final_df.isnull().sum())\n",
        "final_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87833c0",
      "metadata": {
        "id": "b87833c0"
      },
      "outputs": [],
      "source": [
        "#بدي اختار الاعمده اللي لازم ازبط الميموري الهم\n",
        "final_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdac7c25",
      "metadata": {
        "id": "fdac7c25"
      },
      "outputs": [],
      "source": [
        "def reduce_memory_FE(df , col_name):\n",
        "\n",
        "    for col in col_name:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if \"int\" in str(col_type):\n",
        "            df[col] = df[col].astype(\"int32\")\n",
        "\n",
        "        elif \"float\" in str(col_type):\n",
        "            df[col] = df[col].astype(\"float32\")\n",
        "\n",
        "        elif col_type == \"object\":\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae4c2a9",
      "metadata": {
        "id": "fae4c2a9"
      },
      "outputs": [],
      "source": [
        "col_name = [\"uxp_reorder_ratio\" , \"u_avg_basket_last_3\" , \"uxp_total_bought\" , \"p_total_purchases_log\" , \"product_avg_cart_position\" , \"product_reorder_rate\" , \"product_total_purchases\" , \"u_total_orders_log\" , \"user_avg_basket_size\" , \"user_reorder_ratio\" , \"is_weekend\" , \"time_of_day\"]\n",
        "\n",
        "final_df = reduce_memory_FE(final_df, col_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3da99f4",
      "metadata": {
        "id": "e3da99f4"
      },
      "outputs": [],
      "source": [
        "final_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacc11e6",
      "metadata": {
        "id": "dacc11e6"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.drop(columns=['user_days_since_last_order_x' ,\n",
        "                                  'user_days_since_last_order_y'] ,\n",
        "    errors='ignore'\n",
        ")\n",
        "\n",
        "#ما عرفت اقلل الذاكره اكثر من هيك 😢\n",
        "final_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ca72fc",
      "metadata": {
        "id": "61ca72fc"
      },
      "outputs": [],
      "source": [
        "#final_df.to_csv('archive/new_instacart_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4a7bcc",
      "metadata": {
        "id": "1c4a7bcc"
      },
      "source": [
        "5: Dimensionality & collinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388e21eb",
      "metadata": {
        "id": "388e21eb"
      },
      "outputs": [],
      "source": [
        "id_cols = ['order_id' , 'user_id' , 'product_id' , 'aisle_id' , 'department_id']\n",
        "low_cols = [\"department_id\" , \"order_dow\" , \"time_of_day\"]\n",
        "high_cols = [\"user_id\" , \"product_id\" , \"aisle_id\"]\n",
        "\n",
        "final_df[high_cols] = final_df[high_cols].astype(str)\n",
        "#هالحركه سويتها بعد ما متت وانا بحلل الكود بعد ما طلعلي التنبيه هاض\n",
        "#Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
        "#لما راجعت الانكوديرز تذكرت انه التارقيت ما بشتغل غير مع نصوص والاعمده اللي انا معطيه اياهم رقميات\n",
        "\n",
        "target_col = \"reordered\"\n",
        "Frequency_col = \"product_name\"\n",
        "\n",
        "#عدد الاعمده كبير جدا فقلت بعمل لوب + استثناءات عشان اريح راسي\n",
        "num_cols = (final_df.drop(columns=[target_col]).select_dtypes(include=[\"int32\" , \"float32\"]).columns.tolist())\n",
        "num_cols = [c for c in num_cols if c not in id_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "653b1743",
      "metadata": {
        "id": "653b1743"
      },
      "outputs": [],
      "source": [
        "#بدي استخدم VIF , هاض عباره عن فنكشن رياضي ببين قديش في ارتباط وتكرار بين الاعمده نفسهم\n",
        "#الهدف منه اني اشوف شو في اعمده فيهم تشابه كبير وبقدمو نفس المعلومه تقريبا عشان احذف واحد منهم\n",
        "\n",
        "SAMPLE_SIZE = 50000\n",
        "V = final_df[num_cols].sample(n=SAMPLE_SIZE , random_state = 42)\n",
        "\n",
        "vif = pd.DataFrame()\n",
        "vif[\"feature\"] = V[num_cols].columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(V.values , i) for i in range(V.shape[1])]\n",
        "\n",
        "#تحت 5 ممتاز\n",
        "#بين ال 6 وال 10 مقبول\n",
        "#اكثر من هيك بدك تشوف شو و وين في ترابط غير مهم وتبلش تحذف\n",
        "#ال inf حذف مباشره\n",
        "\n",
        "vif.sort_values(\"VIF\" , ascending = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1eb004",
      "metadata": {
        "id": "6e1eb004"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8 , 5))\n",
        "plt.scatter(vif[\"VIF\"] , vif[\"feature\"])\n",
        "plt.axvline(10 , color = 'red' , linestyle = '--')\n",
        "plt.xlabel(\"VIF\")\n",
        "plt.title(\"VIF Scatter Plot\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f310ebf",
      "metadata": {
        "id": "0f310ebf"
      },
      "outputs": [],
      "source": [
        "#حذفت القيم اللانهائيه , والقيم اللي فيها النسبه عاليه , لانهم بدلو على تكرار وتشابه المعلومات , يعني لو خليتهم كلهم زي كأني مكرر نفس العامود ما فرقت\n",
        "drop_cols = [\n",
        "    \"user_days_since_last_order\" ,\n",
        "    \"uxp_days_last_order_\" ,\n",
        "    #u_total_orders_log كنت بدي اخليه بما انه تعبنا عليه بمرحلة الهندسه بس النسبه فيه كانت كثير عاليه :(\n",
        "    \"user_total_orders\" , \n",
        "    \"user_reorder_ratio\" ,\n",
        "    \"product_reorder_rate\" ,\n",
        "    \"product_avg_cart_position\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "final_df = final_df.drop(columns = drop_cols , errors=\"ignore\")\n",
        "\n",
        "#صار عندي ايرور بالبريبروسيسر لانه الداتا صار فيها عدم تطابق بعد الدروب ف بدي ارجع انسخ الاعمده كمان مره\n",
        "num_cols = (final_df.drop(columns=[target_col]).select_dtypes(include=[\"int32\" , \"float32\"]).columns.tolist())\n",
        "num_cols = [c for c in num_cols if c not in id_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "542dcb37",
      "metadata": {
        "id": "542dcb37"
      },
      "source": [
        "6+7: preprocessing and scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50781d9a",
      "metadata": {
        "id": "50781d9a"
      },
      "outputs": [],
      "source": [
        "def best_params_for_TE(DF , high_cols):\n",
        "\n",
        "    #المشكله اللي صارت انه لما اقسم الداتا , لسا ما شغل البريبروسيسر عليها ف لسا مش كل الفيتشرز تحولو لقيم رقميه\n",
        "    del DF[\"product_name\"]\n",
        "\n",
        "    unique_users = DF['user_id'].unique()\n",
        "    selected_users = np.random.choice(unique_users , size = 3000 , replace=False)\n",
        "    #رفعت عدد العينات ل 5000 والكود طول لتنه اشتغل ف عشان هيك قللتهم ك حل وسط وهون طلع معي افضل نتيجع بعد عدة تكرارات\n",
        "\n",
        "    df_check = DF[DF['user_id'].isin(selected_users)].copy()\n",
        "\n",
        "    df_check[high_cols] = df_check[high_cols].astype(str)\n",
        "\n",
        "    xc = df_check.drop(\"reordered\", axis=1)\n",
        "    yc = df_check['reordered']\n",
        "\n",
        "    xc_train , xc_test , yc_train , yc_test = train_test_split(xc , yc , test_size = 0.2 , random_state = 42)\n",
        "\n",
        "    pipeline = make_pipeline(\n",
        "        ce.TargetEncoder(cols=high_cols) ,\n",
        "        RandomForestClassifier(\n",
        "            n_estimators = 100,  # عدد الأشجار\n",
        "            max_depth = 10 ,      # عمق الشجرة (عشان ما يوخذ وقت طويل)\n",
        "            random_state = 42 ,\n",
        "            n_jobs = -1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    param_grid = {\n",
        "        'targetencoder__smoothing': [1, 10, 50] ,\n",
        "        'targetencoder__min_samples_leaf': [1, 20]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator = pipeline ,\n",
        "        param_grid = param_grid ,\n",
        "        cv = 3 ,\n",
        "        scoring = 'roc_auc' ,\n",
        "        verbose = 1\n",
        "    )\n",
        "\n",
        "    print(\"\\n\")\n",
        "    grid_search.fit(xc_train, yc_train)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print(f\"Best ROC_AUC: {grid_search.best_score_:.4f}\")\n",
        "    print(\"Best Parameters:\")\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "    #تجربه فاشله لتحديد افضل المعاملات\n",
        "    #السبب انه عدد العينات كبير نسبيا ومدامني بجرب كل رقم بلوب لحال ف رح يوخذ مني وقت كبيييييير جدااااا\n",
        "    '''\n",
        "    smoothing_op = [1, 2, 10, 20, 50, 100]\n",
        "    leaf_op = [1, 5, 10, 20, 50]\n",
        "    Kfold_op = [3, 5, 10]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for k in Kfold_op:\n",
        "        current_kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "        for sm in smoothing_op:\n",
        "            for leaf in leaf_op:\n",
        "\n",
        "                encoder = ce.TargetEncoder(\n",
        "                    cols=high_cols,\n",
        "                    min_samples_leaf=leaf,\n",
        "                    smoothing=sm\n",
        "                )\n",
        "\n",
        "                model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "                pipeline = make_pipeline(encoder, model)\n",
        "\n",
        "                try:\n",
        "                    scores = cross_val_score(pipeline, xc, yc, cv=current_kf, scoring=\"roc_auc\")\n",
        "                    mean_auc = scores.mean()\n",
        "                    std_auc = scores.std()\n",
        "\n",
        "                    results.append({\n",
        "                        'n_splits': k,\n",
        "                        'smoothing': sm,\n",
        "                        'min_samples_leaf': leaf,\n",
        "                        'auc_mean': mean_auc,\n",
        "                        'auc_std': std_auc\n",
        "                    })\n",
        "\n",
        "                    print(f\"K={k}, Smooth={sm}, Leaf={leaf} -> AUC: {mean_auc:.4f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error at K={k}, Smooth={sm}, Leaf={leaf}: {e}\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    sorted_results = results_df.sort_values(by='auc_mean', ascending=False)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"TOP 3 PARAMETER COMBINATIONS:\")\n",
        "    print(sorted_results.head(3))\n",
        "    print()\n",
        "\n",
        "    if not sorted_results.empty:\n",
        "        best_params = sorted_results.iloc[0]\n",
        "        print(f\"\\nthe best:\\nSmoothing: {best_params['smoothing']}\\nMin Samples Leaf: {best_params['min_samples_leaf']}\\nK-Fold Splits: {int(best_params['n_splits'])}\")\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6791e61a",
      "metadata": {
        "id": "6791e61a"
      },
      "outputs": [],
      "source": [
        "def which_scaler(num_cols):\n",
        "\n",
        "    SAMPLE_SIZE = 500000\n",
        "    df_sampled = DF.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
        "\n",
        "    fig, axes = plt.subplots(len(num_cols), 2, figsize=(14, 4 * len(num_cols)))\n",
        "    plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "    for i, col in enumerate(num_cols):\n",
        "\n",
        "        sns.histplot(\n",
        "            df_sampled[col],\n",
        "            kde=True,\n",
        "            ax=axes[i, 0],\n",
        "            color='skyblue',\n",
        "            edgecolor='black',\n",
        "            line_kws={'linewidth': 3}\n",
        "        )\n",
        "        axes[i, 0].set_title(f'Distribution of {col}', fontsize=12)\n",
        "        axes[i, 0].set_xlabel(col, fontsize=10)\n",
        "        axes[i, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "        stats.probplot(\n",
        "            df_sampled[col].dropna(),\n",
        "            dist=\"norm\",\n",
        "            plot=axes[i, 1]\n",
        "        )\n",
        "        axes[i, 1].set_title(f'Q-Q Plot of {col}', fontsize=12)\n",
        "        axes[i, 1].set_xlabel('Theoretical Quantiles (Normal)', fontsize=10)\n",
        "        axes[i, 1].set_ylabel('Sample Quantiles', fontsize=10)\n",
        "\n",
        "    plt.savefig('numerical_features_distribution_analysis.png', bbox_inches='tight')\n",
        "    print(\"تم حفظ تحليل التوزيعات في 'numerical_features_distribution_analysis.png'\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b074bb0",
      "metadata": {
        "id": "9b074bb0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "num_cols = [\"order_hour_of_day\", \"days_since_prior_order\", \"add_to_cart_order\", \"order_number\"]\n",
        "low_cols = [\"department_id\", \"order_dow\"]\n",
        "high_cols = [\"user_id\", \"product_id\", \"aisle_id\"]\n",
        "target_col = \"reordered\"\n",
        "Frequency_col = \"product_name\"\n",
        "\n",
        "#الرسم ببينلك انه التوزيع مبعثر وغير طبيعي وبحتوي على اوتلايرز كثيييييررر ,  عشان هيك كان افضل خيار استخدام\n",
        "#SD لانه افضل بالتعامل مع الاوتلايرز وما بتأثر فيهم بشكل واضح وسلبي\n",
        "\n",
        "#which_scaler(num_cols)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5d4e34",
      "metadata": {
        "id": "6c5d4e34"
      },
      "outputs": [],
      "source": [
        "print(\"Numeric Columns:\" , len(num_cols))\n",
        "print(num_cols)\n",
        "\n",
        "#بعد اكثر من تجربه افضل ناتج طلعلي كن زي اللي حطيتهم بالبريبروسيس\n",
        "#best_params_for_TE(final_df , high_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf024c3d",
      "metadata": {
        "id": "cf024c3d"
      },
      "source": [
        "8: Imbalanced data handling (classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba744831",
      "metadata": {
        "id": "ba744831"
      },
      "outputs": [],
      "source": [
        "final_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "921d2b17",
      "metadata": {
        "id": "921d2b17"
      },
      "source": [
        "اللي قاعد بصير هسا اني نقلت الانكوديرز لبعد مرحلة الهندسه , ليش ؟\n",
        "لانه بكل بساطه احترت كيف فعليا المفروض نسوي انكودينق للاعمده الجديده اللي عملناهم بعد الهندسه وكان هاض الحل الوحيد المنطقي ++ ما بزبط اسوي\n",
        "VIF وانا عامل انكودينق"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4f4a9a",
      "metadata": {
        "id": "2a4f4a9a"
      },
      "outputs": [],
      "source": [
        "\"\"\"#الموضوع هاض عباره عن وجود عدم توازين في تصنيف البيانات , مثلا الكلاس الاول نسبته اعلى من الثاني , ف هيك الموديل بصير يعتمد عالاول ويهمل الثاني\n",
        "#ف هيك بصير الموديل فاشل باكتشاف الانماط الجديد\n",
        "\n",
        "#تجهيز بيانات التصنيف#\n",
        "\n",
        "#جبت عينات من الداتا لانه اللابتوب شلف عندي لما اشتغلت عالداتا كامله , حاولت اكبر نسبة العينات قد ما بقدر\n",
        "SAMPLE_SIZE = 1000000\n",
        "final_sample = final_df.sample(n = SAMPLE_SIZE , random_state = 42)\n",
        "\n",
        "x_c = final_sample.drop(\"reordered\", axis=1)\n",
        "y_c = final_sample[\"reordered\"]\n",
        "\n",
        "xc_train , xc_test , yc_train , yc_test = train_test_split(x_c , y_c , test_size = 0.3 , stratify = y_c , random_state = 42)\n",
        "\n",
        "#طلعلي هيك SettingWithCopyWarning\n",
        "xc_train = xc_train.copy()\n",
        "xc_test  = xc_test.copy()\n",
        "\n",
        "#هالحركه سويتها بعد ما متت وانا بحلل الكود بعد ما طلعلي التنبيه هاض\n",
        "#Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
        "#لما راجعت الانكوديرز تذكرت انه التارقيت ما بشتغل غير مع نصوص والاعمده اللي انا معطيه اياهم رقميات\n",
        "\n",
        "train_cls = preprocessor.fit_transform(xc_train , yc_train)\n",
        "test_cls = preprocessor.transform(xc_test)\n",
        "\n",
        "''''''''''''''''''''''''''''''''''''''''''''''''\n",
        "#طيب هون بالتايم اوير سبلتنق المفروض نقسم الداتا حسب التوقيت طيب ليش ؟\n",
        "#الداتا الجديده بقصد فيها الي صارت اخر شي او الطلبات الجديده او يعني الي اخر اشي عمله الزبون\n",
        "#اولا عشان التقسيم العشوائي بالداتا تبعتنا بعمللنا مشكله شو المشكله هي انو المودل لما يجي يعمل فيت للداتا رح يعمل فيت ويتعلم على داتا جدي\n",
        "#بعد مايعمل فيت على داتا ممكن تكون جديده ممكن يكون التيست عنا داتا قديمه فهيك المودل رح يكون متعلم من داتا الجديده وهيك بغش حالو\n",
        "#وهيك بغش حالو  وبجيب اكيورسي عاليه كذابه تمام فا لازم نحل هالمشكله\n",
        "#طيب لازم نعمل سامبل سايز لانو الامور هيك حتصير كثير كبيره بدونو ومع السموت هاض الداتا بتتضاعف ومابتحمل الرام\n",
        "#ولازن مانتسخدم السامبلز سايز عادي عشان مانخرب توزيع الداتا افضل يعني\n",
        "#لو استحدمت سامبلنق عادي رح نفقد كثير داتا يعني مثلا ممكن ليوز معين نفقد الاوردر تاعو رقم 3 او اورد رقم 5\n",
        "#بينما باليوزر سامبلنق انا باخذ كل الداتا لاشخاص اقل بس\n",
        "#  User Sampling\n",
        "unique_users = final_df['user_id'].unique()\n",
        "\n",
        "# هون السايز خليتو انتجر لانو مثيود الرانودوم تشويس لازمها عدد صحيح مش عشري\n",
        "new_size = int(len(unique_users) * 0.05)\n",
        "\n",
        "#  اخترت اليوزرز بشكل عشوائي بدون تكرار\n",
        "my_users = np.random.choice(unique_users, size=new_size , replace=False)\n",
        "\n",
        "#  فلترت الداتا خليت بس الأسطر اللي بتخص اليوزرز اللي اخترتهم\n",
        "final_sample = final_df[final_df['user_id'].isin(my_users)].copy()\n",
        "# هسا لازم نتاكد انو الداتا مرتبه\n",
        "\n",
        "final_sample = final_sample.sort_values(by=[\"user_id\",\"order_number\"])\n",
        "\n",
        "# هسا بعد مارتبنا مطلوب منا نعمل نقسم الداتا طيب شو في طريقه نقسمها في اكثر من طريقه لكن كلهم معقدين فا هاي ابسط اشي لحالتنا\n",
        "#الي هي انو نعمل كولوم جديده نحط فيه الداتا الجديده تمام وبنقسم على ااساسها\n",
        "#هون الهدف نعمل كولوم جديد فيه قيم صح وخطا الان القيم الصح او الترو بنحطها للتيست وقيم الفولس بنحطها للترين تمام ها\n",
        "#الان كيف نعمل هاي القصه من خلال انو بنمرر كولم الاوردر نمبر تمام وبنقارن كل اوردر لكل يوزر مع الماكس او الطلب الاخير لهاض اليوز\n",
        "#فلو كان اليوزر الطلب الي بنقارنه هو نفسه الطلب الاخير اله حيرجع ترو تمام\n",
        "#هون جربت استخدم بدون ترانسفورم لاني ماكنت اعرفها طلع عندي ايرور والايرور لانو لو خليت الميثود بدونها رح ترجعلي داتا مضغوطه لانو بجيب الزبده لكل\n",
        "#لانو بجيب الزبده لكل كولوم فالحل ترانسفورم عشان نطبق الماكس عكل نقطه داتا عندي تم ؟ تم\n",
        "\n",
        "final_sample[\"last__orders\"]= final_sample[\"order_number\"] == final_sample.groupby(\"user_id\")[\"order_number\"].transform(\"max\")\n",
        "#هون قسمنا زي ما مطلوب منا\n",
        "train_df = final_sample[final_sample['last__orders'] == False]\n",
        "test_df = final_sample[final_sample['last__orders'] == True]\n",
        "#===\n",
        "\n",
        "# هون بدنا نقسم الداتا تاعت التريت بدنا نعطي الاكس الكولمز المهمه بس ونشيل كولوم التارقيت منها\n",
        "not_for_X_columns = ['reordered', 'eval_set', 'last__orders',  'order_id',]\n",
        "X_train = train_df.drop(columns=not_for_X_columns, errors='ignore')\n",
        "y_train = train_df['reordered']\n",
        "\n",
        "#نفس الشي للتيست\n",
        "X_test = test_df.drop(columns=not_for_X_columns, errors='ignore')\n",
        "y_test = test_df['reordered']\n",
        "\n",
        "# Fit على التدريب فقط\n",
        "train_cls = preprocessor.fit_transform(X_train , y_train)\n",
        "test_cls = preprocessor.transform(X_test)\n",
        "\"\"\"\n",
        "\n",
        "target_users_count = 4000\n",
        "\n",
        "unique_users = final_df['user_id'].unique()\n",
        "\n",
        "np.random.seed(42)\n",
        "my_users = np.random.choice(unique_users , size=target_users_count , replace=False)\n",
        "\n",
        "final_sample = final_df[final_df['user_id'].isin(my_users)].copy()\n",
        "\n",
        "final_sample = final_sample.sort_values(by=[\"user_id\", \"order_number\"])\n",
        "final_sample[\"last__orders\"] = final_sample[\"order_number\"] == final_sample.groupby(\"user_id\")[\"order_number\"].transform(\"max\")\n",
        "\n",
        "train_df = final_sample[final_sample['last__orders'] == False]\n",
        "test_df = final_sample[final_sample['last__orders'] == True]\n",
        "\n",
        "not_for_X_columns = ['reordered' , 'eval_set' , 'last__orders' ,  'order_id' , 'uxp_reorder_ratio', 'add_to_cart_order'   , 'order_number']\n",
        "\n",
        "xc_train = train_df.drop(columns=not_for_X_columns , errors='ignore')\n",
        "yc_train = train_df['reordered']\n",
        "\n",
        "xc_test = test_df.drop(columns = not_for_X_columns , errors = 'ignore')\n",
        "yc_test = test_df['reordered']\n",
        "\n",
        "real_num_cols = [c for c in num_cols if c not in not_for_X_columns]\n",
        "real_low_cols = [c for c in low_cols if c not in not_for_X_columns]\n",
        "real_high_cols = [c for c in high_cols if c not in not_for_X_columns]\n",
        "\n",
        "#مدامني قسمت الداتا حسب الزمن فهيك عالاغلب مش رح احتاجه\n",
        "KF = KFold(n_splits = 3 , shuffle = True , random_state = 42)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"encoding\" , OneHotEncoder(handle_unknown = \"ignore\" , sparse_output = True , drop = \"first\") , real_low_cols) ,\n",
        "\n",
        "        # Target Encoding يُستخدم مع الأعمدة الفئوية ذات عدد القيم الكبير.\n",
        "        # يتم تحويل كل فئة إلى متوسط قيمة المتغير الهدف المرتبط بها.\n",
        "        # لتجنب تسريب الهدف (Target Leakage)، يتم تطبيق الترميز داخل\n",
        "        # الـ Cross-Validation بحيث يُحسب الترميز من بيانات التدريب فقط.\n",
        "        # معاملات min_samples_leaf و smoothing تقلل تأثير الفئات النادرة\n",
        "        # عبر تقريبها من المتوسط العام، مما يحد من الـ overfitting.\n",
        "        (\"target_encoding\" , ce.TargetEncoder(min_samples_leaf = 20 , smoothing = 50) , real_high_cols) ,\n",
        "        (\"Frequency\" , ce.CountEncoder(normalize = True) , Frequency_col) ,\n",
        "        (\"scaling\" , StandardScaler() , real_num_cols)\n",
        "                 ]\n",
        ")\n",
        "# معامل الفريكوانسي ترو ليش؟ , لانه اذا حطيتو فولز اللي رح يصير انه رح يوخذ عدد التكرارات زي ما هو في هيك بصير عندي تباين كبير ورح يصير بحاجه لسكيلينق\n",
        "#اما هيك اللي رح يعملو انه رح يحولهم لنسبة بين ال 0 وال 1\n",
        "\n",
        "\n",
        "\n",
        "train_cls = preprocessor.fit_transform(xc_train , yc_train)\n",
        "test_cls = preprocessor.transform(xc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c06d70",
      "metadata": {
        "id": "e3c06d70"
      },
      "outputs": [],
      "source": [
        "available_users = xc_train[\"user_id\"].unique()\n",
        "n_users = len(available_users)\n",
        "n_users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11140884",
      "metadata": {
        "id": "11140884"
      },
      "outputs": [],
      "source": [
        "#اصريت اتأكد انه الترانسفورم تطبق\n",
        "print(train_cls.shape)\n",
        "print(xc_train.shape)\n",
        "print()\n",
        "preprocessor.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa1a40c",
      "metadata": {
        "id": "eaa1a40c"
      },
      "outputs": [],
      "source": [
        "print(yc_train.value_counts(normalize=True))\n",
        "\n",
        "plt.figure(figsize=(6 , 4))\n",
        "sns.countplot(x=yc_train)\n",
        "plt.title(\"Class Distribution in Training Set\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "#هيك بين عندي انه في تفاوت بنسب الكلاسين اللي عندي , الفرق ما بأثر كثير بس لازم اعمل موازنه لانه الدكتور طلب"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe25caa3",
      "metadata": {
        "id": "fe25caa3"
      },
      "outputs": [],
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "smote_xtrain , smote_ytrain = smote.fit_resample(train_cls, yc_train)      #بتعمل KNN\n",
        "#اللي بصير هون انه السموت رح تزيد قيم الكلاس القليل بقيم ثناعيه او وهميه عشان التوازن\n",
        "print(smote_ytrain.value_counts())\n",
        "\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "'''rus = RandomUnderSampler(random_state = 42)\n",
        "under_xtrain , under_ytrain = rus.fit_resample(train_cls , yc_train)\n",
        "#هاي بتحذف من الكلاس الكبير بطريقه يصير قريب للكلاس الاقل , فيها مشكله انه ممكن تخسر بيانات مهمه من وراها واصلا هيك هيك الدقه فيها مش احسن اشي ف ما رح نستخدمها\n",
        "print(under_ytrain.value_counts())'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e4f494e",
      "metadata": {},
      "outputs": [],
      "source": [
        "rus = RandomUnderSampler(random_state = 42)\n",
        "under_xtrain , under_ytrain = rus.fit_resample(train_cls , yc_train)\n",
        "#هاي بتحذف من الكلاس الكبير بطريقه يصير قريب للكلاس الاقل , فيها مشكله انه ممكن تخسر بيانات مهمه من وراها واصلا هيك هيك الدقه فيها مش احسن اشي ف ما رح نستخدمها\n",
        "print(under_ytrain.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7dd078",
      "metadata": {
        "id": "cd7dd078"
      },
      "outputs": [],
      "source": [
        "'''model_original = RandomForestClassifier(n_estimators = 50 , max_depth = 20 , class_weight = \"balanced\")\n",
        "#class_weight = 'balanced' هاي بتعطي اهميه اكبر للكلاس الاقل , يعني بتعاقب الموديل لما يهمله ف مجازيا بزيد وزنه , تعتبر بديل لكلشي عملناه تحت\n",
        "#بدونها رح يكون في تحييز للكلاس اللي حجمه اكبر , وليش ؟ لانه الموديل بفهم انه هاض الكلاس لانه اكبر معناها هاض مهم والثاني لا\n",
        "model_original.fit(train_cls , yc_train)\n",
        "\n",
        "pred_O = model_original.predict(test_cls)\n",
        "print(classification_report(yc_test , pred_O))\n",
        "print(model_original.score(test_cls , yc_test))\n",
        "\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "model_smote = RandomForestClassifier(n_estimators = 50 , max_depth = 20)\n",
        "model_smote.fit(smote_xtrain , smote_ytrain)\n",
        "\n",
        "pred_S = model_smote.predict(test_cls)\n",
        "print(classification_report(yc_test , pred_S))\n",
        "print(model_smote.score(test_cls , yc_test))\n",
        "\n",
        "#السكور زاد بنسبه خفيفه , استدعاء الفئه اللي كانت اقل تحسن , الدقه قلت بنسبة خفيفه والسبب انه بطل يعتمد على كلاس واحد صار يعتمد على ثنين بالتصنيف ف نسبة الدقه اقل هون\n",
        "\n",
        "\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "model_under = RandomForestClassifier(n_estimators = 50 , max_depth = 20 , random_state = 42)\n",
        "model_under.fit(under_xtrain , under_ytrain)\n",
        "\n",
        "pred_U = model_under.predict(test_cls)\n",
        "print(classification_report(yc_test , pred_U))\n",
        "print(model_under.score(test_cls , yc_test))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d321154",
      "metadata": {
        "id": "1d321154"
      },
      "outputs": [],
      "source": [
        "'''fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "models_list = [\n",
        "    (model_original, \"Original (Class Weights)\"),\n",
        "    (model_smote, \"SMOTE\"),\n",
        "    (model_under, \"UnderSampling\")\n",
        "]\n",
        "\n",
        "for i, (model, title) in enumerate(models_list):\n",
        "    disp = ConfusionMatrixDisplay.from_estimator(\n",
        "        model,\n",
        "        test_cls,\n",
        "        yc_test,\n",
        "        display_labels=['Not Reordered', 'Reordered'],\n",
        "        cmap=plt.cm.Blues,\n",
        "        normalize='true',\n",
        "        ax=axes[i]\n",
        "    )\n",
        "    axes[i].set_title(title)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''\n",
        "#اول مره جربت اشغل هذول عالبيانات قبل التوزيع حسب الزمن , كان تقسيم عشوائي , وطلع معي انع الالفضل سموت , بس بعد التقسيم حسب الزمن رح نعتمد الداتا الاصليه\n",
        "## رجعت جربت بيانات اكبر وطلع معي السموت اعلى بنسبه بسيييييييطه وعشان هيك مش رح اهتم بالنسبه هاي ورح اعتمد الداتا الاصليه لانها ما بتوخذ وقت عكس السموت"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "406aeeb5",
      "metadata": {
        "id": "406aeeb5"
      },
      "source": [
        "رح نعتمد هذول بالتدريب train_cls , yc_train\n",
        "\n",
        "وهذول بالاختبار test_cls , yc_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02076bca",
      "metadata": {
        "id": "02076bca"
      },
      "source": [
        "Task A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f911c4",
      "metadata": {
        "id": "16f911c4"
      },
      "outputs": [],
      "source": [
        "#هون رح اعتمد الاندر , لانه الموديل هون بصنف حسب الاقرب وبحالتي هاي ما بزبط يكون الداتا فيها عدم توازن , وما بقدر استخدم\n",
        "# class_weight = 'balanced'\n",
        "\n",
        "'''np.random.seed(42)\n",
        "\n",
        "small_users = np.random.choice(xc_train['user_id'].unique(), size=3000, replace=False)\n",
        "\n",
        "xtrain_tune = xc_train[xc_train['user_id'].isin(small_users)].copy()\n",
        "ytrain_tune = yc_train.loc[xtrain_tune.index].copy()\n",
        "\n",
        "xtest_tune = xc_test[xc_test['user_id'].isin(small_users)].copy()\n",
        "ytest_tune = yc_test.loc[xtest_tune.index].copy()'''\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tuning_x = xc_train.copy()\n",
        "tuning_y = yc_train.copy()\n",
        "\n",
        "t = TimeSeriesSplit(n_splits = 3)\n",
        "\n",
        "'''trainK = []\n",
        "testK = []\n",
        "k_values = range(1 , 22 , 2)\n",
        "\n",
        "for n in k_values:\n",
        "    tempK = KNeighborsClassifier(n_neighbors = n , n_jobs = -1)\n",
        "    Knn_model = tempK.fit(under_xtrain , under_ytrain)\n",
        "    trainK.append(tempK.score(x_trainS , y_trainS))\n",
        "    testK.append(tempK.score(x_testS , y_testS))\n",
        "\n",
        "print(testK)\n",
        "\n",
        "plt.plot(k_values , trainK , label = \"Traink\")\n",
        "plt.plot(k_values , testK , label = \"Testk\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"n_neighbors\")\n",
        "plt.legend()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546d4361",
      "metadata": {},
      "outputs": [],
      "source": [
        "#هاض ببينلك كيف بتتغير الدقه مع زيادة \\ تغيير حجم الداتا \n",
        "\n",
        "def learning_curves_cls(model , x_train , y_train , cv):\n",
        "\n",
        "    train_sizes=np.linspace(0.1, 1.0, 5)\n",
        "\n",
        "    train_sizes , train_scores , test_scores = learning_curve(\n",
        "        model , \n",
        "        x_train , y_train ,\n",
        "        cv = cv , n_jobs = -1 \n",
        "    )\n",
        "\n",
        "    train_mean = np.mean(train_scores , axis = 1)\n",
        "    train_std  = np.std(train_scores , axis = 1)\n",
        "    test_mean  = np.mean(test_scores , axis = 1)\n",
        "    test_std   = np.std(test_scores , axis = 1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Learning Curve (Full Data)\")\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.grid()\n",
        "\n",
        "    #  المعادله هاي عشان ارسم التباين\n",
        "    plt.fill_between(train_sizes , train_mean - train_std , train_mean + train_std , alpha = 0.1 , color = \"r\")\n",
        "    plt.fill_between(train_sizes , test_mean - test_std , test_mean + test_std , alpha = 0.1 , color = \"g\")\n",
        "\n",
        "    plt.plot(train_sizes , train_mean , 'o-' , color = \"r\" , label = \"Training score\")\n",
        "    plt.plot(train_sizes , test_mean , 'o-' , color = \"g\" , label = \"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc = \"best\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b586b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def val_curves(model , x_train , y_train , param_name , cv , param_range):\n",
        "\n",
        "    train_scores, test_scores = validation_curve(\n",
        "        model , \n",
        "        x_train , \n",
        "        y_train , \n",
        "        param_name = param_name , \n",
        "        param_range = param_range ,\n",
        "        cv = cv , \n",
        "        scoring = \"accuracy\" ,\n",
        "        n_jobs = -1\n",
        "    )\n",
        "\n",
        "    train_mean = np.mean(train_scores , axis=1)\n",
        "    train_std = np.std(train_scores , axis=1)\n",
        "    test_mean = np.mean(test_scores , axis=1)\n",
        "    test_std = np.std(test_scores , axis=1)\n",
        "\n",
        "    plt.figure(figsize = (10, 6))\n",
        "    plt.title(\"Validation Curve\")\n",
        "    plt.xlabel(\"Param (Complexity)\")\n",
        "    plt.ylabel(\"score (Higher is Better)\")\n",
        "\n",
        "    plt.plot(param_range, train_mean , label = \"Training Score\" , color = \"darkorange\", lw = 2 , marker = 'o')\n",
        "    plt.fill_between(param_range, train_mean - train_std , train_mean + train_std , alpha = 0.1 , color = \"darkorange\")\n",
        "\n",
        "    # رسم منطقة التيست\n",
        "    plt.plot(param_range , test_mean , label = \"Validation Score\" , color = \"navy\" , lw = 2, marker = 'o')\n",
        "    plt.fill_between(param_range , test_mean - test_std , test_mean + test_std , alpha = 0.1 , color = \"navy\")\n",
        "\n",
        "    plt.legend(loc = \"best\")\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36a7698b",
      "metadata": {
        "id": "36a7698b"
      },
      "source": [
        "1- KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e97d996",
      "metadata": {
        "id": "0e97d996"
      },
      "outputs": [],
      "source": [
        "'''FAST_TUNING_USERS = 1000\n",
        "\n",
        "np.random.seed(42)\n",
        "# بنختار عينة جديدة صغيرة من العينة الأصلية الكبيرة\n",
        "KNNsmall_users = np.random.choice(xtrain_tune['user_id'].unique(), size=800, replace=False)\n",
        "\n",
        "# تصفية الداتا\n",
        "knn_x_small = xtrain_tune[xtrain_tune['user_id'].isin(small_users)].copy()\n",
        "knn_y_small = ytrain_tune[xtrain_tune['user_id'].isin(small_users)]\n",
        "\n",
        "KNN_pipline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    KNeighborsClassifier()\n",
        ")\n",
        "\n",
        "\n",
        "t = TimeSeriesSplit(n_splits = 3)\n",
        "\n",
        "param_grid = {\n",
        "    \"kneighborsclassifier__n_neighbors\": [7 , 10 , 20 , 25 , 30] ,\n",
        "    \"kneighborsclassifier__weights\" : ['distance' , 'uniform']\n",
        "}\n",
        "\n",
        "KNN_grid_search = GridSearchCV(\n",
        "    estimator = KNN_pipline ,\n",
        "    param_grid = param_grid ,\n",
        "    cv = t ,\n",
        "    scoring = 'f1' ,\n",
        "    n_jobs = -1 ,\n",
        "    verbose=3\n",
        ")\n",
        "\n",
        "print(\"Done.....\")\n",
        "\n",
        "KNN_grid_search.fit(knn_x_small , knn_y_small)\n",
        "\n",
        "print(\"Best CV score:\" , KNN_grid_search.best_score_)\n",
        "print(\"Best params:\" , KNN_grid_search.best_params_)'''\n",
        "\n",
        "\n",
        "KNNsmall_users = np.random.choice(tuning_x['user_id'].unique() , size = 2000 , replace = False)\n",
        "\n",
        "knn_x_small = tuning_x[tuning_x['user_id'].isin(KNNsmall_users)].copy()\n",
        "knn_y_small = tuning_y[tuning_x['user_id'].isin(KNNsmall_users)]\n",
        "\n",
        "KNN_pipline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    KNeighborsClassifier(n_jobs = -1)\n",
        ")\n",
        "\n",
        "KNNparam_grid = {\n",
        "    \"kneighborsclassifier__n_neighbors\": [7 , 10 , 20 , 25 , 30] ,\n",
        "    \"kneighborsclassifier__weights\" : ['distance' , 'uniform']\n",
        "}\n",
        "\n",
        "KNN_grid_search = GridSearchCV(\n",
        "    estimator = KNN_pipline ,\n",
        "    param_grid = KNNparam_grid ,\n",
        "    cv = t ,\n",
        "    scoring = \"f1\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "KNN_grid_search.fit(knn_x_small , knn_y_small)\n",
        "\n",
        "print(\"\\nBest Score:\", KNN_grid_search.best_score_)\n",
        "print(\"Best Parameters:\", KNN_grid_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4953978",
      "metadata": {
        "id": "c4953978"
      },
      "outputs": [],
      "source": [
        "best_w = KNN_grid_search.best_params_['kneighborsclassifier__weights']\n",
        "best_k = KNN_grid_search.best_params_['kneighborsclassifier__n_neighbors']\n",
        "\n",
        "finalKNN_pip = make_pipeline(\n",
        "        preprocessor ,\n",
        "        KNeighborsClassifier(n_neighbors = best_k , weights = best_w , n_jobs = -1)\n",
        ")\n",
        "\n",
        "finalKNN_pip.fit(xc_train, yc_train)\n",
        "\n",
        "\n",
        "y_pred_KNN = finalKNN_pip.predict(xc_test)\n",
        "\n",
        "print(classification_report(yc_test , y_pred_KNN))\n",
        "print(accuracy_score(yc_test , y_pred_KNN))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816c7928",
      "metadata": {
        "id": "816c7928"
      },
      "outputs": [],
      "source": [
        "#بعد تطبيق تقسيم زمني واقعي ومعالجة عدم التوازن، انخفضت الدقة من قيم مرتفعة غير واقعية إلى ~0.7،\n",
        "#إلا أن قدرة النموذج على اكتشاف المنتجات المعاد طلبها (Recall) تحسّنت بشكل واضح،\n",
        "#مما يجعل النموذج أكثر ملاءمة للاستخدام الحقيقي."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f866ab53",
      "metadata": {
        "id": "f866ab53"
      },
      "source": [
        "2-Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068f5ca7",
      "metadata": {
        "id": "068f5ca7"
      },
      "outputs": [],
      "source": [
        "tuning_users = np.random.choice(tuning_x[\"user_id\"].unique() , size = 2500 , replace = False)\n",
        "\n",
        "# تجهيز عينة للتونينق\n",
        "LoR_x_small = tuning_x[tuning_x['user_id'].isin(tuning_users)].copy().sort_values(by=['user_id'])\n",
        "LoR_y_small = tuning_y.loc[LoR_x_small.index]\n",
        "\n",
        "LoR_pipline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    LogisticRegression(solver = 'liblinear' , class_weight = 'balanced' , random_state = 42 , n_jobs = -1)\n",
        ")\n",
        "\n",
        "LoRparam_grid = {\n",
        "    \"logisticregression__C\": [0.001 , 0.01 , 0.1 , 1 , 10] ,\n",
        "    \"logisticregression__penalty\": ['l1' , 'l2']\n",
        "}\n",
        "\n",
        "LoR_grid_search = GridSearchCV(\n",
        "    estimator = LoR_pipline ,\n",
        "    param_grid = LoRparam_grid ,\n",
        "    cv = t ,\n",
        "    scoring = \"f1\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "LoR_grid_search.fit(LoR_x_small , LoR_y_small)\n",
        "\n",
        "print(\"\\nBest Score:\", LoR_grid_search.best_score_)\n",
        "print(\"Best Parameters:\", LoR_grid_search.best_params_)\n",
        "\n",
        "\"\"\"Best Score: 0.7038598274542919\n",
        "Best Parameters: {'logisticregression__C': 0.001, 'logisticregression__penalty': 'l2'}\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacdee95",
      "metadata": {
        "id": "cacdee95"
      },
      "outputs": [],
      "source": [
        "SAMPLE_N = 75000 \n",
        "\n",
        "xc_train_small = xc_train.sample(n=SAMPLE_N, random_state=42)\n",
        "yc_train_small = yc_train.loc[xc_train_small.index]\n",
        "\n",
        "best_C = LoR_grid_search.best_params_[\"logisticregression__C\"]\n",
        "best_P = LoR_grid_search.best_params_[\"logisticregression__penalty\"]\n",
        "\n",
        "final_LoR_pipeline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    LogisticRegression(solver = 'liblinear' , C = best_C , penalty = best_P , class_weight = 'balanced' , random_state = 42 , n_jobs = -1)\n",
        ")\n",
        "\n",
        "final_LoR_pipeline.fit(xc_train_small , yc_train_small)\n",
        "\n",
        "y_pred_LoR = final_LoR_pipeline.predict(xc_test)\n",
        "\n",
        "print(classification_report(yc_test , y_pred_LoR))\n",
        "print(f\"Logistic Accuracy: {accuracy_score(yc_test , y_pred_LoR)}\")\n",
        "\n",
        "learning_curves_cls(final_LoR_pipeline , xc_train_small , yc_train_small , t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "581d09ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Logistic Accuracy: 0.7295685052244335  الدقه هيك قبل ما اشوف التباين وقب ما اعدل\n",
        "#Logistic Accuracy: 0.7314973303017143  هيك بعد , مش هالفرق بس كويس\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3854d970",
      "metadata": {},
      "outputs": [],
      "source": [
        "param_range = [0.001 , 0.01 , 0.1 , 1 , 10]\n",
        "val_curves(final_LoR_pipeline , xc_train , yc_train , \"logisticregression__C\" , t , param_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "399975da",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''numeric_df = train_df.select_dtypes(include=['number'])\n",
        "correlations = numeric_df.corrwith(train_df['reordered']).sort_values(ascending=False)\n",
        "\n",
        "print(correlations.head(10))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b12644",
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_model = final_LoR_pipeline.named_steps['logisticregression'] \n",
        "preprocessor = final_LoR_pipeline.named_steps['columntransformer'] \n",
        "\n",
        "try:\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "    feature_names = [name.split('__')[-1] for name in feature_names]\n",
        "except:\n",
        "    feature_names = [f\"Feature_{i}\" for i in range(len(lr_model.coef_[0]))]\n",
        "\n",
        "if len(feature_names) != len(lr_model.coef_[0]):\n",
        "    print(f\"Mismatch! Names: {len(feature_names)}, Coefs: {len(lr_model.coef_[0])}\")\n",
        "    feature_names = [f\"Feat_{i}\" for i in range(len(lr_model.coef_[0]))]\n",
        "\n",
        "coefs = pd.DataFrame({\n",
        "    'Feature': feature_names ,\n",
        "    'Coefficient': lr_model.coef_[0]\n",
        "})\n",
        "\n",
        "coefs['Abs_Value'] = coefs['Coefficient'].abs()\n",
        "top_20_coefs = coefs.sort_values(by = 'Abs_Value' , ascending = False).head(20)\n",
        "\n",
        "plt.figure(figsize = (10 , 8))\n",
        "colors = ['blue' if x > 0 else 'red' for x in top_20_coefs['Coefficient']]\n",
        "plt.barh(top_20_coefs['Feature'] , top_20_coefs['Coefficient'] , color = colors)\n",
        "\n",
        "plt.title(\"Top 20 Coefficients in Logistic Regression (L2 Regularization)\")\n",
        "plt.xlabel(\"Coefficient Value (Impact on Log-Odds)\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x' , linestyle = '--' , alpha = 0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca5f6c61",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_sample = xc_train.sample(n = 100 , random_state = 42)\n",
        "X_train_transformed = preprocessor.transform(X_train_sample)\n",
        "\n",
        "X_test_sample_lin = xc_test.sample(n = 1000 , random_state = 42)\n",
        "X_test_transformed_lin = preprocessor.transform(X_test_sample_lin)\n",
        "\n",
        "explainer_linear = shap.LinearExplainer(lr_model , X_train_transformed)\n",
        "shap_values_linear = explainer_linear.shap_values(X_test_transformed_lin)\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_linear , X_test_transformed_lin , feature_names = feature_names , show = False)\n",
        "plt.title(\"Logistic Regression SHAP Summary\" , fontsize = 16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd5a355",
      "metadata": {
        "id": "bdd5a355"
      },
      "source": [
        "3- Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8348ab2",
      "metadata": {
        "id": "f8348ab2"
      },
      "outputs": [],
      "source": [
        "tuning_users = np.random.choice(tuning_x[\"user_id\"].unique(), size = 800 , replace = False)\n",
        "RF_x_small = tuning_x[tuning_x['user_id'].isin(tuning_users)].copy().sort_values(by = ['user_id'])\n",
        "RF_y_small = tuning_y.loc[RF_x_small.index]\n",
        " \n",
        "RF_pipline = make_pipeline(\n",
        "    preprocessor ,\n",
        "    RandomForestClassifier(class_weight = \"balanced\" , random_state = 42 , n_jobs = -1)\n",
        ")\n",
        "\n",
        "RFparam_grid = {\n",
        "    \"randomforestclassifier__n_estimators\": [100 , 150 , 200] ,\n",
        "    \"randomforestclassifier__max_depth\": [None , 10 , 20] ,\n",
        "    \"randomforestclassifier__min_samples_split\": [2 , 5 , 10] ,\n",
        "    \"randomforestclassifier__min_samples_leaf\": [1 , 2 , 4] ,\n",
        "    \"randomforestclassifier__class_weight\": ['balanced' , 'balanced_subsample']\n",
        "}\n",
        "\n",
        "RF_grid_search = GridSearchCV(\n",
        "    estimator = RF_pipline ,\n",
        "    param_grid = RFparam_grid ,\n",
        "    cv = t ,\n",
        "    scoring = \"f1\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "RF_grid_search.fit(RF_x_small , RF_y_small)\n",
        "\n",
        "print(\"\\nBest Score:\", RF_grid_search.best_score_)\n",
        "print(\"Best Parameters:\", RF_grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd8469f0",
      "metadata": {
        "id": "bd8469f0"
      },
      "outputs": [],
      "source": [
        "best_N = RF_grid_search.best_params_[\"randomforestclassifier__n_estimators\"]\n",
        "best_MD = RF_grid_search.best_params_[\"randomforestclassifier__max_depth\"]\n",
        "best_MSP = RF_grid_search.best_params_[ \"randomforestclassifier__min_samples_split\"]\n",
        "best_MSL = RF_grid_search.best_params_[\"randomforestclassifier__min_samples_leaf\"]\n",
        "best_W = RF_grid_search.best_params_[\"randomforestclassifier__class_weight\"]\n",
        "\n",
        "final_RF_pipeline = make_pipeline(\n",
        "    preprocessor,\n",
        "    RandomForestClassifier(n_estimators = best_N , max_depth = best_MD ,\n",
        "                    min_samples_leaf = best_MSL , min_samples_split = best_MSP ,\n",
        "                 class_weight = best_W , n_jobs = -1 , random_state = 42\n",
        "                        )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "final_RF_pipeline.fit(xc_train , yc_train)\n",
        "\n",
        "y_pred_RF = final_RF_pipeline.predict(xc_test)\n",
        "\n",
        "print(classification_report(yc_test , y_pred_RF))\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(yc_test , y_pred_RF)}\")\n",
        "\n",
        "learning_curves_cls(final_RF_pipeline , xc_train , yc_train , t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa306c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "param_range = [1, 3, 5, 8, 10, 15, 20]\n",
        "val_curves(final_RF_pipeline , xc_train , yc_train , \"randomforestclassifier__max_depth\" , t , param_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e4f8ba",
      "metadata": {},
      "source": [
        "4- Gradient boosting classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985eed62",
      "metadata": {},
      "outputs": [],
      "source": [
        "# رح نسخدم هالمره الترين سي ل س عشان معمولها بري بروسس وكلها ارقام لانو لو استخدمنا الداتا العاديه رح نتطر نحذ الكولومز الي مش رقميه وبالتالي نخسر بيانات مهمه\n",
        "tuning_users_ids = np.random.choice(xc_train['user_id'].unique(), size=1000, replace=False)\n",
        "\n",
        "\n",
        "# بما أن train_cls يطابق xc_train سطراً بسطر، نستخدم شرط xc_train لقص train_cls\n",
        "XGB_x_small = train_cls[xc_train['user_id'].isin(tuning_users_ids)]\n",
        "XGB_y_small = yc_train[xc_train['user_id'].isin(tuning_users_ids)]\n",
        "\n",
        "ratio = float(np.sum(XGB_y_small == 0)) / np.sum(XGB_y_small == 1)\n",
        "\n",
        "print(f\"Tuning shape: {XGB_x_small.shape}\")\n",
        "print(f\"Calculated Ratio: {ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce705a15",
      "metadata": {},
      "outputs": [],
      "source": [
        " # تعريف الموديل مباشرة بدون Pipeline لأن الداتا جاهزة\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    scale_pos_weight=ratio,\n",
        "    use_label_encoder=False,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "XGBparam_grid = {\n",
        "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"max_depth\": [3, 5, 7]\n",
        "}\n",
        "\n",
        "\n",
        "XGB_grid_search = GridSearchCV(\n",
        "    estimator = xgb_model,\n",
        "    param_grid = XGBparam_grid,\n",
        "    cv = 3,\n",
        "    scoring = \"f1\",\n",
        "    n_jobs = -1,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "XGB_grid_search.fit(XGB_x_small, XGB_y_small)\n",
        "\n",
        "print(\"\\nBest Score (F1):\", XGB_grid_search.best_score_)\n",
        "print(\"Best Parameters:\", XGB_grid_search.best_params_)\n",
        "# بعد التعديل ارتفعت السكور من 72 ل 77\n",
        "# Best Score (F1): 0.7726193292101673\n",
        "# Best Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db22d071",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_LR = XGB_grid_search.best_params_[\"learning_rate\"]\n",
        "best_NE = XGB_grid_search.best_params_[\"n_estimators\"]\n",
        "best_MD = XGB_grid_search.best_params_[\"max_depth\"]\n",
        "\n",
        "# print(f\"Learning Rate: {best_LR}, N_Estimators: {best_NE}, Max_Depth: {best_MD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b892226",
      "metadata": {},
      "outputs": [],
      "source": [
        "full_ratio = float(np.sum(yc_train == 0)) / np.sum(yc_train == 1)\n",
        "print(f\"Full Data Ratio: {full_ratio:.2f}\")\n",
        "\n",
        "\n",
        "#  #  الموديل النهائي\n",
        "# final_xgb = XGBClassifier(\n",
        "#     learning_rate = best_LR,\n",
        "#     n_estimators = best_NE,\n",
        "#     max_depth = best_MD,\n",
        "#     scale_pos_weight = full_ratio,\n",
        "#     objective = 'binary:logistic',\n",
        "#     eval_metric = 'logloss',\n",
        "#     use_label_encoder = False,\n",
        "#     random_state = 42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # على كل الداتا\n",
        "# final_xgb.fit(train_cls, yc_train)\n",
        "\n",
        "\n",
        "#========================================================\n",
        "# عشان نعمل خظوه الايرلي سبلتنق لازم نقسم الداتا كمان مره ولازم ننتبه يكون تقسيمنا براعي الزمن الي قسمناه بالبدايه تمام\n",
        "#لانو الايرلي ستوبنق بحتاج نعمل فاليديشن اكثر من مره حتى يعرف متى يوقف\n",
        "# بنجيب كل اليوزرز من الداتا الأصلية وبنقسمهم 90% تدريب 10% مراقبة\n",
        "train_users, val_users = train_test_split(xc_train['user_id'].unique(), test_size=0.10, random_state=42)\n",
        "\n",
        "# 2. عمل الماسك (Masks) لتحديد أماكن هدول اليوزرز في الداتا\n",
        "# train_mask: بيحدد أسطر يوزرز التدريب\n",
        "# val_mask: بيحدد أسطر يوزرز المراقبة\n",
        "train_mask = xc_train['user_id'].isin(train_users).values\n",
        "val_mask = xc_train['user_id'].isin(val_users).values\n",
        "\n",
        "# 3. تطبيق الماسك على الداتا المعالجة الجاهزة (train_cls)\n",
        "# هيك ضمنا إن X_tr و X_val مفصولين تماماً بناءً على الأشخاص\n",
        "X_tr = train_cls[train_mask]\n",
        "y_tr = yc_train[train_mask]\n",
        "\n",
        "X_val = train_cls[val_mask]\n",
        "y_val = yc_train[val_mask]\n",
        "\n",
        "print(f\"Train shapes: {X_tr.shape}, Val shapes: {X_val.shape}\")\n",
        "\n",
        "\n",
        "final_xgb = xgb.XGBClassifier(\n",
        "    learning_rate = best_LR,\n",
        "    n_estimators = 3000,         # رقم كبير جدا نعطيه راحته\n",
        "    max_depth = best_MD,\n",
        "    # scale_pos_weight = full_ratio,\n",
        "    objective = 'binary:logistic',\n",
        "    eval_metric = 'logloss',\n",
        "    use_label_encoder = False,\n",
        "    early_stopping_rounds = 50,  #وقف اذا ماتحسن بعد 50 راوند\n",
        "    random_state = 42,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "final_xgb.fit(\n",
        "    X_tr, y_tr,\n",
        "    eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Training Stopped at iteration: {final_xgb.best_iteration}\")\n",
        "print(f\"Best Validation Score: {final_xgb.best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a39b589",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_xgb = final_xgb.predict(test_cls)\n",
        "\n",
        "print(\" XGBoost Report\")\n",
        "print(classification_report(yc_test, y_pred_xgb))\n",
        "\n",
        "print(f\"XGBoost Accuracy: {final_xgb.score(test_cls, yc_test)}\")\n",
        "# هاي النتائج قبل الايرلي ستوبنق\n",
        "# XGBoost Report\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.50      0.71      0.59     23041\n",
        "#            1       0.85      0.70      0.77     55245\n",
        "\n",
        "#     accuracy                           0.70     78286\n",
        "#    macro avg       0.67      0.71      0.68     78286\n",
        "# weighted avg       0.75      0.70      0.71     78286\n",
        "\n",
        "# XGBoost Accuracy: 0.702922617070740\n",
        "\n",
        "# النتائج هاي بعد الايرلي ستوبنق وفرقت معنا منيح\n",
        "# XGBoost Report\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.51      0.74      0.60     23041\n",
        "#            1       0.87      0.70      0.77     55245\n",
        "\n",
        "#     accuracy                           0.71     78286\n",
        "#    macro avg       0.69      0.72      0.69     78286\n",
        "# weighted avg       0.76      0.71      0.72     78286\n",
        "\n",
        "# XGBoost Accuracy: 0.7107784278159569\n",
        "#بنلاحظ الاكيروسي زادت تقريبا 1 بالميه\n",
        "# وال ري كول لكلاس 0 تحسن بشكل منيح من 71 ل 74 يعني المودل صار احسن بالي مابشترو\n",
        "#======================================\n",
        "\n",
        "\n",
        "# بعد اضافه تو كولمز مهمين\n",
        "\n",
        "#  XGBoost Report\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.75      0.97      0.85     23041\n",
        "#            1       0.98      0.87      0.92     55245\n",
        "\n",
        "#     accuracy                           0.90     78286\n",
        "#    macro avg       0.87      0.92      0.88     78286\n",
        "# weighted avg       0.92      0.90      0.90     78286"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2cca90",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Plotting Feature Importance\")\n",
        "\n",
        "try:\n",
        "\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "    clean_names = [name.split('__')[-1] for name in feature_names]\n",
        "\n",
        "    final_xgb.get_booster().feature_names = clean_names\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    xgb.plot_importance(\n",
        "        final_xgb,\n",
        "        ax=ax,\n",
        "        max_num_features=20,     \n",
        "        height=0.5,\n",
        "        importance_type='weight', \n",
        "        color='teal',\n",
        "        title=\"Top 20 Features Influencing Reorders (XGBoost)\"\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not extract feature names automatically (Error: {e}). Plotting by index instead...\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    xgb.plot_importance(final_xgb, ax=ax, max_num_features=20, height=0.5, color='teal')\n",
        "    plt.title(\"Top 20 Features (by Index)\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1923e2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "xgboost_model = final_xgb \n",
        "\n",
        "X_test_sample = xc_test.sample(n=1000, random_state=42)\n",
        "y_test_sample = yc_test.loc[X_test_sample.index]\n",
        "\n",
        "X_test_transformed = preprocessor.transform(X_test_sample)\n",
        "\n",
        "try:\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "    feature_names = [name.split('__')[-1] for name in feature_names]\n",
        "except:\n",
        "    print(\"Could not extract feature names automatically.\")\n",
        "    feature_names = [f\"Feature_{i}\" for i in range(X_test_transformed.shape[1])]\n",
        "\n",
        "\n",
        "print(\"Calculating SHAP values\")\n",
        "\n",
        "explainer = shap.TreeExplainer(xgboost_model)\n",
        "shap_values = explainer.shap_values(X_test_transformed)\n",
        "\n",
        "\n",
        "print(\"Plotting SHAP Summary Plot\")\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, show=False)\n",
        "plt.title(\"XGBoost Feature Importance (SHAP)\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "mean_shap = np.abs(shap_values).mean(axis=0)\n",
        "top_3_indices = np.argsort(mean_shap)[-3:] \n",
        "\n",
        "print(\"Plotting Dependence Plots...\")\n",
        "for idx in top_3_indices:\n",
        "    shap.dependence_plot(idx, shap_values, X_test_transformed, feature_names=feature_names)\n",
        "\n",
        "\n",
        "print(\"Generating Local Explanation (Force Plot)...\")\n",
        "\n",
        "preds = xgboost_model.predict(X_test_transformed)\n",
        "actual = y_test_sample.values\n",
        "\n",
        "try:\n",
        "    target_idx = np.where((preds == 1) & (actual == 1))[0][0]\n",
        "    \n",
        "    print(f\"Explanation for sample index {target_idx} (Correct Prediction)\")\n",
        "    \n",
        "    shap.force_plot(\n",
        "        explainer.expected_value, \n",
        "        shap_values[target_idx], \n",
        "        X_test_transformed[target_idx], \n",
        "        feature_names=feature_names,\n",
        "        matplotlib=True\n",
        "    )\n",
        "except IndexError:\n",
        "    print(\"لم يتم العثور على حالة True Positive في العينة المختارة.\")\n",
        "\n",
        "\n",
        "\n",
        "    #الرسمه الاولى بتبينلك قديش تأثير العامود بقرار التنبؤ , يعني اذا كان موجب رايح عاليمين بكون العامود هاض كثير اثر على اعادة الشراء والعكس صحيح\n",
        "    #كل ما زادت عدد طلبات الزبون بزيد احتمالية اعادة الشراء\n",
        "\n",
        "    #الرسمه الرابعه , بتوضح انه واحد من العوامل المهمه كثير لاعادة الشراء انه يكون الزبون مشتري نفس المنتج من قبل\n",
        "\n",
        "    #الرسمه الثالثه بتوضح انه في علاقة عكسية , بمعنى انه مثلا اذا كان الزبون مشتري مبارح ف صعب انه يرجع يشتري كمان مره بعد فتره قصيره نسبيا\n",
        "\n",
        "    #الصوره الثانيه بتحكي انه مثلا الزبائن الجداد بكون هدفهم يشترو شغلات اساسيه ف عشان هيك بكون في تكرار كثير بالشراء \n",
        "    #اما الزبائن القدام بكونو بشترو شغلات جديده عن العاده عشان هيك نسبة اعادة الشراء قليله نسبيا\n",
        "\n",
        "    #الرسمه الاخيره , مسكنا زبون واحد وشفنا ليش الموديل حكى انه بنسبة 85 بالميه رح يكرر الشراء\n",
        "    #الاعتماد كان على :\n",
        "\n",
        "    \"\"\"\n",
        "    days_since_prior_order: الزبون صرله فترة مش طالب (القيمة عالية)، فهاد دفش الاحتمالية لفوق.\n",
        "\n",
        "    uxp_total_bought: الزبون شاري هذا المنتج كثير مرات قبل هيك.\n",
        "\n",
        "    85 هذا الزبون بيحب هذا المنتج (شراه كثير)، وصرله فترة مش طالبه (خلص من عنده)، لذلك الموديل توقع الشراء بنسبة                  \n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9664db2",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": final_LoR_pipeline ,\n",
        "    \"Random Forest\": final_RF_pipeline ,   \n",
        "    \"KNN Classifier\": finalKNN_pip  \n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name , model in models.items():\n",
        "    print(f\"Testing {name}\")\n",
        "    scores = cross_val_score(model , xc_train , yc_train , cv = 5 , scoring = 'accuracy' , n_jobs = -1)\n",
        "    \n",
        "    for score in scores:\n",
        "        results.append({\"Model\": name , \"Accuracy\": score})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "plt.figure(figsize = (12 , 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.boxplot(x = \"Model\" , y = \"Accuracy\" , data = df_results , palette = \"viridis\" , width = 0.5)\n",
        "sns.stripplot(x = \"Model\" , y = \"Accuracy\" , data = df_results , color = 'black' , alpha = 0.5 , jitter = True)\n",
        "plt.title('Classification Models Benchmark (5-Fold CV)' , fontsize = 15)\n",
        "plt.ylabel('Accuracy Score' , fontsize = 12)\n",
        "plt.ylim(0.6 , 1.0)  \n",
        "plt.show()\n",
        "\n",
        "#هالرسمه العاطله بتبينلك مقارنه بدقة كل موديل \n",
        "#اللوجيستيك الخط تبعه كثير رفيع ف هاض بدل على انه مستقر وشو ما عملت فيه ما رح يزيد\n",
        "#الراندوم وضعها نار شرار واتوقع لو اني حطيت ال سي في 10 كان رح يعطيني احتمالية اكيوريسي اعلى بس جهازي رح يفقع طبعا\n",
        "#ال كنن زباله ورح يضل طول عمره زباله"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bddecf5b",
      "metadata": {
        "id": "bddecf5b"
      },
      "source": [
        "RF: 0.7339 | LoR: 0.7066 | KNN: 0.7227  قبل\n",
        "\n",
        "RF: 0.8551 | LoR: 0.7295 | KNN: 0.7630  بعد"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78b8aaab",
      "metadata": {
        "id": "78b8aaab"
      },
      "source": [
        "Task B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b7e2e3",
      "metadata": {
        "id": "05b7e2e3"
      },
      "outputs": [],
      "source": [
        "#اول مره عملنا سبلت عملناه على اساس انه التارقيت كل كم اليوزر بشتري , وكانت النتائج كثير سيئه , ممكن الشغل نفسه كان غلط ابصر\n",
        "#بس بعديه حذفناه كامل ورجعنا قسمنا هيك وحلينا كل المشاكل\n",
        "\n",
        "cleaned_df = final_df[final_df['order_number'] > 1].copy()\n",
        "orders_level = (\n",
        "    cleaned_df\n",
        "    .sort_values([\"user_id\" , \"order_number\"])      \n",
        "    .drop_duplicates(\"order_id\" , keep=\"last\")      \n",
        "    .copy()\n",
        ")\n",
        "\n",
        "\n",
        "target_users_count = 30000\n",
        "unique_users = orders_level[\"user_id\"].unique()\n",
        "\n",
        "if target_users_count > len(unique_users):\n",
        "    target_users_count = len(unique_users)\n",
        "\n",
        "np.random.seed(42)\n",
        "my_users = np.random.choice(unique_users, size=target_users_count, replace=False)\n",
        "\n",
        "final_sample = orders_level[orders_level[\"user_id\"].isin(my_users)].copy()\n",
        "final_sample = final_sample.sort_values(by=[\"user_id\", \"order_number\"])\n",
        "\n",
        "\n",
        "final_sample[\"last__orders\"] = (\n",
        "    final_sample[\"order_number\"]\n",
        "    == final_sample.groupby(\"user_id\")[\"order_number\"].transform(\"max\")\n",
        ")\n",
        "\n",
        "train_df = final_sample[final_sample[\"last__orders\"] == False].copy()\n",
        "test_df  = final_sample[final_sample[\"last__orders\"] == True].copy()\n",
        "\n",
        "y_col = \"days_since_prior_order\"\n",
        "#احتياط المفروض\n",
        "train_df = train_df.dropna(subset=[y_col])\n",
        "test_df = test_df.dropna(subset=[y_col])\n",
        "\n",
        "not_for_X_columns = [\n",
        "    \"eval_set\", \"last__orders\", \"order_id\",\n",
        "    \"reordered\", \"add_to_cart_order\" , y_col\n",
        "]\n",
        "#order_number\n",
        "xr_train = train_df.drop(columns=not_for_X_columns + [\"user_id\"], errors=\"ignore\")\n",
        "yr_train = train_df[y_col].astype(\"float32\")\n",
        "\n",
        "xr_test  = test_df.drop(columns=not_for_X_columns + [\"user_id\"], errors=\"ignore\")\n",
        "yr_test  = test_df[y_col].astype(\"float32\")\n",
        "\n",
        "\n",
        "\n",
        "real_num_col_reg = xr_train.select_dtypes(include=[\"int32\", \"float32\", \"float64\"]).columns.tolist()\n",
        "\n",
        "real_num_col_reg = [c for c in real_num_col_reg if c not in id_cols]\n",
        "\n",
        "real_low_col_reg  = [c for c in low_cols  if c in xr_train.columns]\n",
        "real_high_col_reg = [c for c in high_cols if c in xr_train.columns]\n",
        "real_freq_col_reg = [c for c in Frequency_col if c in xr_train.columns]\n",
        "\n",
        "\n",
        "\n",
        "preprocessor_reg = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"encoding\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop=\"first\"), real_low_col_reg),\n",
        "        (\"target_encoding\", ce.TargetEncoder(min_samples_leaf=20, smoothing=50), real_high_col_reg),\n",
        "        (\"frequency\", ce.CountEncoder(normalize=True), real_freq_col_reg),\n",
        "        (\"scaling\", StandardScaler(), real_num_col_reg),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "train_reg = preprocessor_reg.fit_transform(xr_train, yr_train)\n",
        "test_reg  = preprocessor_reg.transform(xr_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9WDukkTR-645",
      "metadata": {
        "id": "9WDukkTR-645"
      },
      "outputs": [],
      "source": [
        "print(f\"xr_train shape : {xr_train.shape}\")\n",
        "print(f\"train_reg shape : {train_reg.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5933f9bb",
      "metadata": {
        "id": "5933f9bb"
      },
      "source": [
        "1- KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745b008a",
      "metadata": {
        "id": "745b008a"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tuning_xr = xr_train.copy()\n",
        "tuning_yr = yr_train.copy()\n",
        "GF = GroupKFold(n_splits=5)\n",
        "\n",
        "tuning_groups = train_df[\"user_id\"].copy()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337a4046",
      "metadata": {},
      "outputs": [],
      "source": [
        "def val_curves(model , x_train , y_train , param_name , cv , param_range):\n",
        "\n",
        "    train_scores, test_scores = validation_curve(\n",
        "        model , \n",
        "        x_train , \n",
        "        y_train , \n",
        "        param_name = param_name , \n",
        "        param_range = param_range ,\n",
        "        cv = cv , \n",
        "        scoring = \"neg_root_mean_squared_error\" , \n",
        "        n_jobs = -1\n",
        "    )\n",
        "\n",
        "    train_mean = abs(np.mean(train_scores , axis=1))\n",
        "    train_std = np.std(train_scores , axis=1)\n",
        "    test_mean = abs(np.mean(test_scores , axis=1))\n",
        "    test_std = np.std(test_scores , axis=1)\n",
        "\n",
        "    plt.figure(figsize = (10, 6))\n",
        "    plt.title(\"Validation Curve\")\n",
        "    plt.xlabel(\"param (Complexity)\")\n",
        "    plt.ylabel(\"RMSE Error (Lower is Better)\")\n",
        "\n",
        "    plt.plot(param_range, train_mean , label = \"Training Error\" , color = \"darkorange\", lw = 2 , marker = 'o')\n",
        "    plt.fill_between(param_range, train_mean - train_std , train_mean + train_std , alpha = 0.1 , color = \"darkorange\")\n",
        "\n",
        "    # رسم منطقة التيست\n",
        "    plt.plot(param_range , test_mean , label = \"Validation Error\" , color = \"navy\" , lw = 2, marker = 'o')\n",
        "    plt.fill_between(param_range , test_mean - test_std , test_mean + test_std , alpha = 0.1 , color = \"navy\")\n",
        "\n",
        "    plt.legend(loc = \"best\")\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae309f9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def learning_curves_reg(model , x_train , y_train , cv ):\n",
        "\n",
        "    train_sizes=np.linspace(0.1, 1.0, 5)\n",
        "\n",
        "    train_sizes , train_scores , test_scores = learning_curve(\n",
        "        model , \n",
        "        x_train , y_train ,\n",
        "        cv = cv , n_jobs = -1 ,\n",
        "        scoring='neg_root_mean_squared_error' ,\n",
        "    )\n",
        "\n",
        "    train_mean = abs(np.mean(train_scores , axis = 1))\n",
        "    train_std  = np.std(train_scores , axis = 1)\n",
        "    test_mean  = abs(np.mean(test_scores , axis = 1))\n",
        "    test_std   = np.std(test_scores , axis = 1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Learning Curve (Full Data)\")\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.grid()\n",
        "\n",
        "    #  المعادله هاي عشان ارسم التباين\n",
        "    plt.fill_between(train_sizes , train_mean - train_std , train_mean + train_std , alpha = 0.1 , color = \"r\")\n",
        "    plt.fill_between(train_sizes , test_mean - test_std , test_mean + test_std , alpha = 0.1 , color = \"g\")\n",
        "\n",
        "    plt.plot(train_sizes , train_mean , 'o-' , color = \"r\" , label = \"Training score\")\n",
        "    plt.plot(train_sizes , test_mean , 'o-' , color = \"g\" , label = \"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc = \"best\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84920968",
      "metadata": {},
      "outputs": [],
      "source": [
        "def Q_Q(model , xr_train , yr_train , xr_test , yr_test):\n",
        "\n",
        "    model.fit(xr_train , yr_train)\n",
        "    y_pred = model.predict(xr_test)\n",
        "    residuals = yr_test - y_pred\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "    plt.title('Q-Q Plot for Residuals Normality')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892808f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def Residual_plots(model , xr_train , yr_train , xr_test , yr_test):\n",
        "\n",
        "    model.fit(xr_train , yr_train)\n",
        "    y_pred = model.predict(xr_test)\n",
        "\n",
        "    #الفرق بين القيم الاصليه والمتوقعه , الهدف منها انك تشوف التباين \n",
        "    residuals = yr_test - y_pred\n",
        "\n",
        "    plt.figure(figsize = (10 , 5))\n",
        "    sns.scatterplot(x = y_pred , y = residuals)\n",
        "    plt.axhline(y = 0 , color = 'r' , linestyle = '--')   \n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Residual Plot')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d34944",
      "metadata": {
        "id": "45d34944"
      },
      "outputs": [],
      "source": [
        "def model_score(model , xr_train , yr_train , xr_test , yr_test):\n",
        "    model.fit(xr_train , yr_train)\n",
        "    y_pred = model.predict(xr_test)\n",
        "\n",
        "    mae = mean_absolute_error(yr_test , y_pred)\n",
        "    print(f\"MAE:  {mae}\")\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(yr_test , y_pred))\n",
        "    print(f\"RMSE: {rmse}\")\n",
        "\n",
        "    r2 = r2_score(yr_test , y_pred)\n",
        "    print(f\"R2:   {r2}\")\n",
        "\n",
        "    mse = mean_squared_error(yr_test, y_pred)   \n",
        "    print(f\"MSE:  {mse}\")         \n",
        "\n",
        "    #اللي بتعمله هاي انها بتعمل خصم على كل فيتشر جديد بدخل للموديل , اذا الفيتشر الجديد زاد السكور معناها انه فيه معلومات مهمه ومفيده للموديل , واذا العكس معناها انه زباله وحكي فاضي\n",
        "    n = len(yr_test)\n",
        "    p = xr_test.shape[1]   \n",
        "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "    print(f\"Adj R2: {adj_r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35db1a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def het_breu(model , xr_train , yr_train , test_reg , yr_test):\n",
        "\n",
        "    model.fit(xr_train , yr_train)\n",
        "    y_pred = model.predict(xr_test)\n",
        "    residuals = yr_test - y_pred\n",
        "    xtest_with_const = sm.add_constant(test_reg)\n",
        "\n",
        "    lm , p_value , fvalue , f_pvalue = het_breuschpagan(residuals , xtest_with_const)\n",
        "\n",
        "    \"\"\"\n",
        "    lm-> Lagrange Multiplier Statistic   بقيس قوة العلاقه بين مربع الاخطاء والمتغيرات المستقله\n",
        "    p_value-> القيمه الاختماله الخاصه ب lm \n",
        "    fvalue-> F-statistic\n",
        "    f_pvalue-> نفس الاولى\n",
        "    #رح استخدم الاولى لانها ادق على البانات الكبيره من الاخيره\n",
        "    \"\"\"\n",
        "\n",
        "    if p_value < 0.05 :\n",
        "        print(\"Heteroscedasticity\") #تباين سيء ومش ثابت\n",
        "\n",
        "    else:\n",
        "        print(\"Homoscedasticity\")   #تباين ثابت"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde17af1",
      "metadata": {
        "id": "bde17af1",
        "outputId": "ef9fc9d7-0ba3-4b5a-889a-7a540e3ea507"
      },
      "outputs": [],
      "source": [
        "KNNRsmall_users = np.random.choice(tuning_groups.unique(), size=5000, replace=False)\n",
        "\n",
        "mask = tuning_groups.isin(KNNRsmall_users)\n",
        "\n",
        "KNNR_x_small = tuning_xr.loc[mask].copy()\n",
        "KNNR_y_small = tuning_yr.loc[mask].copy()\n",
        "KNNR_groups_small = tuning_groups.loc[mask].copy()\n",
        "\n",
        "KNNR_pipline = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    KNeighborsRegressor()\n",
        ")\n",
        " #  [7 , 10 , 20 , 25 , 30] هون اول اشي عملنا اخترنا\n",
        "#تمام بما انو المودل اختار 30 هاض بيعني انو لسا في مجال نضيف عدد اكبر من النيبرز فا رح نعدلها ل [30, 50, 70, 100]\n",
        "# لو بدك تاكد تقلل ال 100 لو بتعمل اندر فيت وشوف\n",
        "KNNRparan_grid = {\n",
        "    \"kneighborsregressor__n_neighbors\": [30, 50, 70, 100] ,\n",
        "    \"kneighborsregressor__weights\" : ['distance' , 'uniform']\n",
        "}\n",
        "\n",
        "KNNR_grid_search = GridSearchCV(\n",
        "    estimator = KNNR_pipline ,\n",
        "    param_grid = KNNRparan_grid ,\n",
        "    cv = GF ,\n",
        "    scoring = \"r2\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "\n",
        "KNNR_grid_search.fit(KNNR_x_small , KNNR_y_small , groups = KNNR_groups_small)\n",
        "\n",
        "print(\"\\nBest Score:\" , KNNR_grid_search.best_score_)\n",
        "print(\"Best Parameters:\" , KNNR_grid_search.best_params_)\n",
        "#النتائج هاي قبل تزبيط السبلت\n",
        "#\"\"\"Best Score: -8.939542770385742        neg_root_mean_squared_error\n",
        "#Best Parameters: {'kneighborsregressor__n_neighbors': 30, 'kneighborsregressor__weights': 'uniform'}\"\"\"\n",
        "#=============================================\n",
        "#النتائج هاي بعد تزبيط السبلت وعدد الجيران\n",
        "#Best Score: 0.19590063095092775\n",
        "#Best Parameters: {'kneighborsregressor__n_neighbors': 100, 'kneighborsregressor__weights': 'uniform'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da3640ba",
      "metadata": {
        "id": "da3640ba",
        "outputId": "4c8631b0-9a5e-4f23-8f06-169fa85081da"
      },
      "outputs": [],
      "source": [
        "best_Rk = KNNR_grid_search.best_params_[\"kneighborsregressor__n_neighbors\"]\n",
        "\n",
        "best_Rw = KNNR_grid_search.best_params_[\"kneighborsregressor__weights\"]\n",
        "\n",
        "final_KNNR_pip = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    KNeighborsRegressor(n_neighbors = best_Rk , weights = best_Rw , n_jobs = -1)\n",
        ")\n",
        "\n",
        "model_score(final_KNNR_pip , xr_train , yr_train , xr_test , yr_test)\n",
        "Residual_plots(final_KNNR_pip , xr_train , yr_train , xr_test , yr_test)\n",
        "\n",
        "het_breu(final_KNNR_pip , xr_train , yr_train , test_reg , yr_test)\n",
        "\n",
        "#قبل التعديل على السبلت وعدد الجيران هيك كانت النتائج\n",
        "# MAE:  5.356390476226807\n",
        "# RMSE: 7.753037626488352\n",
        "# R2:   0.3814122676849365\n",
        "\n",
        "#==============================\n",
        "#بعد التعديل\n",
        "# MAE:  4.928064823150635\n",
        "# RMSE: 6.896374702453613\n",
        "# R2:   0.5105602741241455\n",
        "#شوف فرق ال ار 2 بعد التعديل مش بطال على كي ان ان على داتا زي هيك"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe1542d",
      "metadata": {},
      "outputs": [],
      "source": [
        "Q_Q(final_KNNR_pip , xr_train , yr_train , xr_test , yr_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81039735",
      "metadata": {
        "id": "81039735"
      },
      "source": [
        "2- Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7c4690",
      "metadata": {
        "id": "5b7c4690"
      },
      "outputs": [],
      "source": [
        "DTRsmall_users = np.random.choice(tuning_groups.unique(), size=5000, replace=False)\n",
        "mask = tuning_groups.isin(DTRsmall_users)\n",
        "\n",
        "DTR_x_small = tuning_xr.loc[mask].copy()\n",
        "DTR_y_small = tuning_yr.loc[mask].copy()\n",
        "DTR_groups_small = tuning_groups.loc[mask].copy()\n",
        "\n",
        "\n",
        "DTR_pipline = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    DecisionTreeRegressor(random_state = 42)\n",
        ")\n",
        "\n",
        "DTRparam_grid = {\n",
        "    \"decisiontreeregressor__max_depth\": [None , 10 , 20 , 30] ,\n",
        "    \"decisiontreeregressor__min_samples_split\": [2 , 10 , 20] ,\n",
        "    \"decisiontreeregressor__min_samples_leaf\": [1 , 10 , 50] ,\n",
        "}\n",
        "\n",
        "#ال KF اعطتني نفس نتيجة ال GF\n",
        "DTR_grid_search = GridSearchCV(\n",
        "    estimator = DTR_pipline ,\n",
        "    param_grid = DTRparam_grid ,\n",
        "    cv = KF ,\n",
        "    scoring = \"r2\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "DTR_grid_search.fit(DTR_x_small, DTR_y_small, groups = DTR_groups_small)\n",
        "\n",
        "print(\"\\nBest Score:\" , DTR_grid_search.best_score_)\n",
        "print(\"Best Parameters:\" , DTR_grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be356c8e",
      "metadata": {
        "id": "be356c8e",
        "outputId": "6844bce8-c357-484b-9514-67ca28982b5d"
      },
      "outputs": [],
      "source": [
        "best_RMD = DTR_grid_search.best_params_[\"decisiontreeregressor__max_depth\"]\n",
        "best_RMSS = DTR_grid_search.best_params_[ \"decisiontreeregressor__min_samples_split\"]\n",
        "best_RMSL = DTR_grid_search.best_params_[\"decisiontreeregressor__min_samples_leaf\"]\n",
        "\n",
        "final_DTR_pip = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    DecisionTreeRegressor(max_depth = best_RMD , min_samples_leaf = best_RMSL ,\n",
        "                           min_samples_split = best_RMSS , random_state = 42)\n",
        ")\n",
        "model_score(final_DTR_pip , xr_train , yr_train , xr_test , yr_test)\n",
        "Residual_plots(final_DTR_pip , xr_train , yr_train , xr_test , yr_test)\n",
        "\n",
        "het_breu(final_DTR_pip , xr_train , yr_train , test_reg , yr_test)\n",
        "\n",
        "#قبل تعديل السبلت\n",
        "# MAE:  5.257224113534276\n",
        "# RMSE: 7.805453685792402\n",
        "# R2:   0.3730198334737874\n",
        " #بعد التعديل\n",
        "#MAE:  4.519589961657373\n",
        "#RMSE: 6.618155207417279\n",
        "#R2:   0.5464313036296291\n",
        "#MSE:  43.799978349464446\n",
        "#Adj R2: 0.5463093765607123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e5b8d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "Q_Q(final_DTR_pip , xr_train , yr_train , xr_test , yr_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735e2066",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_curves_reg(final_DTR_pip , xr_train , yr_train , KF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76365b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "#اخذت اهم باراميتر واكثر واحد بأثر على تعقيد الموديل\n",
        "#اخذت القيم زي ما همه من التونينق بس بدونو النان , اعطاني عليها ايرور\n",
        "depth_range = [1, 3, 5, 7, 10, 15, 20 , 30]\n",
        "\n",
        "val_curves(final_DTR_pip , xr_train , yr_train , \"decisiontreeregressor__max_depth\" , KF , depth_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7d8d7c",
      "metadata": {},
      "source": [
        "3- Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0251981b",
      "metadata": {},
      "outputs": [],
      "source": [
        "RFRsmall_users = np.random.choice(tuning_groups.unique(), size=1000, replace=False)\n",
        "mask = tuning_groups.isin(RFRsmall_users)\n",
        "\n",
        "RFR_x_small = tuning_xr.loc[mask].copy()\n",
        "RFR_y_small = tuning_yr.loc[mask].copy()\n",
        "RFR_groups_small = tuning_groups.loc[mask].copy()\n",
        "\n",
        "RFR_pipline = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    RandomForestRegressor(random_state = 42 , n_jobs = -1)\n",
        ")\n",
        "\n",
        "RFRparam_grid = {\n",
        "    \"randomforestregressor__n_estimators\": [100 , 150 , 200],\n",
        "    \"randomforestregressor__max_depth\": [None , 10 , 20],\n",
        "    \"randomforestregressor__min_samples_leaf\": [1, 2, 5] ,\n",
        "    \"randomforestregressor__min_samples_split\": [2 , 5 , 10]\n",
        "}\n",
        "\n",
        "RFR_grid_search = GridSearchCV(\n",
        "    estimator = RFR_pipline ,\n",
        "    param_grid = RFRparam_grid ,\n",
        "    cv = GF ,\n",
        "    scoring = \"r2\" ,\n",
        "    n_jobs = -1\n",
        ")\n",
        "\n",
        "RFR_grid_search.fit(RFR_x_small, RFR_y_small, groups = RFR_groups_small)\n",
        "\n",
        "print(\"\\nBest Score:\" , RFR_grid_search.best_score_)\n",
        "print(\"Best Parameters:\" , RFR_grid_search.best_params_)\n",
        "\n",
        "\"\"\"Best Score: 0.1056874835441742\n",
        "Best Parameters: {'randomforestregressor__max_depth': 10, 'randomforestregressor__min_samples_leaf': 1, 'randomforestregressor__min_samples_split': 10, 'randomforestregressor__n_estimators': 200}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be13b504",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_N = RFR_grid_search.best_params_[\"randomforestregressor__n_estimators\"]\n",
        "best_MD = RFR_grid_search.best_params_[ \"randomforestregressor__max_depth\"]\n",
        "best_MSL = RFR_grid_search.best_params_[\"randomforestregressor__min_samples_leaf\"]\n",
        "best_MSS = RFR_grid_search.best_params_[\"randomforestregressor__min_samples_split\"]\n",
        "\n",
        "final_RFR_pip = make_pipeline(\n",
        "    preprocessor_reg ,\n",
        "    RandomForestRegressor(n_estimators = best_N , max_depth = None ,\n",
        "                    min_samples_leaf = 10 , min_samples_split = best_MSS ,\n",
        "                    n_jobs = -1 , random_state = 42 , criterion=\"squared_error\"\n",
        "                        )\n",
        ")\n",
        "\n",
        "#criterion -> CART algorithm قبل شوي قرأتها بالسلايدات يسعد مساك دكتور\n",
        "#وللأسف الشديد ما فرقت معي بالنتيجه نهائيا\n",
        "\n",
        "#الماكس ديبث والسامبل ليف , حطيتهم بشكل يدوي بعد تجارب كثيره , لانه الموديل صارله بالبدايه صارله اندر فيت , بعدين زدت سبة الداتا وصار\n",
        "#اندر فيت , ف ضلييت اعدل واجرب لتني اعتمدت هذول \n",
        "#رفعت السامبلز من 10000 الى 30000 :)\n",
        "\n",
        "model_score(final_RFR_pip , xr_train , yr_train , xr_test , yr_test) \n",
        "Residual_plots(final_RFR_pip , xr_train , yr_train , xr_test , yr_test)\n",
        "\n",
        "r2_train = final_RFR_pip.score(xr_train, yr_train)\n",
        "\n",
        "het_breu(final_RFR_pip , xr_train , yr_train , test_reg , yr_test)\n",
        "\n",
        "\"\"\"\"MAE:  4.53925179592098\n",
        "RMSE: 6.636387873223536\n",
        "R2:   0.5439287458669073\n",
        "MSE:  44.04164400386841\n",
        "Adj R2: 0.5438061460674092\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0dc688",
      "metadata": {},
      "outputs": [],
      "source": [
        "Q_Q(final_RFR_pip , xr_train , yr_train , xr_test , yr_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
